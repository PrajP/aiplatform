{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "#ENDPOINT = '<YOUR_ENDPOINT>' e.g. '337dd39580cbcbd2-dot-us-central2.pipelines.googleusercontent.com'\n",
    "INPUT_FILE = 'gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx'\n",
    "ARTIFACT_STORE_URI = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'X_train.xlsx')\n",
    "TRAINING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'training')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'X_validate.xlsx')\n",
    "VALIDATION_FILE_DIR='{}/{}'.format(DATA_ROOT, 'validation')\n",
    "TESTING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Write the training APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Write the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split  ## from analysis\n",
    "\n",
    "#simple sklearn impute and scale numeric pipeline\n",
    "from sklearn.pipeline import Pipeline ## from analysis\n",
    "from sklearn.impute import SimpleImputer ## from analysis\n",
    "from sklearn.preprocessing import StandardScaler ## from analysis\n",
    "import numpy as np ## from analysis\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, input_file, training_dataset, validation_dataset, n_estimators, max_depth, min_samples_leaf, max_features, min_samples_split, class_weight, max_leaf_nodes, random_state, hptune, bootstrap):\n",
    "\n",
    "   \n",
    "    obj = input_file\n",
    "    print(\"obj\", obj)\n",
    "    data = pd.read_excel(obj,sheet_name='data') \n",
    "    print(\"Opened excel file and assigned to data\")\n",
    "    meta_data = pd.read_excel(obj,sheet_name='meta data') \n",
    "    print(\"Just after file opening\")\n",
    "    \n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "\n",
    "    ## Preprocess    \n",
    "    #Prepare data for analysis\n",
    "    #Split out numeric from categorical varibles\n",
    "\n",
    "    ##var_type_filter = [x in ['physiological','biochemical','process'] for x in meta_data['variable type']]\n",
    "    var_type_filter = [x in ['independent'] for x in meta_data['variable type']]\n",
    "    var_dtype_filter = (data.dtypes == 'float64') | (data.dtypes == 'int64')\n",
    "\n",
    "    numeric_vars = (var_type_filter & var_dtype_filter).values\n",
    "    numeric_x_data = data[data.columns[numeric_vars]]\n",
    "\n",
    "    #things to try to predict\n",
    "    y_data = data[data.columns[(meta_data['target'] == 1).values]]\n",
    "\n",
    "    #meta data about variables\n",
    "    meta_data = meta_data.query('name in {}'.format(list(data.columns[numeric_vars].values))).set_index('name')\n",
    " \n",
    "    #Variables which will be used to build the model\n",
    "    ####data.columns[numeric_vars].values\n",
    "        \n",
    "\n",
    "\n",
    "    model_target = 'Run_Performance' ## Select target for classification\n",
    "    \n",
    "    print(\"before splitting data\")\n",
    "    #maintain class balance\n",
    "    X_train, X_test, y_train, y_test = train_test_split(numeric_x_data, y_data, test_size=0.25, stratify = y_data[model_target], random_state=42)\n",
    "\n",
    "    #split train set to create a pseudo test or validation dataset\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.33, stratify= y_train[model_target], random_state=42)\n",
    "    \n",
    "    \n",
    "    print('The training, validation and test data contain {}, {} and {} rows respectively'.format(len(X_train),len(X_validate),len(X_test)))\n",
    "\n",
    "    #save_list_train = ['X_train','y_train','meta_data']\n",
    "    #save_list_validate = ['X_validate','y_validate','meta_data']\n",
    "    #save_list_test = ['X_test','y_test','meta_data']\n",
    "\n",
    "    #for x in save_list_train:\n",
    "    #    print(\"training_dataset\", training_dataset)\n",
    "    #    obj = pd.DataFrame(globals()[x])\n",
    "    #    cmd = \"obj.to_csv('{}/{}.csv')\".format(training_dataset, x)\n",
    "    #    eval(cmd)\n",
    "\n",
    "    #for y in save_list_validate:\n",
    "    #    print(\"validation_dataset\", validation_dataset)\n",
    "    #    obj = pd.DataFrame(globals()[y])\n",
    "    #    cmd = \"obj.to_csv('{}/{}.csv')\".format(validation_dataset, y)\n",
    "    #    eval(cmd)  \n",
    "\n",
    "    #for z in save_list_test:\n",
    "    #    print(\"testing_dataset\", testing_dataset)\n",
    "    #    obj = pd.DataFrame(globals()[z])\n",
    "    #    cmd = \"obj.to_csv('{}/{}.csv')\".format(testing_dataset, z)\n",
    "    #    eval(cmd)\n",
    "\n",
    "    ## Train, optimize and validate predictive model\n",
    "    ### Train\n",
    "\n",
    "\n",
    "\n",
    "    classifier = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                max_features=max_features,\n",
    "                min_samples_split=min_samples_split,\n",
    "                class_weight=class_weight,\n",
    "                max_leaf_nodes=max_leaf_nodes,\n",
    "                random_state=random_state,\n",
    "                bootstrap=bootstrap\n",
    "\n",
    "     )\n",
    "\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    #auto scale\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    estimator = Pipeline([\n",
    "      ('imputer', imputer),\n",
    "      ('scaler', scaler),\n",
    "      ('classifier', classifier),\n",
    "    ])\n",
    "    \n",
    "\n",
    "    #prepare data for modeling\n",
    "    #use the pipeline created above\n",
    "    #_X_train = pipe.fit_transform(X_train)\n",
    "    #_y_train = y_train[model_target]    ## selected target label for prediction\n",
    "    #_X_test = pipe.fit_transform(X_validate)\n",
    "    #_y_test = y_validate[model_target]\n",
    "\n",
    "    #_X_train = pipe.fit_transform(X_train)\n",
    "    _X_train = X_train    \n",
    "    _y_train = y_train[model_target]    ## selected target label for prediction\n",
    "    _X_test = X_validate\n",
    "    _y_test = y_validate[model_target]\n",
    "    \n",
    "\n",
    "    print('Starting training: n_estimators={}, max_depth={}, min_samples_leaf={}, max_features={}, min_samples_split={}, class_weight={}, max_leaf_nodes={}, random_state={}, hptune={}, bootstrap={}'.format(n_estimators, max_depth, min_samples_leaf, max_features, min_samples_split, class_weight, max_leaf_nodes, random_state, hptune, bootstrap))\n",
    "\n",
    "    #estimator.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter) \n",
    "    estimator.set_params(classifier__n_estimators=n_estimators, classifier__max_depth=max_depth, classifier__min_samples_leaf=min_samples_leaf, \n",
    "                         classifier__max_features=max_features, classifier__min_samples_split=min_samples_split, classifier__class_weight=class_weight,\n",
    "                         classifier__max_leaf_nodes=max_leaf_nodes, classifier__random_state=random_state, classifier__bootstrap=bootstrap) \n",
    "    #pipeline.fit(X_train, y_train)\n",
    "    estimator.fit(_X_train, _y_train)\n",
    "\n",
    "    \n",
    "    if hptune:\n",
    "        accuracy = estimator.score(_X_test, _y_test)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(estimator, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Package the script into a docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 21 file(s) totalling 105.4 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://etl-project-datahub_cloudbuild/source/1599254137.08997-248e7a5e3b0b46b685f30d3a09b6c756.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/etl-project-datahub/builds/5e998e48-afcc-4ffb-ad9f-9f02eebfabc0].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/5e998e48-afcc-4ffb-ad9f-9f02eebfabc0?project=448067079266].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"5e998e48-afcc-4ffb-ad9f-9f02eebfabc0\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://etl-project-datahub_cloudbuild/source/1599254137.08997-248e7a5e3b0b46b685f30d3a09b6c756.tgz#1599254137590293\n",
      "Copying gs://etl-project-datahub_cloudbuild/source/1599254137.08997-248e7a5e3b0b46b685f30d3a09b6c756.tgz#1599254137590293...\n",
      "/ [1 files][ 26.2 KiB/ 26.2 KiB]                                                \n",
      "Operation completed over 1 objects/26.2 KiB.                                     \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  124.4kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 34126d51881b\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=ecab971f38002d0f102663fcf9354f01dd2e8fac4f636046528afa1117481220\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=59bce18b77904981acf2fdfec303d291d995ec927cf35632bf989cb8fc8a061c\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=ee7ebab45e2036b3456c5245bc325f4ea62d7d520ea3a3d66b5f17f24e1879ff\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container 34126d51881b\n",
      " ---> e2445654a472\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in bad493852edc\n",
      "Removing intermediate container bad493852edc\n",
      " ---> 0b181bc86986\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> a2089bcca898\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 08765e6a6c6b\n",
      "Removing intermediate container 08765e6a6c6b\n",
      " ---> fa07bffb7dfa\n",
      "Successfully built fa07bffb7dfa\n",
      "Successfully tagged gcr.io/etl-project-datahub/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/etl-project-datahub/trainer_image:latest\n",
      "The push refers to repository [gcr.io/etl-project-datahub/trainer_image]\n",
      "ddd6c0519b08: Preparing\n",
      "7270090179d7: Preparing\n",
      "d0fce945869e: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "c51fe61c6231: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "ddd6c0519b08: Pushed\n",
      "7270090179d7: Pushed\n",
      "69b6474ff053: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "d0fce945869e: Pushed\n",
      "latest: digest: sha256:7373f15361f0a7782d49a1a8d41ffd834725235173af75b22fc802a3173300a2 size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                              STATUS\n",
      "5e998e48-afcc-4ffb-ad9f-9f02eebfabc0  2020-09-04T21:15:37+00:00  3M46S     gs://etl-project-datahub_cloudbuild/source/1599254137.08997-248e7a5e3b0b46b685f30d3a09b6c756.tgz  gcr.io/etl-project-datahub/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter tuning jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 4\n",
    "    maxParallelTrials: 4\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE\n",
    "    algorithm: RANDOM_SEARCH\n",
    "    params:\n",
    "    - parameterName: n_estimators\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 200\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: max_depth\n",
    "      type: INTEGER\n",
    "      minValue: 3\n",
    "      maxValue: 100\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: min_samples_leaf\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 500\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: max_features\n",
    "      type: DOUBLE\n",
    "      minValue: 0.5\n",
    "      maxValue: 1.0\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: min_samples_split\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          2,\n",
    "          5,\n",
    "          10\n",
    "      ]\n",
    "    - parameterName: class_weight\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"balanced\",\n",
    "          \"balanced_subsample\"\n",
    "      ]\n",
    "    - parameterName: max_leaf_nodes\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 500\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: random_state\n",
    "      type: INTEGER\n",
    "      minValue: 35\n",
    "      maxValue: 75\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: bootstrap\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"TRUE\",\n",
    "          \"FALSE\"\n",
    "      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Start the hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "job_dir -> GCS Path for storing the job packages & model.\n",
    "\n",
    "training_dataset_path -> GCS path holding training dataset.\n",
    "\n",
    "validation_dataset_path -> GCS path holding validation dataset.\n",
    "\n",
    "alpha -> hyperparameter\n",
    "\n",
    "max_iter -> hyperparameter\n",
    "\n",
    "hptune -> variable to decide if hyperparameter tuning is to be done or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'X_train.xlsx')\n",
    "TRAINING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'training')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'X_validate.xlsx')\n",
    "VALIDATION_FILE_DIR='{}/{}'.format(DATA_ROOT, 'validation')\n",
    "TESTING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "#TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'dataset.xlsx')\n",
    "#VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200904_212118] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200904_212118\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200904_212118\n",
      "jobId: JOB_20200904_212118\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "  \n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset=$TRAINING_FILE_DIR \\\n",
    "--validation_dataset=$VALIDATION_FILE_DIR \\\n",
    "--input_file=$INPUT_FILE \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2020-09-04T20:35:21Z'\n",
      "etag: _zwYn855T4M=\n",
      "jobId: JOB_20200904_203519\n",
      "startTime: '2020-09-04T20:35:23Z'\n",
      "state: RUNNING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --training_dataset=gs://workshop_trial_artifact_store_pp/data/training\n",
      "  - --validation_dataset=gs://workshop_trial_artifact_store_pp/data/validation\n",
      "  - --input_file=gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx\n",
      "  - --hptune\n",
      "  hyperparameters:\n",
      "    algorithm: RANDOM_SEARCH\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: accuracy\n",
      "    maxParallelTrials: 4\n",
      "    maxTrials: 4\n",
      "    params:\n",
      "    - maxValue: 200.0\n",
      "      minValue: 10.0\n",
      "      parameterName: n_estimators\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 100.0\n",
      "      minValue: 3.0\n",
      "      parameterName: max_depth\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 500.0\n",
      "      minValue: 10.0\n",
      "      parameterName: min_samples_leaf\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 1.0\n",
      "      minValue: 0.5\n",
      "      parameterName: max_features\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: DOUBLE\n",
      "    - discreteValues:\n",
      "      - 2.0\n",
      "      - 5.0\n",
      "      - 10.0\n",
      "      parameterName: min_samples_split\n",
      "      type: DISCRETE\n",
      "    - categoricalValues:\n",
      "      - balanced\n",
      "      - balanced_subsample\n",
      "      parameterName: class_weight\n",
      "      type: CATEGORICAL\n",
      "    - maxValue: 500.0\n",
      "      minValue: 10.0\n",
      "      parameterName: max_leaf_nodes\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 75.0\n",
      "      minValue: 35.0\n",
      "      parameterName: random_state\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - categoricalValues:\n",
      "      - 'TRUE'\n",
      "      - 'FALSE'\n",
      "      parameterName: bootstrap\n",
      "      type: CATEGORICAL\n",
      "  jobDir: gs://workshop_trial_artifact_store_pp/jobs/JOB_20200904_203519\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/etl-project-datahub/trainer_image:latest\n",
      "  region: us-central1\n",
      "trainingOutput:\n",
      "  isHyperparameterTuningJob: true\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20200904_203519?project=etl-project-datahub\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2FJOB_20200904_203519&project=etl-project-datahub\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for streaming the logs\n",
    "#!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20200904_203519',\n",
       " 'trainingInput': {'args': ['--training_dataset=gs://workshop_trial_artifact_store_pp/data/training',\n",
       "   '--validation_dataset=gs://workshop_trial_artifact_store_pp/data/validation',\n",
       "   '--input_file=gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx',\n",
       "   '--hptune'],\n",
       "  'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "   'params': [{'parameterName': 'n_estimators',\n",
       "     'minValue': 10,\n",
       "     'maxValue': 200,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'max_depth',\n",
       "     'minValue': 3,\n",
       "     'maxValue': 100,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'min_samples_leaf',\n",
       "     'minValue': 10,\n",
       "     'maxValue': 500,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'max_features',\n",
       "     'minValue': 0.5,\n",
       "     'maxValue': 1,\n",
       "     'type': 'DOUBLE',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'min_samples_split',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [2, 5, 10]},\n",
       "    {'parameterName': 'class_weight',\n",
       "     'type': 'CATEGORICAL',\n",
       "     'categoricalValues': ['balanced', 'balanced_subsample']},\n",
       "    {'parameterName': 'max_leaf_nodes',\n",
       "     'minValue': 10,\n",
       "     'maxValue': 500,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'random_state',\n",
       "     'minValue': 35,\n",
       "     'maxValue': 75,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'bootstrap',\n",
       "     'type': 'CATEGORICAL',\n",
       "     'categoricalValues': ['TRUE', 'FALSE']}],\n",
       "   'maxTrials': 4,\n",
       "   'maxParallelTrials': 4,\n",
       "   'hyperparameterMetricTag': 'accuracy',\n",
       "   'enableTrialEarlyStopping': True,\n",
       "   'algorithm': 'RANDOM_SEARCH'},\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://workshop_trial_artifact_store_pp/jobs/JOB_20200904_203519',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/etl-project-datahub/trainer_image:latest'}},\n",
       " 'createTime': '2020-09-04T20:35:21Z',\n",
       " 'startTime': '2020-09-04T20:35:23Z',\n",
       " 'state': 'RUNNING',\n",
       " 'trainingOutput': {'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'accuracy'},\n",
       " 'etag': 't3id35uXkgc='}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
