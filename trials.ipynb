{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "#ENDPOINT = '<YOUR_ENDPOINT>' e.g. '337dd39580cbcbd2-dot-us-central2.pipelines.googleusercontent.com'\n",
    "INPUT_FILE = 'gs://input_data_amy_bkt1/input_data/Anonymized_Fermentation_Data_final.xlsx'\n",
    "ARTIFACT_STORE_URI = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'X_train.xlsx')\n",
    "TRAINING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'training')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'X_validate.xlsx')\n",
    "VALIDATION_FILE_DIR='{}/{}'.format(DATA_ROOT, 'validation')\n",
    "TESTING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_path=TRAINING_FILE_PATH \n",
    "validation_dataset_path=VALIDATION_FILE_PATH \n",
    "training_dataset=TRAINING_FILE_DIR \n",
    "validation_dataset=VALIDATION_FILE_DIR \n",
    "testing_dataset=TESTING_FILE_DIR \n",
    "input_file=INPUT_FILE \n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "#import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "#import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split  ## from analysis\n",
    "\n",
    "#simple sklearn impute and scale numeric pipeline\n",
    "from sklearn.pipeline import Pipeline ## from analysis\n",
    "from sklearn.impute import SimpleImputer ## from analysis\n",
    "from sklearn.preprocessing import StandardScaler ## from analysis\n",
    "import numpy as np ## from analysis\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj gs://input_data_amy_bkt1/input_data/Anonymized_Fermentation_Data_final.xlsx\n",
      "Opened excel file and assigned to data\n",
      "Just after file opening\n"
     ]
    }
   ],
   "source": [
    "obj = input_file\n",
    "print(\"obj\", obj)\n",
    "data = pd.read_excel('gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx',sheet_name='data') \n",
    "print(\"Opened excel file and assigned to data\")\n",
    "meta_data = pd.read_excel('gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx',sheet_name='meta data') \n",
    "print(\"Just after file opening\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#prepare data for analysis\n",
    "#split out numeric from categorical varibles\n",
    "var_type_filter = [x in ['independent'] for x in meta_data['variable type']]\n",
    "var_dtype_filter = (data.dtypes == 'float64') | (data.dtypes == 'int64')\n",
    "\n",
    "numeric_vars = (var_type_filter & var_dtype_filter).values\n",
    "numeric_x_data = data[data.columns[numeric_vars]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#things to try to predict\n",
    "y_data = data[data.columns[(meta_data['target'] == 1).values]]\n",
    "\n",
    "#meta data about variables\n",
    "meta_data = meta_data.query('name in {}'.format(list(data.columns[numeric_vars].values))).set_index('name')\n",
    "\n",
    "#Variables which will be used to build the model\n",
    "####data.columns[numeric_vars].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before splitting data\n",
      "The training, validation and test data contain 787, 389 and 392 rows respectively\n"
     ]
    }
   ],
   "source": [
    "model_target = 'Run_Performance' ## Select target for classification\n",
    "\n",
    "print(\"before splitting data\")\n",
    "#maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(numeric_x_data, y_data, test_size=0.25, stratify = y_data[model_target], random_state=42)\n",
    "\n",
    "#split train set to create a pseudo test or validation dataset\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.33, stratify= y_train[model_target], random_state=42)\n",
    "\n",
    "\n",
    "print('The training, validation and test data contain {}, {} and {} rows respectively'.format(len(X_train),len(X_validate),len(X_test)))\n",
    "\n",
    "save_list_train = ['X_train','y_train','meta_data']\n",
    "save_list_validate = ['X_validate','y_validate','meta_data']\n",
    "save_list_test = ['X_test','y_test','meta_data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before splitting data\n",
      "The training, validation and test data contain 787, 389 and 392 rows respectively\n",
      "training_dataset gs://workshop_trial_artifact_store_pp/data/training\n",
      "cmd obj.to_csv('gs://workshop_trial_artifact_store_pp/data/training/X_train.csv')\n",
      "training_dataset gs://workshop_trial_artifact_store_pp/data/training\n",
      "cmd obj.to_csv('gs://workshop_trial_artifact_store_pp/data/training/y_train.csv')\n",
      "training_dataset gs://workshop_trial_artifact_store_pp/data/training\n",
      "cmd obj.to_csv('gs://workshop_trial_artifact_store_pp/data/training/meta_data.csv')\n",
      "validation_dataset gs://workshop_trial_artifact_store_pp/data/validation\n",
      "validation_dataset gs://workshop_trial_artifact_store_pp/data/validation\n",
      "validation_dataset gs://workshop_trial_artifact_store_pp/data/validation\n",
      "testing_dataset gs://workshop_trial_artifact_store_pp/data/testing\n",
      "testing_dataset gs://workshop_trial_artifact_store_pp/data/testing\n",
      "testing_dataset gs://workshop_trial_artifact_store_pp/data/testing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "\n",
    "training_dataset_path=TRAINING_FILE_PATH \n",
    "validation_dataset_path=VALIDATION_FILE_PATH \n",
    "training_dataset=TRAINING_FILE_DIR \n",
    "validation_dataset=VALIDATION_FILE_DIR \n",
    "testing_dataset=TESTING_FILE_DIR \n",
    "input_file=INPUT_FILE \n",
    "\n",
    "\n",
    "model_target = 'Run_Performance' ## Select target for classification\n",
    "\n",
    "print(\"before splitting data\")\n",
    "#maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(numeric_x_data, y_data, test_size=0.25, stratify = y_data[model_target], random_state=42)\n",
    "\n",
    "#split train set to create a pseudo test or validation dataset\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.33, stratify= y_train[model_target], random_state=42)\n",
    "\n",
    "\n",
    "print('The training, validation and test data contain {}, {} and {} rows respectively'.format(len(X_train),len(X_validate),len(X_test)))\n",
    "\n",
    "save_list_train = ['X_train','y_train','meta_data']\n",
    "save_list_validate = ['X_validate','y_validate','meta_data']\n",
    "save_list_test = ['X_test','y_test','meta_data']\n",
    "\n",
    "\n",
    "\n",
    "save_list_train = ['X_train','y_train','meta_data']\n",
    "save_list_validate = ['X_validate','y_validate','meta_data']\n",
    "save_list_test = ['X_test','y_test','meta_data']\n",
    "\n",
    "\n",
    "for x in save_list_train:\n",
    "    print(\"training_dataset\", training_dataset)\n",
    "    obj = pd.DataFrame(globals()[x])\n",
    "    cmd = \"obj.to_csv('{}/{}.csv')\".format(training_dataset, x)\n",
    "    print(\"cmd\" , cmd)\n",
    "    eval(cmd)\n",
    "\n",
    "for y in save_list_validate:\n",
    "    print(\"validation_dataset\", validation_dataset)\n",
    "    obj = pd.DataFrame(globals()[y])\n",
    "    cmd = \"obj.to_csv('{}/{}.csv')\".format(validation_dataset, y)\n",
    "    eval(cmd)  \n",
    "    \n",
    "for z in save_list_test:\n",
    "    print(\"testing_dataset\", testing_dataset)\n",
    "    obj = pd.DataFrame(globals()[z])\n",
    "    cmd = \"obj.to_csv('{}/{}.csv')\".format(testing_dataset, z)\n",
    "    eval(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_dataset gs://workshop_trial_artifact_store_pp/data/training\n"
     ]
    },
    {
     "ename": "FileCreateError",
     "evalue": "[Errno 2] No such file or directory: 'gs://workshop_trial_artifact_store_pp/data/training/X_train.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xlsxwriter/workbook.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xlsxwriter/workbook.py\u001b[0m in \u001b[0;36m_store_workbook\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xlsxwriter/workbook.py\u001b[0m in \u001b[0;36m_store_workbook\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m             xlsx_file = ZipFile(self.filename, \"w\", compression=ZIP_DEFLATED,\n\u001b[0;32m--> 632\u001b[0;31m                                 allowZip64=self.allow_zip64)\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gs://workshop_trial_artifact_store_pp/data/training/X_train.xlsx'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileCreateError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-27f55a1c0830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mwriter_cmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pd.ExcelWriter('{}/{}.xlsx', engine='xlsxwriter').save()\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m#eval(writer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#writer.save()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_xlsxwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mSave\u001b[0m \u001b[0mworkbook\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     def write_cells(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xlsxwriter/workbook.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileCreateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLargeZipFile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 raise FileSizeError(\"Filesize would require ZIP64 extensions. \"\n",
      "\u001b[0;31mFileCreateError\u001b[0m: [Errno 2] No such file or directory: 'gs://workshop_trial_artifact_store_pp/data/training/X_train.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "\n",
    "\n",
    "for x in save_list_train:\n",
    "    print(\"training_dataset\", training_dataset)\n",
    "    obj = pd.DataFrame(globals()[x])\n",
    "    #cmd = \"obj.to_csv('../data/{}.csv')\".format(x)\n",
    "    #cmd = \"obj.to_excel('/home/jupyter/aiplatform/data/{}.csv')\".format(x)\n",
    "    #writer_cmd = pd.ExcelWriter('{}.xlsx', engine='xlsxwriter')\n",
    "    #cmd = \"obj.to_excel(pd.ExcelWriter('{}/{}.xlsx', engine='xlsxwriter'), sheet_name='data')\".format(training_dataset, x)\n",
    "    eval(cmd)\n",
    "    writer_cmd = \"pd.ExcelWriter('{}/{}.xlsx', engine='xlsxwriter').save()\".format(training_dataset, x)\n",
    "    eval(writer_cmd)\n",
    "    #eval(writer)\n",
    "    #writer.save()\n",
    "    #cmd = \"obj.to_excel('/home/jupyter/aiplatform/data/{}.xlsx')\".format(x)\n",
    "    #cmd = \"obj.to_csv('gs://workshop_trial_artifact_store_pp/data/training/{}.csv')\".format(x)\n",
    "    #eval(cmd)\n",
    "\n",
    "    \n",
    "for x in save_list_train:\n",
    "        print(\"training_dataset\", training_dataset)\n",
    "        obj = pd.DataFrame(globals()[x])\n",
    "        cmd = \"obj.to_csv('{}/{}.xlsx')\".format(training_dataset, x)\n",
    "        eval(cmd)\n",
    "    #    #cmd = \"obj.to_csv('../data/{}.csv')\".format(x)\n",
    "    #    cmd = \"obj.to_excel('{}/{}.xlsx')\".format(training_dataset, x)\n",
    "    #    eval(cmd)\n",
    "    \n",
    "    for x in save_list_validate:\n",
    "        print(\"validation_dataset\", validation_dataset)\n",
    "        obj = pd.DataFrame(globals()[x])\n",
    "        cmd = \"obj.to_csv('{}/{}.csv')\".format(validation_dataset, x)\n",
    "        eval(cmd)    \n",
    "    #    #cmd = \"obj.to_csv('../data/{}.csv')\".format(x)\n",
    "    #    cmd = \"obj.to_excel('{}/{}.xlsx')\".format(validation_dataset, x)\n",
    "    #    eval(cmd)    \n",
    "    \n",
    "    for x in save_list_test:\n",
    "        print(\"testing_dataset\", testing_dataset)\n",
    "        obj = pd.DataFrame(globals()[x])\n",
    "        cmd = \"obj.to_csv('{}/{}.csv')\".format(testing_dataset, x)\n",
    "        eval(cmd)\n",
    "    #    #cmd = \"obj.to_csv('../data/{}.csv')\".format(x)\n",
    "    #    cmd = \"obj.to_excel('{}/{}.xlsx')\".format(testing_dataset, x)\n",
    "    #    eval(cmd)    \n",
    "        \n",
    "\n",
    "    \n",
    "    #df1.to_excel(writer, sheet_name='Sheet1')\n",
    "#df2.to_excel(writer, sheet_name='Sheet2')\n",
    "#df3.to_excel(writer, sheet_name='Sheet3')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "#writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    var_type_filter = [x in ['independent'] for x in meta_data['variable type']]\n",
    "    var_dtype_filter = (data.dtypes == 'float64') | (data.dtypes == 'int64')\n",
    "\n",
    "    numeric_vars = (var_type_filter & var_dtype_filter).values\n",
    "    numeric_x_data = data[data.columns[numeric_vars]]\n",
    "\n",
    "    #things to try to predict\n",
    "    y_data = data[data.columns[(meta_data['target'] == 1).values]]\n",
    "\n",
    "    #meta data about variables\n",
    "    meta_data = meta_data.query('name in {}'.format(list(data.columns[numeric_vars].values))).set_index('name')\n",
    " \n",
    "    #Variables which will be used to build the model\n",
    "    ####data.columns[numeric_vars].values\n",
    "        \n",
    "\n",
    "\n",
    "    model_target = 'Run_Performance' ## Select target for classification\n",
    "    \n",
    "    print(\"before splitting data\")\n",
    "    #maintain class balance\n",
    "    X_train, X_test, y_train, y_test = train_test_split(numeric_x_data, y_data, test_size=0.25, stratify = y_data[model_target], random_state=42)\n",
    "\n",
    "    #split train set to create a pseudo test or validation dataset\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.33, stratify= y_train[model_target], random_state=42)\n",
    "    \n",
    "    \n",
    "    print('The training, validation and test data contain {}, {} and {} rows respectively'.format(len(X_train),len(X_validate),len(X_test)))\n",
    "\n",
    "    save_list_train = ['X_train','y_train','meta_data']\n",
    "    save_list_validate = ['X_validate','y_validate','meta_data']\n",
    "    save_list_test = ['X_test','y_test','meta_data']\n",
    "\n",
    "    for x in save_list_train:\n",
    "        print(\"training_dataset\", training_dataset)\n",
    "        obj = pd.DataFrame(globals()[x])\n",
    "        #cmd = \"obj.to_csv('../data/{}.csv')\".format(x)\n",
    "        #cmd = \"obj.to_csv('/home/jupyter/aiplatform/data/{}.csv')\".format(x)\n",
    "        cmd = \"obj.to_excel('{}/{}.xlsx')\".format(training_dataset, x)\n",
    "        eval(cmd)\n",
    "    \n",
    "    for x in save_list_validate:\n",
    "        print(\"validation_dataset\", validation_dataset)\n",
    "        obj = pd.DataFrame(globals()[x])\n",
    "        #cmd = \"obj.to_csv('../data/{}.csv')\".format(x)\n",
    "        #cmd = \"obj.to_csv('/home/jupyter/aiplatform/data/{}.csv')\".format(x)\n",
    "        cmd = \"obj.to_excel('{}/{}.xlsx')\".format(validation_dataset, x)\n",
    "        eval(cmd)    \n",
    "    \n",
    "    for x in save_list_test:\n",
    "        print(\"testing_dataset\", testing_dataset)\n",
    "        obj = pd.DataFrame(globals()[x])\n",
    "        #cmd = \"obj.to_csv('../data/{}.csv')\".format(x)\n",
    "        #cmd = \"obj.to_csv('/home/jupyter/aiplatform/data/{}.csv')\".format(x)\n",
    "        cmd = \"obj.to_excel('{}/{}.xlsx')\".format(testing_dataset, x)\n",
    "        eval(cmd)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Create some Pandas dataframes from some data.\n",
    "df1 = pd.DataFrame({'Data': [11, 12, 13, 14]})\n",
    "df2 = pd.DataFrame({'Data': [21, 22, 23, 24]})\n",
    "df3 = pd.DataFrame({'Data': [31, 32, 33, 34]})\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('my.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "df1.to_excel(writer, sheet_name='Sheet1')\n",
    "df2.to_excel(writer, sheet_name='Sheet2')\n",
    "df3.to_excel(writer, sheet_name='Sheet3')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('classifier',\n",
       "                 RandomForestClassifier(max_depth=10, min_samples_leaf=2,\n",
       "                                        n_estimators=5))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#simple sklearn impute and scale numeric pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "alpha = 'alpha'\n",
    "max_iter = 2\n",
    "\n",
    "classifier = RandomForestClassifier(\n",
    "            n_estimators=3,\n",
    "            max_depth=5,\n",
    "            min_samples_leaf=2\n",
    " )\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "  ('imputer', SimpleImputer(strategy='median')),\n",
    "  ('scaler', StandardScaler()),\n",
    " ])\n",
    "\n",
    "\n",
    "transform_list = []\n",
    "  # If there exist numerical columns\n",
    "transform_list.extend([\n",
    "    ('numeric', numeric_transformer)\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transform_list)\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "#auto scale\n",
    "scaler = StandardScaler()\n",
    "\n",
    "estimator = Pipeline([\n",
    "  ('imputer', imputer),\n",
    "  ('scaler', scaler),\n",
    "  ('classifier', classifier),\n",
    "])\n",
    "\n",
    "#prepare data for modeling\n",
    "#use the pipeline created above\n",
    "#_X_train = pipe.fit_transform(X_train)\n",
    "#_y_train = y_train[model_target]    ## selected target label for prediction\n",
    "#_X_test = pipe.fit_transform(X_validate)\n",
    "#_y_test = y_validate[model_target]\n",
    "\n",
    "#_X_train = pipe.fit_transform(X_train)\n",
    "_X_train = X_train    \n",
    "_y_train = y_train[model_target]    ## selected target label for prediction\n",
    "_X_test = X_validate\n",
    "_y_test = y_validate[model_target]\n",
    "\n",
    "#print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "\n",
    "estimator.set_params(classifier__n_estimators=5, classifier__max_depth=10, classifier__min_samples_leaf=2 ) \n",
    "#pipeline.fit(X_train, y_train)\n",
    "estimator.fit(_X_train, _y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
