{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import uuid\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "#ENDPOINT = '<YOUR_ENDPOINT>' e.g. '337dd39580cbcbd2-dot-us-central2.pipelines.googleusercontent.com' (once you create Kubeflow pipeline - use settings)\n",
    "INPUT_FILE = 'gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx'\n",
    "ARTIFACT_STORE_URI = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'X_train.xlsx')\n",
    "TRAINING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'training')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'X_validate.xlsx')\n",
    "VALIDATION_FILE_DIR='{}/{}'.format(DATA_ROOT, 'validation')\n",
    "TESTING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'testing')\n",
    "TESTING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'testing', 'X_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Write the training APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_FOLDER = 'train_image'\n",
    "os.makedirs(TRAIN_IMAGE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAIN_IMAGE_FOLDER}/train.py\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split  ## from analysis\n",
    "\n",
    "#simple sklearn impute and scale numeric pipeline\n",
    "from sklearn.pipeline import Pipeline ## from analysis\n",
    "from sklearn.impute import SimpleImputer ## from analysis\n",
    "from sklearn.preprocessing import StandardScaler ## from analysis\n",
    "import numpy as np ## from analysis\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, input_file, training_dataset, validation_dataset, testing_dataset, n_estimators, max_depth, min_samples_leaf, max_features, min_samples_split, class_weight, max_leaf_nodes, random_state, hptune, bootstrap):\n",
    "\n",
    "   \n",
    "    obj = input_file\n",
    "    print(\"obj\", obj)\n",
    "    data = pd.read_excel(obj,sheet_name='data') \n",
    "    meta_data = pd.read_excel(obj,sheet_name='meta data') \n",
    "    \n",
    "    ## Preprocess    \n",
    "    #Prepare data for analysis\n",
    "    #Split out numeric from categorical varibles\n",
    "\n",
    "    ##var_type_filter = [x in ['physiological','biochemical','process'] for x in meta_data['variable type']]\n",
    "    var_type_filter = [x in ['independent'] for x in meta_data['variable type']]\n",
    "    var_dtype_filter = (data.dtypes == 'float64') | (data.dtypes == 'int64')\n",
    "\n",
    "    numeric_vars = (var_type_filter & var_dtype_filter).values\n",
    "    numeric_x_data = data[data.columns[numeric_vars]]\n",
    "\n",
    "    #things to try to predict\n",
    "    y_data = data[data.columns[(meta_data['target'] == 1).values]]\n",
    "\n",
    "    #meta data about variables\n",
    "    meta_data = meta_data.query('name in {}'.format(list(data.columns[numeric_vars].values))).set_index('name')\n",
    " \n",
    "    \n",
    "    \n",
    "    #Variables which will be used to build the model\n",
    "    model_target = 'Run_Performance' ## Select target for classification\n",
    "    \n",
    "    y_data = data[[model_target]]\n",
    "    \n",
    "    \n",
    "    #maintain class balance\n",
    "    X_train, X_test, y_train, y_test = train_test_split(numeric_x_data, y_data, test_size=0.25, stratify = y_data[model_target], random_state=42)\n",
    "\n",
    "    #split train set to create a pseudo test or validation dataset\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.33, stratify= y_train[model_target], random_state=42)\n",
    "    \n",
    "    \n",
    "    print('The training, validation and test data contain {}, {} and {} rows respectively'.format(len(X_train),len(X_validate),len(X_test)))\n",
    "\n",
    "    training_file = 'X_train'\n",
    "    testing_file = 'X_test'\n",
    "    validation_file = 'X_validate'\n",
    "    serving_file = 'X_serving'\n",
    "  \n",
    "    Xy_train = X_train.join(y_train)\n",
    "    cmd = \"Xy_train.to_csv('{}/{}.csv', index=False)\".format(training_dataset, training_file)\n",
    "    eval(cmd)\n",
    "    print(\"Saved training files for later..\")\n",
    "    Xy_test =X_test.join(y_test)\n",
    "    cmd = \"Xy_test.to_csv('{}/{}.csv', index=False)\".format(testing_dataset, testing_file)\n",
    "    print(\"Saved testing files for later..\")\n",
    "    eval(cmd)\n",
    "  \n",
    "    cmd = \"X_test.to_csv('{}/{}.csv', index=False)\".format(testing_dataset, serving_file)\n",
    "    print(\"Saved serving instance files for later..\")\n",
    "    eval(cmd)\n",
    "\n",
    "    Xy_validate =X_validate.join(y_validate)\n",
    "    cmd = \"X_validate.to_csv('{}/{}.csv', index=False)\".format(validation_dataset, validation_file)\n",
    "    eval(cmd)\n",
    "    print(\"Saved validation files for later..\")\n",
    "\n",
    "    \n",
    "    if not hptune:\n",
    "        #df_train = pd.concat([df_train, df_validation])\n",
    "        X_train = pd.concat([X_train, X_validate])\n",
    "        y_train = pd.concat([y_train, y_validate])\n",
    "        \n",
    "\n",
    "\n",
    "    ## Train, optimize and validate predictive model\n",
    "    ### Train\n",
    "\n",
    "\n",
    "\n",
    "    classifier = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                max_features=max_features,\n",
    "                min_samples_split=min_samples_split,\n",
    "                class_weight=class_weight,\n",
    "                max_leaf_nodes=max_leaf_nodes,\n",
    "                random_state=random_state,\n",
    "                bootstrap=bootstrap\n",
    "\n",
    "     )\n",
    "\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    #auto scale\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    estimator = Pipeline([\n",
    "      ('imputer', imputer),\n",
    "      ('scaler', scaler),\n",
    "      ('classifier', classifier),\n",
    "    ])\n",
    "    \n",
    "\n",
    "    #prepare data for modeling\n",
    "    #use the pipeline created above\n",
    "\n",
    "    #_X_train = pipe.fit_transform(X_train)\n",
    "    _X_train = X_train    \n",
    "    _y_train = y_train[model_target]    ## selected target label for prediction\n",
    "    _X_validate = X_validate\n",
    "    _y_validate = y_validate[model_target]\n",
    "\n",
    "    _X_test = X_test\n",
    "    _y_test = y_test[model_target]\n",
    "\n",
    "\n",
    "    \n",
    "    print('Starting training: n_estimators={}, max_depth={}, min_samples_leaf={}, max_features={}, min_samples_split={}, class_weight={}, max_leaf_nodes={}, random_state={}, hptune={}, bootstrap={}'.format(n_estimators, max_depth, min_samples_leaf, max_features, min_samples_split, class_weight, max_leaf_nodes, random_state, hptune, bootstrap))\n",
    "\n",
    "    #estimator.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter) \n",
    "    estimator.set_params(classifier__n_estimators=n_estimators, classifier__max_depth=max_depth, classifier__min_samples_leaf=min_samples_leaf, \n",
    "                         classifier__max_features=max_features, classifier__min_samples_split=min_samples_split, classifier__class_weight=class_weight,\n",
    "                         classifier__max_leaf_nodes=max_leaf_nodes, classifier__random_state=random_state, classifier__bootstrap=bootstrap) \n",
    "    #pipeline.fit(X_train, y_train)\n",
    "    estimator.fit(_X_train, _y_train)\n",
    "\n",
    "    \n",
    "    if hptune:\n",
    "        accuracy = estimator.score(_X_validate, _y_validate)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(estimator, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Package the script into a docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAIN_IMAGE_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $TRAINER_IMAGE $TRAIN_IMAGE_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Run hyperparameter tuning jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAIN_IMAGE_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 4\n",
    "    maxParallelTrials: 4\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE\n",
    "    algorithm: RANDOM_SEARCH\n",
    "    params:\n",
    "    - parameterName: n_estimators\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 200\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: max_depth\n",
    "      type: INTEGER\n",
    "      minValue: 3\n",
    "      maxValue: 100\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: min_samples_leaf\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 500\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: max_features\n",
    "      type: DOUBLE\n",
    "      minValue: 0.5\n",
    "      maxValue: 1.0\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: min_samples_split\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          2,\n",
    "          5,\n",
    "          10\n",
    "      ]\n",
    "    - parameterName: class_weight\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"balanced\",\n",
    "          \"balanced_subsample\"\n",
    "      ]\n",
    "    - parameterName: max_leaf_nodes\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 500\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: random_state\n",
    "      type: INTEGER\n",
    "      minValue: 35\n",
    "      maxValue: 75\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: bootstrap\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"TRUE\",\n",
    "          \"FALSE\"\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'X_train.xlsx')\n",
    "TRAINING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'training')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'X_validate.xlsx')\n",
    "VALIDATION_FILE_DIR='{}/{}'.format(DATA_ROOT, 'validation')\n",
    "TESTING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter Tuning Job\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "  \n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$TRAINER_IMAGE \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAIN_IMAGE_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset=$TRAINING_FILE_DIR \\\n",
    "--validation_dataset=$VALIDATION_FILE_DIR \\\n",
    "--testing_dataset=$TESTING_FILE_DIR \\\n",
    "--input_file=$INPUT_FILE \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5b:Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning Job description\n",
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5c:Monitor the job - stream the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for streaming the logs\n",
    "#!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6c: Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['trainingOutput']['trials'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Retrain the model with the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the best parameters\n",
    "CLASS_WEIGHT = response['trainingOutput']['trials'][0]['hyperparameters']['class_weight']\n",
    "MAX_DEPTH = response['trainingOutput']['trials'][0]['hyperparameters']['max_depth']\n",
    "MAX_FEATURES = response['trainingOutput']['trials'][0]['hyperparameters']['max_features']\n",
    "MAX_LEAF_NODES = response['trainingOutput']['trials'][0]['hyperparameters']['max_leaf_nodes']\n",
    "MIN_SAMPLE_LEAF = response['trainingOutput']['trials'][0]['hyperparameters']['min_samples_leaf']\n",
    "MIN_SAMPLE_SPLIT = response['trainingOutput']['trials'][0]['hyperparameters']['min_samples_split']\n",
    "N_ESTIMATORS = response['trainingOutput']['trials'][0]['hyperparameters']['n_estimators']\n",
    "RANDOM_STATE = response['trainingOutput']['trials'][0]['hyperparameters']['random_state']\n",
    "BOOTSTRAP = response['trainingOutput']['trials'][0]['hyperparameters']['bootstrap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now select the best parameters for deploying the model\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "#BOOTSTRAP = \"TRUE\"\n",
    "#CLASS_WEIGHT = \"balanced_subsample\"\n",
    "#MAX_DEPTH = \"22\"\n",
    "#MAX_FEATURES = \"0.92715679885914515\"\n",
    "#MAX_LEAF_NODES = \"156\"\n",
    "#MIN_SAMPLE_LEAF = \"55\"\n",
    "#MIN_SAMPLE_SPLIT = \"5\"\n",
    "#N_ESTIMATORS = \"147\"\n",
    "#RANDOM_STATE = \"37\"\n",
    "\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$TRAINER_IMAGE \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "-- \\\n",
    "--training_dataset=$TRAINING_FILE_DIR \\\n",
    "--validation_dataset=$VALIDATION_FILE_DIR \\\n",
    "--testing_dataset=$TESTING_FILE_DIR \\\n",
    "--input_file=$INPUT_FILE \\\n",
    "--bootstrap=$BOOTSTRAP \\\n",
    "--class_weight=$CLASS_WEIGHT \\\n",
    "--max_depth=$MAX_DEPTH \\\n",
    "--max_features=$MAX_FEATURES \\\n",
    "--min_samples_leaf=$MIN_SAMPLE_LEAF \\\n",
    "--max_leaf_nodes=$MAX_LEAF_NODES \\\n",
    "--min_samples_split=$MIN_SAMPLE_SPLIT \\\n",
    "--n_estimators=$N_ESTIMATORS \\\n",
    "--random_state=$RANDOM_STATE \\\n",
    "--nohptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Model Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'amyris_RFClassifiervRFCL'\n",
    "labels = \"task=classifier,domain=biotech\"\n",
    "filter = 'name:{}'.format(model_name)\n",
    "models = !(gcloud ai-platform models list --filter={filter} --format='value(name)')\n",
    "\n",
    "#if not models:\n",
    "!gcloud ai-platform models create  $model_name \\\n",
    "    --regions=$REGION \\\n",
    "    --labels=$labels\n",
    "#else:\n",
    "#    print(\"Model: {} already exists.\".format(models[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 'v01'\n",
    "filter = 'name:{}'.format(model_version)\n",
    "# versions = !(gcloud ai-platform versions list --model={model_name} --format='value(name)' --filter={filter})\n",
    "\n",
    "\n",
    "#if not versions:\n",
    "!gcloud ai-platform versions create {model_version} \\\n",
    "    --model={model_name} \\\n",
    "    --origin=$JOB_DIR \\\n",
    "    --runtime-version=1.15 \\\n",
    "    --framework=scikit-learn \\\n",
    "    --python-version=3.7\n",
    "#else:\n",
    "#     print(\"Model version: {} already exists.\".format(versions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Predict with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input file is in json format. Use that for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'X_train.xlsx')\n",
    "TRAINING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'training')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'X_validate.xlsx')\n",
    "VALIDATION_FILE_DIR='{}/{}'.format(DATA_ROOT, 'validation')\n",
    "TESTING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input file with JSON formated instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "input_file = 'serving_instance_1a.json'\n",
    "X_test_file= '{}/{}'.format(TESTING_FILE_DIR,'X_serving.csv')\n",
    "X_test_file\n",
    "##'gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv'\n",
    "##'gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv'\n",
    "\n",
    "#X_test = pd.read_csv('gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv')\n",
    "X_test = pd.read_csv(X_test_file)\n",
    "\n",
    "X_test.head()\n",
    "\n",
    "with open(input_file, 'w') as f:\n",
    "    for index, row in X_test.head().iterrows():\n",
    "        f.write(json.dumps(list(row.values)))\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "--model $model_name \\\n",
    "--version $model_version \\\n",
    "--json-instances $input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous training pipeline with KFP and Cloud AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_FOLDER = 'pipeline'\n",
    "os.makedirs(PIPELINE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./pipeline/amyris_pipeline.py\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"KFP pipeline orchestrating BigQuery and Cloud AI Platform services.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from helper_components import evaluate_model\n",
    "from helper_components import retrieve_best_run\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "USE_KFP_SA = os.getenv('USE_KFP_SA')\n",
    "TRAINING_FILE_PATH = 'gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx'\n",
    "INPUT_FILE = 'gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx'\n",
    "TRAINING_FILE_DIR = 'gs://workshop_trial_artifact_store_pp/data/training'\n",
    "TESTING_FILE_DIR = 'gs://workshop_trial_artifact_store_pp/data/testing'\n",
    "VALIDATION_FILE_DIR = 'gs://workshop_trial_artifact_store_pp/data/validation'\n",
    "\n",
    "\n",
    "TESTING_FILE_PATH = 'gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv'\n",
    "# VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "# TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "# SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 3,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"algorithm\": \"RANDOM_SEARCH\",\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"n_estimators\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 10,\n",
    "                \"maxValue\": 200,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"max_leaf_nodes\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 10,\n",
    "                \"maxValue\": 500,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"max_depth\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 3,\n",
    "                \"maxValue\": 20,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"min_samples_split\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [2,5,10]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"min_samples_leaf\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 10,\n",
    "                \"maxValue\": 500,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"max_features\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.5,\n",
    "                \"maxValue\": 1.0,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },    \n",
    "            {\n",
    "                \"parameterName\": \"class_weight\",\n",
    "                \"type\": \"CATEGORICAL\",\n",
    "                \"categoricalValues\": [\n",
    "                              \"balanced\",\n",
    "                              \"balanced_subsample\"\n",
    "                          ]\n",
    "            },  \n",
    "\n",
    "             {\n",
    "                \"parameterName\": \"random_state\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 35,\n",
    "                \"maxValue\": 75,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"bootstrap\",\n",
    "                \"type\": \"CATEGORICAL\",\n",
    "                \"categoricalValues\": [\n",
    "                              \"TRUE\",\n",
    "                              \"FALSE\"\n",
    "                          ]\n",
    "            } \n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# # Helper functions\n",
    "# def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "#     \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "#     sampling_query_template = \"\"\"\n",
    "#          SELECT *\n",
    "#          FROM \n",
    "#              `{{ source_table }}` AS cover\n",
    "#          WHERE \n",
    "#          MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "#          \"\"\"\n",
    "#     query = Template(sampling_query_template).render(\n",
    "#         source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "#     return query\n",
    "\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "# bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(\n",
    "    retrieve_best_run, base_image=BASE_IMAGE)\n",
    "evaluate_model_op = func_to_container_op(evaluate_model, base_image=BASE_IMAGE)\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Amyris Classifier Training',\n",
    "    description='The pipeline training and deploying the Amyris classifierpipeline_yaml'\n",
    ")\n",
    "def amyris_train(project_id,\n",
    "                    region,\n",
    "                    gcs_root,\n",
    "                    evaluation_metric_name,\n",
    "                    evaluation_metric_threshold,\n",
    "                    model_id,\n",
    "                    version_id,\n",
    "                    replace_existing_version,\n",
    "                    hypertune_settings=HYPERTUNE_SETTINGS,\n",
    "                    dataset_location='US'):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    # Create the training split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "#     training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "#     create_training_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=training_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "#     # Create the validation split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "#     validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "#     create_validation_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=validation_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "    # Create the testing split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "#     testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "#     create_testing_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=testing_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "        #'--training_dataset_path',\n",
    "       # TRAINING_FILE_PATH,\n",
    "        '--training_dataset', TRAINING_FILE_DIR,\n",
    "        '--validation_dataset', VALIDATION_FILE_DIR,\n",
    "        '--testing_dataset', TESTING_FILE_DIR, \n",
    "        '--input_file', INPUT_FILE, \n",
    "        '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                                kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri='gcr.io/etl-project-datahub/trainer_image:latest',\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(\n",
    "            project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    train_args = [\n",
    "       # '--training_dataset_path',\n",
    "       #TRAINING_FILE_PATH,\n",
    "        '--training_dataset', TRAINING_FILE_DIR,\n",
    "        '--validation_dataset', VALIDATION_FILE_DIR,\n",
    "        '--testing_dataset',  TESTING_FILE_DIR, \n",
    "        '--input_file', INPUT_FILE, \n",
    "        '--n_estimators',get_best_trial.outputs['n_estimators'], \n",
    "        '--max_leaf_nodes',get_best_trial.outputs['max_leaf_nodes'], \n",
    "        '--max_depth',get_best_trial.outputs['max_depth'],\n",
    "        '--min_samples_split',get_best_trial.outputs['min_samples_split'],\n",
    "        '--bootstrap' ,get_best_trial.outputs['bootstrap'],\n",
    "        '--random_state' ,get_best_trial.outputs['random_state'],\n",
    "        '--max_features' ,get_best_trial.outputs['max_features'],\n",
    "        '--class_weight' ,get_best_trial.outputs['class_weight'],\n",
    "        '--min_samples_leaf',get_best_trial.outputs['min_samples_leaf'],\n",
    "        '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri='gcr.io/etl-project-datahub/trainer_image:latest',\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=TESTING_FILE_PATH,\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "        deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "    # Configure the pipeline to run using the service account defined\n",
    "      # in the user-gcp-sa k8s secret\n",
    "    if USE_KFP_SA == 'True':\n",
    "        kfp.dsl.get_pipeline_conf().add_op_transformer(\n",
    "              use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./pipeline/helper_components.py\n",
    "\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\"\"\"Helper components.\"\"\"\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def retrieve_best_run(project_id: str, job_id: str) -> NamedTuple('Outputs', [('metric_value', float), ('n_estimators', int),\n",
    "                            ('max_leaf_nodes', int), ('max_depth', int), ('min_samples_split', int), ('min_samples_leaf', int),\n",
    "                            ('bootstrap', str), ('random_state', int), ('max_features', float), ('class_weight', str)]):\n",
    "    \n",
    "    \n",
    "    \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "    from googleapiclient import discovery\n",
    "    from googleapiclient import errors\n",
    "    \n",
    "    ml = discovery.build('ml', 'v1')\n",
    "\n",
    "    job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "    request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "    try:\n",
    "        response = request.execute()\n",
    "    except errors.HttpError as err:\n",
    "        print(err)\n",
    "    except:\n",
    "        print('Unexpected error')\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "    metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "\n",
    "    n_estimators = int(best_trial['hyperparameters']['n_estimators'])\n",
    "    max_leaf_nodes = int(best_trial['hyperparameters']['max_leaf_nodes'])\n",
    "    max_depth = int(best_trial['hyperparameters']['max_depth'])\n",
    "    min_samples_split = int(best_trial['hyperparameters']['min_samples_split'])\n",
    "    min_samples_leaf = int(best_trial['hyperparameters']['min_samples_leaf'])\n",
    "    bootstrap = str(best_trial['hyperparameters']['bootstrap'])\n",
    "    random_state = int(best_trial['hyperparameters']['random_state'])\n",
    "    max_features = float(best_trial['hyperparameters']['max_features'])\n",
    "    class_weight = str(best_trial['hyperparameters']['class_weight'])\n",
    "        \n",
    "    return (metric_value, n_estimators, max_leaf_nodes, max_depth, min_samples_split, min_samples_leaf, bootstrap, random_state,max_features, class_weight )\n",
    "\n",
    "\n",
    "def evaluate_model(dataset_path: str, model_path: str, metric_name: str) -> NamedTuple('Outputs', [('metric_name', str), ('metric_value', float),\n",
    "                            ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \n",
    "    \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "    df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "    X_test = df_test.drop('Run_Performance', axis=1)\n",
    "    y_test = df_test['Run_Performance']\n",
    "\n",
    "    # Copy the model from GCS\n",
    "    model_filename = 'model.pkl'\n",
    "    gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "    print(gcs_model_filepath)\n",
    "    subprocess.check_call(['gsutil', 'cp', gcs_model_filepath, model_filename],\n",
    "                        stderr=sys.stdout)\n",
    "\n",
    "    with open(model_filename, 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "        \n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if metric_name == 'accuracy':\n",
    "        metric_value = accuracy_score(y_test, y_hat)\n",
    "    elif metric_name == 'recall':\n",
    "        metric_value = recall_score(y_test, y_hat)\n",
    "    else:\n",
    "        metric_name = 'N/A'\n",
    "        metric_value = 0\n",
    "\n",
    "    # Export the metric\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "    }\n",
    "\n",
    "    return (metric_name, metric_value, json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat base_image/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat train_image/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an empty folder base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_FOLDER = 'base_image'\n",
    "os.makedirs(BASELINE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./base_image/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ENDPOINT = 'caa07ec40e216a8-dot-us-central2.pipelines.googleusercontent.com'\n",
    "#'19a5aed0f754a516-dot-us-central2.pipelines.googleusercontent.com'\n",
    "ARTIFACT_STORE_URI = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "client = kfp.Client(host='caa07ec40e216a8-dot-us-central2.pipelines.googleusercontent.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/amyris_pipeline.py --output amyris_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head amyris_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deploy pipeline package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME='amyris_pipeline_RF8'\n",
    "\n",
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "amyris_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID=##<Your Pipeline ID> e.g.'701161a5-26df-4c7a-81b6-4b6f5fc5ab99' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'amyrisRF_RF2'\n",
    "RUN_ID = 'Run_001'\n",
    "EVALUATION_METRIC = 'accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD = '0.69'\n",
    "MODEL_ID = 'amyris_RFClassifiervRFC1' #'amyris_endtoendRF2'\n",
    "VERSION_ID = 'v01'\n",
    "REPLACE_EXISTING_VERSION = 'True'\n",
    "\n",
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$GCS_STAGING_PATH \\\n",
    "region=$REGION \\\n",
    "evaluation_metric_name=$EVALUATION_METRIC \\\n",
    "evaluation_metric_threshold=$EVALUATION_METRIC_THRESHOLD \\\n",
    "model_id=$MODEL_ID \\\n",
    "version_id=$VERSION_ID \\\n",
    "replace_existing_version=$REPLACE_EXISTING_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI/CD Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = 'caa07ec40e216a8-dot-us-central2.pipelines.googleusercontent.com' ##'<YOUR_ENDPOINT>'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFP_CLI_FOLDER = 'kfp-cli'\n",
    "os.makedirs(KFP_CLI_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {KFP_CLI_FOLDER}/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install kfp==0.2.5\n",
    "ENTRYPOINT [\"/bin/bash\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat kfp-cli/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='kfp-cli'\n",
    "TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --timeout 15m --tag {IMAGE_URI} kfp-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_ENDPOINT={},\\\n",
    "_TRAINER_IMAGE_NAME=trainer_image,\\\n",
    "_BASE_IMAGE_NAME=base_image,\\\n",
    "TAG_NAME=latest,\\\n",
    "_PIPELINE_FOLDER=.,\\\n",
    "_PIPELINE_DSL=amyris_pipeline.py,\\\n",
    "_PIPELINE_PACKAGE=amyris_pipeline.yaml,\\\n",
    "_PIPELINE_NAME=amyris_pipeline_RF8,\\\n",
    "_RUNTIME_VERSION=1.15,\\\n",
    "_PYTHON_VERSION=3.7,\\\n",
    "_USE_KFP_SA=True,\\\n",
    "_COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
    "\"\"\".format(ENDPOINT).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "# Submits a Cloud Build job that builds and deploys\n",
    "# the pipelines and pipelines components \n",
    "#\n",
    "# Build and deploy a TFX pipeline. This is an interim solution till tfx CLI fully \n",
    "# supports automated building and deploying.\n",
    "# \n",
    "\n",
    "steps:\n",
    "# Build the trainer image\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME', '.']\n",
    "  dir: $_PIPELINE_FOLDER/train_image\n",
    "  \n",
    "# Build the base image for lightweight components\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME', '.']\n",
    "  dir: $_PIPELINE_FOLDER/base_image\n",
    "\n",
    "# Compile the pipeline\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    dsl-compile --py $_PIPELINE_DSL --output $_PIPELINE_PACKAGE\n",
    "  env:\n",
    "  - 'BASE_IMAGE=gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME'\n",
    "  - 'TRAINER_IMAGE=gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME'\n",
    "  - 'RUNTIME_VERSION=$_RUNTIME_VERSION'\n",
    "  - 'PYTHON_VERSION=$_PYTHON_VERSION'\n",
    "  - 'COMPONENT_URL_SEARCH_PREFIX=$_COMPONENT_URL_SEARCH_PREFIX'\n",
    "  - 'USE_KFP_SA=$_USE_KFP_SA'\n",
    "  dir: $_PIPELINE_FOLDER/pipeline\n",
    "  \n",
    " # Upload the pipeline\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    kfp --endpoint $_ENDPOINT pipeline upload -p ${_PIPELINE_NAME}_$TAG_NAME $_PIPELINE_PACKAGE\n",
    "  dir: $_PIPELINE_FOLDER/pipeline\n",
    "\n",
    "\n",
    "# Push the images to Container Registry \n",
    "images: ['gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME', 'gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit . --config cloudbuild.yaml --substitutions {SUBSTITUTIONS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up GitHub integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
