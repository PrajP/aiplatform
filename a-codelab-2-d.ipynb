{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "#ENDPOINT = '<YOUR_ENDPOINT>' e.g. '337dd39580cbcbd2-dot-us-central2.pipelines.googleusercontent.com'\n",
    "INPUT_FILE = 'gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx'\n",
    "ARTIFACT_STORE_URI = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'X_train.xlsx')\n",
    "TRAINING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'training')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'X_validate.xlsx')\n",
    "VALIDATION_FILE_DIR='{}/{}'.format(DATA_ROOT, 'validation')\n",
    "TESTING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'testing')\n",
    "TESTING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'testing', 'X_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Write the training APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Write the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split  ## from analysis\n",
    "\n",
    "#simple sklearn impute and scale numeric pipeline\n",
    "from sklearn.pipeline import Pipeline ## from analysis\n",
    "from sklearn.impute import SimpleImputer ## from analysis\n",
    "from sklearn.preprocessing import StandardScaler ## from analysis\n",
    "import numpy as np ## from analysis\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, input_file, training_dataset, validation_dataset, testing_dataset, n_estimators, max_depth, min_samples_leaf, max_features, min_samples_split, class_weight, max_leaf_nodes, random_state, hptune, bootstrap):\n",
    "\n",
    "   \n",
    "    obj = input_file\n",
    "    print(\"obj\", obj)\n",
    "    data = pd.read_excel(obj,sheet_name='data') \n",
    "    meta_data = pd.read_excel(obj,sheet_name='meta data') \n",
    "    \n",
    "    ## Preprocess    \n",
    "    #Prepare data for analysis\n",
    "    #Split out numeric from categorical varibles\n",
    "\n",
    "    ##var_type_filter = [x in ['physiological','biochemical','process'] for x in meta_data['variable type']]\n",
    "    var_type_filter = [x in ['independent'] for x in meta_data['variable type']]\n",
    "    var_dtype_filter = (data.dtypes == 'float64') | (data.dtypes == 'int64')\n",
    "\n",
    "    numeric_vars = (var_type_filter & var_dtype_filter).values\n",
    "    numeric_x_data = data[data.columns[numeric_vars]]\n",
    "\n",
    "    #things to try to predict\n",
    "    y_data = data[data.columns[(meta_data['target'] == 1).values]]\n",
    "\n",
    "    #meta data about variables\n",
    "    meta_data = meta_data.query('name in {}'.format(list(data.columns[numeric_vars].values))).set_index('name')\n",
    " \n",
    "    \n",
    "    \n",
    "    #Variables which will be used to build the model\n",
    "    model_target = 'Run_Performance' ## Select target for classification\n",
    "    \n",
    "    y_data = data[[model_target]]\n",
    "    \n",
    "    \n",
    "    #maintain class balance\n",
    "    X_train, X_test, y_train, y_test = train_test_split(numeric_x_data, y_data, test_size=0.25, stratify = y_data[model_target], random_state=42)\n",
    "\n",
    "    #split train set to create a pseudo test or validation dataset\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.33, stratify= y_train[model_target], random_state=42)\n",
    "    \n",
    "    \n",
    "    print('The training, validation and test data contain {}, {} and {} rows respectively'.format(len(X_train),len(X_validate),len(X_test)))\n",
    "\n",
    "    training_file = 'X_train'\n",
    "    testing_file = 'X_test'\n",
    "    validation_file = 'X_validate'\n",
    "    serving_file = 'X_serving'\n",
    "  \n",
    "    Xy_train = X_train.join(y_train)\n",
    "    cmd = \"Xy_train.to_csv('{}/{}.csv', index=False)\".format(training_dataset, training_file)\n",
    "    eval(cmd)\n",
    "    print(\"Saved training files for later..\")\n",
    "    Xy_test =X_test.join(y_test)\n",
    "    cmd = \"Xy_test.to_csv('{}/{}.csv', index=False)\".format(testing_dataset, testing_file)\n",
    "    print(\"Saved testing files for later..\")\n",
    "    eval(cmd)\n",
    "  \n",
    "    cmd = \"X_test.to_csv('{}/{}.csv', index=False)\".format(testing_dataset, serving_file)\n",
    "    print(\"Saved serving instance files for later..\")\n",
    "    eval(cmd)\n",
    "\n",
    "    Xy_validate =X_validate.join(y_validate)\n",
    "    cmd = \"X_validate.to_csv('{}/{}.csv', index=False)\".format(validation_dataset, validation_file)\n",
    "    eval(cmd)\n",
    "    print(\"Saved validation files for later..\")\n",
    "\n",
    "    \n",
    "    if not hptune:\n",
    "        #df_train = pd.concat([df_train, df_validation])\n",
    "        X_train = pd.concat([X_train, X_validate])\n",
    "        y_train = pd.concat([y_train, y_validate])\n",
    "        \n",
    "\n",
    "\n",
    "    ## Train, optimize and validate predictive model\n",
    "    ### Train\n",
    "\n",
    "\n",
    "\n",
    "    classifier = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                max_features=max_features,\n",
    "                min_samples_split=min_samples_split,\n",
    "                class_weight=class_weight,\n",
    "                max_leaf_nodes=max_leaf_nodes,\n",
    "                random_state=random_state,\n",
    "                bootstrap=bootstrap\n",
    "\n",
    "     )\n",
    "\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    #auto scale\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    estimator = Pipeline([\n",
    "      ('imputer', imputer),\n",
    "      ('scaler', scaler),\n",
    "      ('classifier', classifier),\n",
    "    ])\n",
    "    \n",
    "\n",
    "    #prepare data for modeling\n",
    "    #use the pipeline created above\n",
    "\n",
    "    #_X_train = pipe.fit_transform(X_train)\n",
    "    _X_train = X_train    \n",
    "    _y_train = y_train[model_target]    ## selected target label for prediction\n",
    "    _X_validate = X_validate\n",
    "    _y_validate = y_validate[model_target]\n",
    "\n",
    "    _X_test = X_test\n",
    "    _y_test = y_test[model_target]\n",
    "\n",
    "\n",
    "    \n",
    "    print('Starting training: n_estimators={}, max_depth={}, min_samples_leaf={}, max_features={}, min_samples_split={}, class_weight={}, max_leaf_nodes={}, random_state={}, hptune={}, bootstrap={}'.format(n_estimators, max_depth, min_samples_leaf, max_features, min_samples_split, class_weight, max_leaf_nodes, random_state, hptune, bootstrap))\n",
    "\n",
    "    #estimator.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter) \n",
    "    estimator.set_params(classifier__n_estimators=n_estimators, classifier__max_depth=max_depth, classifier__min_samples_leaf=min_samples_leaf, \n",
    "                         classifier__max_features=max_features, classifier__min_samples_split=min_samples_split, classifier__class_weight=class_weight,\n",
    "                         classifier__max_leaf_nodes=max_leaf_nodes, classifier__random_state=random_state, classifier__bootstrap=bootstrap) \n",
    "    #pipeline.fit(X_train, y_train)\n",
    "    estimator.fit(_X_train, _y_train)\n",
    "\n",
    "    \n",
    "    if hptune:\n",
    "        accuracy = estimator.score(_X_validate, _y_validate)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(estimator, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Package the script into a docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 21 file(s) totalling 105.3 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://etl-project-datahub_cloudbuild/source/1599441013.51649-f0124bcd32aa4f2d8fafc985a9ea39fa.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/etl-project-datahub/builds/d8ce2747-8f95-4e43-a92b-e209fb008e53].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/d8ce2747-8f95-4e43-a92b-e209fb008e53?project=448067079266].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"d8ce2747-8f95-4e43-a92b-e209fb008e53\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://etl-project-datahub_cloudbuild/source/1599441013.51649-f0124bcd32aa4f2d8fafc985a9ea39fa.tgz#1599441013924331\n",
      "Copying gs://etl-project-datahub_cloudbuild/source/1599441013.51649-f0124bcd32aa4f2d8fafc985a9ea39fa.tgz#1599441013924331...\n",
      "/ [1 files][ 26.4 KiB/ 26.4 KiB]                                                \n",
      "Operation completed over 1 objects/26.4 KiB.                                     \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  124.4kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 8bbc75c5ece9\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=44ec0cf3a0c1a7102625fe4c52ad579a3fd236fcbe741b582150885377f2f215\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=b07b7f0f0779daf2455227806b824ec5754d45af195583a53c0b43f08af6cf70\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=49b374e01aafc74967cbee9f7249ef5570a7732a8fe9bd079b3eebaed4ee20da\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container 8bbc75c5ece9\n",
      " ---> 4cb7e2b5be8f\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in e10c27c8ccea\n",
      "Removing intermediate container e10c27c8ccea\n",
      " ---> 8f3be0640fb7\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 6248af4de5cb\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 2b4ef1f662bd\n",
      "Removing intermediate container 2b4ef1f662bd\n",
      " ---> 2f9d9efb1b3f\n",
      "Successfully built 2f9d9efb1b3f\n",
      "Successfully tagged gcr.io/etl-project-datahub/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/etl-project-datahub/trainer_image:latest\n",
      "The push refers to repository [gcr.io/etl-project-datahub/trainer_image]\n",
      "3755115a0aef: Preparing\n",
      "3b7529e4c8d7: Preparing\n",
      "fe63651ff77a: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "89212ed9ad75: Layer already exists\n",
      "c51fe61c6231: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "3b7529e4c8d7: Pushed\n",
      "3755115a0aef: Pushed\n",
      "7f996c16a28a: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "fe63651ff77a: Pushed\n",
      "latest: digest: sha256:63756419ac40872807b411a38f1b95703d0758f1bb8a04efc61009ba0c8b925c size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                              STATUS\n",
      "d8ce2747-8f95-4e43-a92b-e209fb008e53  2020-09-07T01:10:14+00:00  3M43S     gs://etl-project-datahub_cloudbuild/source/1599441013.51649-f0124bcd32aa4f2d8fafc985a9ea39fa.tgz  gcr.io/etl-project-datahub/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Run hyperparameter tuning jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the hyperparameter configuration file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall that the training code uses SGDClassifier. The training application has been designed to accept two hyperparameters that control SGDClassifier:\n",
    "\n",
    "* ##### CLASS_WEIGHT \n",
    "* ##### MAX_DEPTH \n",
    "* ##### MAX_FEATURES \n",
    "* ##### MAX_LEAF_NODES \n",
    "* ##### MIN_SAMPLE_LEAF \n",
    "* ##### MIN_SAMPLE_SPLIT \n",
    "* ##### N_ESTIMATORS \n",
    "* ##### RANDOM_STATE\n",
    "\n",
    "#### The below file configures AI Platform hypertuning to run up to 4 trials on up to three nodes and to choose from three discrete values of min_samples_split and the linear range between 10 and 200 for n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 4\n",
    "    maxParallelTrials: 4\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE\n",
    "    algorithm: RANDOM_SEARCH\n",
    "    params:\n",
    "    - parameterName: n_estimators\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 200\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: max_depth\n",
    "      type: INTEGER\n",
    "      minValue: 3\n",
    "      maxValue: 100\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: min_samples_leaf\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 500\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: max_features\n",
    "      type: DOUBLE\n",
    "      minValue: 0.5\n",
    "      maxValue: 1.0\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: min_samples_split\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          2,\n",
    "          5,\n",
    "          10\n",
    "      ]\n",
    "    - parameterName: class_weight\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"balanced\",\n",
    "          \"balanced_subsample\"\n",
    "      ]\n",
    "    - parameterName: max_leaf_nodes\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 500\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: random_state\n",
    "      type: INTEGER\n",
    "      minValue: 35\n",
    "      maxValue: 75\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: bootstrap\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"TRUE\",\n",
    "          \"FALSE\"\n",
    "      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5a: Start the hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "job_dir -> GCS Path for storing the job packages & model.\n",
    "\n",
    "training_dataset_path -> GCS path holding training dataset.\n",
    "\n",
    "validation_dataset_path -> GCS path holding validation dataset.\n",
    "\n",
    "alpha -> hyperparameter\n",
    "\n",
    "max_iter -> hyperparameter\n",
    "\n",
    "hptune -> variable to decide if hyperparameter tuning is to be done or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'X_train.xlsx')\n",
    "TRAINING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'training')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'X_validate.xlsx')\n",
    "VALIDATION_FILE_DIR='{}/{}'.format(DATA_ROOT, 'validation')\n",
    "TESTING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200907_011517] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200907_011517\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200907_011517\n",
      "jobId: JOB_20200907_011517\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameter Tuning Job\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "  \n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset=$TRAINING_FILE_DIR \\\n",
    "--validation_dataset=$VALIDATION_FILE_DIR \\\n",
    "--testing_dataset=$TESTING_FILE_DIR \\\n",
    "--input_file=$INPUT_FILE \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5b:Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2020-09-07T01:15:18Z'\n",
      "endTime: '2020-09-07T01:23:18Z'\n",
      "etag: bDyRO3yxZmY=\n",
      "jobId: JOB_20200907_011517\n",
      "startTime: '2020-09-07T01:15:20Z'\n",
      "state: SUCCEEDED\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --training_dataset=gs://workshop_trial_artifact_store_pp/data/training\n",
      "  - --validation_dataset=gs://workshop_trial_artifact_store_pp/data/validation\n",
      "  - --testing_dataset=gs://workshop_trial_artifact_store_pp/data/testing\n",
      "  - --input_file=gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx\n",
      "  - --hptune\n",
      "  hyperparameters:\n",
      "    algorithm: RANDOM_SEARCH\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: accuracy\n",
      "    maxParallelTrials: 4\n",
      "    maxTrials: 4\n",
      "    params:\n",
      "    - maxValue: 200.0\n",
      "      minValue: 10.0\n",
      "      parameterName: n_estimators\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 100.0\n",
      "      minValue: 3.0\n",
      "      parameterName: max_depth\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 500.0\n",
      "      minValue: 10.0\n",
      "      parameterName: min_samples_leaf\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 1.0\n",
      "      minValue: 0.5\n",
      "      parameterName: max_features\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: DOUBLE\n",
      "    - discreteValues:\n",
      "      - 2.0\n",
      "      - 5.0\n",
      "      - 10.0\n",
      "      parameterName: min_samples_split\n",
      "      type: DISCRETE\n",
      "    - categoricalValues:\n",
      "      - balanced\n",
      "      - balanced_subsample\n",
      "      parameterName: class_weight\n",
      "      type: CATEGORICAL\n",
      "    - maxValue: 500.0\n",
      "      minValue: 10.0\n",
      "      parameterName: max_leaf_nodes\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 75.0\n",
      "      minValue: 35.0\n",
      "      parameterName: random_state\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - categoricalValues:\n",
      "      - 'TRUE'\n",
      "      - 'FALSE'\n",
      "      parameterName: bootstrap\n",
      "      type: CATEGORICAL\n",
      "  jobDir: gs://workshop_trial_artifact_store_pp/jobs/JOB_20200907_011517\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/etl-project-datahub/trainer_image:latest\n",
      "  region: us-central1\n",
      "trainingOutput:\n",
      "  completedTrialCount: '4'\n",
      "  consumedMLUnits: 0.26\n",
      "  hyperparameterMetricTag: accuracy\n",
      "  isHyperparameterTuningJob: true\n",
      "  trials:\n",
      "  - endTime: '2020-09-07T01:22:47Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 0.817481\n",
      "      trainingStep: '1'\n",
      "    hyperparameters:\n",
      "      bootstrap: 'TRUE'\n",
      "      class_weight: balanced_subsample\n",
      "      max_depth: '14'\n",
      "      max_features: '0.87836496965356137'\n",
      "      max_leaf_nodes: '103'\n",
      "      min_samples_leaf: '104'\n",
      "      min_samples_split: '5'\n",
      "      n_estimators: '67'\n",
      "      random_state: '35'\n",
      "    startTime: '2020-09-07T01:15:58.165800177Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '4'\n",
      "  - endTime: '2020-09-07T01:22:37Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 0.673522\n",
      "      trainingStep: '1'\n",
      "    hyperparameters:\n",
      "      bootstrap: 'TRUE'\n",
      "      class_weight: balanced\n",
      "      max_depth: '11'\n",
      "      max_features: '0.52632284680770092'\n",
      "      max_leaf_nodes: '270'\n",
      "      min_samples_leaf: '411'\n",
      "      min_samples_split: '2'\n",
      "      n_estimators: '150'\n",
      "      random_state: '49'\n",
      "    startTime: '2020-09-07T01:15:58.165510495Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '1'\n",
      "  - endTime: '2020-09-07T01:22:38Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 0.326478\n",
      "      trainingStep: '1'\n",
      "    hyperparameters:\n",
      "      bootstrap: 'FALSE'\n",
      "      class_weight: balanced_subsample\n",
      "      max_depth: '27'\n",
      "      max_features: '0.88773469707107822'\n",
      "      max_leaf_nodes: '403'\n",
      "      min_samples_leaf: '436'\n",
      "      min_samples_split: '2'\n",
      "      n_estimators: '152'\n",
      "      random_state: '65'\n",
      "    startTime: '2020-09-07T01:15:58.165668971Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '2'\n",
      "  - endTime: '2020-09-07T01:22:37Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 0.326478\n",
      "      trainingStep: '1'\n",
      "    hyperparameters:\n",
      "      bootstrap: 'TRUE'\n",
      "      class_weight: balanced_subsample\n",
      "      max_depth: '45'\n",
      "      max_features: '0.87798710046158324'\n",
      "      max_leaf_nodes: '87'\n",
      "      min_samples_leaf: '355'\n",
      "      min_samples_split: '2'\n",
      "      n_estimators: '172'\n",
      "      random_state: '65'\n",
      "    startTime: '2020-09-07T01:15:58.165738687Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '3'\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20200907_011517?project=etl-project-datahub\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2FJOB_20200907_011517&project=etl-project-datahub\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning Job description\n",
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5c:Monitor the job - stream the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for streaming the logs\n",
    "#!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6c: Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20200907_011517',\n",
       " 'trainingInput': {'args': ['--training_dataset=gs://workshop_trial_artifact_store_pp/data/training',\n",
       "   '--validation_dataset=gs://workshop_trial_artifact_store_pp/data/validation',\n",
       "   '--testing_dataset=gs://workshop_trial_artifact_store_pp/data/testing',\n",
       "   '--input_file=gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx',\n",
       "   '--hptune'],\n",
       "  'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "   'params': [{'parameterName': 'n_estimators',\n",
       "     'minValue': 10,\n",
       "     'maxValue': 200,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'max_depth',\n",
       "     'minValue': 3,\n",
       "     'maxValue': 100,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'min_samples_leaf',\n",
       "     'minValue': 10,\n",
       "     'maxValue': 500,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'max_features',\n",
       "     'minValue': 0.5,\n",
       "     'maxValue': 1,\n",
       "     'type': 'DOUBLE',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'min_samples_split',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [2, 5, 10]},\n",
       "    {'parameterName': 'class_weight',\n",
       "     'type': 'CATEGORICAL',\n",
       "     'categoricalValues': ['balanced', 'balanced_subsample']},\n",
       "    {'parameterName': 'max_leaf_nodes',\n",
       "     'minValue': 10,\n",
       "     'maxValue': 500,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'random_state',\n",
       "     'minValue': 35,\n",
       "     'maxValue': 75,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'bootstrap',\n",
       "     'type': 'CATEGORICAL',\n",
       "     'categoricalValues': ['TRUE', 'FALSE']}],\n",
       "   'maxTrials': 4,\n",
       "   'maxParallelTrials': 4,\n",
       "   'hyperparameterMetricTag': 'accuracy',\n",
       "   'enableTrialEarlyStopping': True,\n",
       "   'algorithm': 'RANDOM_SEARCH'},\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://workshop_trial_artifact_store_pp/jobs/JOB_20200907_011517',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/etl-project-datahub/trainer_image:latest'}},\n",
       " 'createTime': '2020-09-07T01:15:18Z',\n",
       " 'startTime': '2020-09-07T01:15:20Z',\n",
       " 'endTime': '2020-09-07T01:23:18Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingOutput': {'completedTrialCount': '4',\n",
       "  'trials': [{'trialId': '4',\n",
       "    'hyperparameters': {'bootstrap': 'TRUE',\n",
       "     'max_features': '0.87836496965356137',\n",
       "     'min_samples_split': '5',\n",
       "     'random_state': '35',\n",
       "     'max_depth': '14',\n",
       "     'max_leaf_nodes': '103',\n",
       "     'class_weight': 'balanced_subsample',\n",
       "     'min_samples_leaf': '104',\n",
       "     'n_estimators': '67'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.8174807197943444},\n",
       "    'startTime': '2020-09-07T01:15:58.165800177Z',\n",
       "    'endTime': '2020-09-07T01:22:47Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '1',\n",
       "    'hyperparameters': {'n_estimators': '150',\n",
       "     'max_leaf_nodes': '270',\n",
       "     'min_samples_split': '2',\n",
       "     'random_state': '49',\n",
       "     'max_features': '0.52632284680770092',\n",
       "     'min_samples_leaf': '411',\n",
       "     'class_weight': 'balanced',\n",
       "     'bootstrap': 'TRUE',\n",
       "     'max_depth': '11'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6735218508997429},\n",
       "    'startTime': '2020-09-07T01:15:58.165510495Z',\n",
       "    'endTime': '2020-09-07T01:22:37Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '2',\n",
       "    'hyperparameters': {'n_estimators': '152',\n",
       "     'max_leaf_nodes': '403',\n",
       "     'min_samples_split': '2',\n",
       "     'random_state': '65',\n",
       "     'max_features': '0.88773469707107822',\n",
       "     'max_depth': '27',\n",
       "     'bootstrap': 'FALSE',\n",
       "     'min_samples_leaf': '436',\n",
       "     'class_weight': 'balanced_subsample'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.3264781491002571},\n",
       "    'startTime': '2020-09-07T01:15:58.165668971Z',\n",
       "    'endTime': '2020-09-07T01:22:38Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '3',\n",
       "    'hyperparameters': {'class_weight': 'balanced_subsample',\n",
       "     'max_features': '0.87798710046158324',\n",
       "     'min_samples_split': '2',\n",
       "     'bootstrap': 'TRUE',\n",
       "     'random_state': '65',\n",
       "     'max_depth': '45',\n",
       "     'n_estimators': '172',\n",
       "     'max_leaf_nodes': '87',\n",
       "     'min_samples_leaf': '355'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.3264781491002571},\n",
       "    'startTime': '2020-09-07T01:15:58.165738687Z',\n",
       "    'endTime': '2020-09-07T01:22:37Z',\n",
       "    'state': 'SUCCEEDED'}],\n",
       "  'consumedMLUnits': 0.26,\n",
       "  'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'accuracy'},\n",
       " 'etag': 'bDyRO3yxZmY='}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trialId': '4',\n",
       " 'hyperparameters': {'bootstrap': 'TRUE',\n",
       "  'max_features': '0.87836496965356137',\n",
       "  'min_samples_split': '5',\n",
       "  'random_state': '35',\n",
       "  'max_depth': '14',\n",
       "  'max_leaf_nodes': '103',\n",
       "  'class_weight': 'balanced_subsample',\n",
       "  'min_samples_leaf': '104',\n",
       "  'n_estimators': '67'},\n",
       " 'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.8174807197943444},\n",
       " 'startTime': '2020-09-07T01:15:58.165800177Z',\n",
       " 'endTime': '2020-09-07T01:22:47Z',\n",
       " 'state': 'SUCCEEDED'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['trainingOutput']['trials'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Retrain the model with the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the best parameters\n",
    "CLASS_WEIGHT = response['trainingOutput']['trials'][0]['hyperparameters']['class_weight']\n",
    "MAX_DEPTH = response['trainingOutput']['trials'][0]['hyperparameters']['max_depth']\n",
    "MAX_FEATURES = response['trainingOutput']['trials'][0]['hyperparameters']['max_features']\n",
    "MAX_LEAF_NODES = response['trainingOutput']['trials'][0]['hyperparameters']['max_leaf_nodes']\n",
    "MIN_SAMPLE_LEAF = response['trainingOutput']['trials'][0]['hyperparameters']['min_samples_leaf']\n",
    "MIN_SAMPLE_SPLIT = response['trainingOutput']['trials'][0]['hyperparameters']['min_samples_split']\n",
    "N_ESTIMATORS = response['trainingOutput']['trials'][0]['hyperparameters']['n_estimators']\n",
    "RANDOM_STATE = response['trainingOutput']['trials'][0]['hyperparameters']['random_state']\n",
    "BOOTSTRAP = response['trainingOutput']['trials'][0]['hyperparameters']['bootstrap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200907_012452] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200907_012452\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200907_012452\n",
      "jobId: JOB_20200907_012452\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "#Now select the best parameters for deploying the model\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "#BOOTSTRAP = \"TRUE\"\n",
    "#CLASS_WEIGHT = \"balanced_subsample\"\n",
    "#MAX_DEPTH = \"22\"\n",
    "#MAX_FEATURES = \"0.92715679885914515\"\n",
    "#MAX_LEAF_NODES = \"156\"\n",
    "#MIN_SAMPLE_LEAF = \"55\"\n",
    "#MIN_SAMPLE_SPLIT = \"5\"\n",
    "#N_ESTIMATORS = \"147\"\n",
    "#RANDOM_STATE = \"37\"\n",
    "\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "-- \\\n",
    "--training_dataset=$TRAINING_FILE_DIR \\\n",
    "--validation_dataset=$VALIDATION_FILE_DIR \\\n",
    "--testing_dataset=$TESTING_FILE_DIR \\\n",
    "--input_file=$INPUT_FILE \\\n",
    "--bootstrap=$BOOTSTRAP \\\n",
    "--class_weight=$CLASS_WEIGHT \\\n",
    "--max_depth=$MAX_DEPTH \\\n",
    "--max_features=$MAX_FEATURES \\\n",
    "--min_samples_leaf=$MIN_SAMPLE_LEAF \\\n",
    "--max_leaf_nodes=$MAX_LEAF_NODES \\\n",
    "--min_samples_split=$MIN_SAMPLE_SPLIT \\\n",
    "--n_estimators=$N_ESTIMATORS \\\n",
    "--random_state=$RANDOM_STATE \\\n",
    "--nohptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20200907_012452',\n",
       " 'trainingInput': {'args': ['--training_dataset=gs://workshop_trial_artifact_store_pp/data/training',\n",
       "   '--validation_dataset=gs://workshop_trial_artifact_store_pp/data/validation',\n",
       "   '--testing_dataset=gs://workshop_trial_artifact_store_pp/data/testing',\n",
       "   '--input_file=gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx',\n",
       "   '--bootstrap=TRUE',\n",
       "   '--class_weight=balanced_subsample',\n",
       "   '--max_depth=14',\n",
       "   '--max_features=0.87836496965356137',\n",
       "   '--min_samples_leaf=104',\n",
       "   '--max_leaf_nodes=103',\n",
       "   '--min_samples_split=5',\n",
       "   '--n_estimators=67',\n",
       "   '--random_state=35',\n",
       "   '--nohptune'],\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://workshop_trial_artifact_store_pp/jobs/JOB_20200907_012452',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/etl-project-datahub/trainer_image:latest'}},\n",
       " 'createTime': '2020-09-07T01:24:54Z',\n",
       " 'startTime': '2020-09-07T01:29:12Z',\n",
       " 'endTime': '2020-09-07T01:32:15Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingOutput': {'consumedMLUnits': 0.06},\n",
       " 'etag': 'uOW/zXYCy2g='}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Model Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Created ml engine model [projects/etl-project-datahub/models/amyris_RFClassifiervRFC].\n"
     ]
    }
   ],
   "source": [
    "model_name = 'amyris_RFClassifiervRFC'\n",
    "labels = \"task=classifier,domain=biotech\"\n",
    "filter = 'name:{}'.format(model_name)\n",
    "models = !(gcloud ai-platform models list --filter={filter} --format='value(name)')\n",
    "\n",
    "#if not models:\n",
    "!gcloud ai-platform models create  $model_name \\\n",
    "    --regions=$REGION \\\n",
    "    --labels=$labels\n",
    "#else:\n",
    "#    print(\"Model: {} already exists.\".format(models[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "model_version = 'v01'\n",
    "filter = 'name:{}'.format(model_version)\n",
    "# versions = !(gcloud ai-platform versions list --model={model_name} --format='value(name)' --filter={filter})\n",
    "\n",
    "\n",
    "#if not versions:\n",
    "!gcloud ai-platform versions create {model_version} \\\n",
    "    --model={model_name} \\\n",
    "    --origin=$JOB_DIR \\\n",
    "    --runtime-version=1.15 \\\n",
    "    --framework=scikit-learn \\\n",
    "    --python-version=3.7\n",
    "#else:\n",
    "#     print(\"Model version: {} already exists.\".format(versions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Prediict with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input file is in json format. Use that for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'X_train.xlsx')\n",
    "TRAINING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'training')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'X_validate.xlsx')\n",
    "VALIDATION_FILE_DIR='{}/{}'.format(DATA_ROOT, 'validation')\n",
    "TESTING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://workshop_trial_artifact_store_pp/data/testing'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTING_FILE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input file with JSON formated instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "input_file = 'serving_instance_2.json'\n",
    "X_test_file= '{}/{}'.format(TESTING_FILE_DIR,'X_serving.csv')\n",
    "X_test_file\n",
    "##'gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv'\n",
    "##'gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv'\n",
    "\n",
    "#X_test = pd.read_csv('gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv')\n",
    "X_test = pd.read_csv(X_test_file)\n",
    "\n",
    "X_test.head()\n",
    "\n",
    "with open(input_file, 'w') as f:\n",
    "    for index, row in X_test.head().iterrows():\n",
    "        f.write(json.dumps(list(row.values)))\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NaN, NaN, NaN, NaN, NaN, NaN, 1.022618, NaN, NaN, 0.580472404447558, 98.58567, 94.81158, 93.470631380586, 4.0, 4.0, 0.0, 0.00169426013012948, 14.95093, 33.61875, NaN, 78.16006999999999, 126.72975, 16.18621, 126.72975, 78.16006999999999, 0.02351, 0.00526, 0.0224465933240588, 140.0, 70.52042, 2.1858920909737303, NaN, NaN, NaN, NaN, NaN, NaN, 100000000.0, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 0.61466, NaN, 0.5989, 0.0, 31.237190000000002, 14.02679, NaN, 4.279656, 711.2092200000001, 10.7049]\n",
      "[42.6409517041319, 0.0193221824684571, 0.0118420543402428, 0.00253719182104052, 0.0036820543402428297, 0.7213146226785458, 0.497036, 0.48678195857648104, 2.08721842367735, 0.33542666397717497, 97.62640999999999, 89.44386999999999, 87.32083924606701, 6.0, 6.0, 0.0, 0.0008921167757984731, 69.745, 99.41664, 0.58315, 355.05412, 524.79891, NaN, 524.79891, 355.05412, 0.008159999999999999, 0.00312, 0.013314329119974, 90.0, 89.00098, 1.4682557528184, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 0.35368, NaN, 0.21529, 1.63002, 17.48424, 21.26711, NaN, 12.811126, 1.287626, 15.7848]\n",
      "[NaN, NaN, NaN, NaN, NaN, NaN, 1.6932180000000001, NaN, NaN, 0.9554986283213609, NaN, NaN, NaN, 4.0, 4.0, 0.0, 0.00297494313398885, 18.4575, 18.23629, NaN, 50.302209999999995, 86.99600000000001, 8.78011, 86.99600000000001, 50.302209999999995, 0.030760000000000003, 0.01661, 0.0708817329111439, 140.0, 168.41207, 3.71302716136047, NaN, NaN, NaN, NaN, NaN, NaN, 100000000.0, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 1.24405, NaN, 0.6450600000000001, 12.48656, 16.15727, 19.720470000000002, NaN, 12.304696, 1430.264616, 20.768532999999998]\n",
      "[NaN, NaN, NaN, NaN, NaN, NaN, 0.980806, NaN, NaN, 0.690177863661256, NaN, NaN, NaN, 7.0, 7.0, 0.0, 0.00190368925637996, 31.705920000000003, 40.42024, 7.39486, 149.67999, 229.201, 19.46087, 229.201, 149.67999, 0.01499, 0.0069299999999999995, 0.0295731733337885, 140.0, 87.49154, 3.3447693453404903, 1.7048, 0.0, 22.94114, 0.50056, 3.5184599999999997, 0.45547, 100000000.0, 13.572656333144, 6.9966652, 9.65686244, 56.237906846628, 1.446976942106, 0.203274694945, 0.859721212755, 0.0, 9.789365745419, 0.0, 0.0, 0.26555664073699997, 0.97101394388, 2.7804, NaN, 0.48696000000000006, 3.48728, 27.08137, 11.37292, NaN, 15.430124, NaN, 9.538637]\n",
      "[NaN, NaN, NaN, NaN, NaN, NaN, 1.861936, NaN, NaN, 1.0728828516409301, 97.75506, 87.43739000000001, 85.474473056934, 4.0, 4.0, 0.0, 0.00326319420489824, 19.62814, 16.72851, NaN, 52.7306, 89.08725, 8.05417, 89.08725, 52.7306, 0.0303, 0.01892, 0.08073945735574001, 140.0, 202.54311, 4.35755099669864, NaN, NaN, NaN, NaN, NaN, NaN, 100000000.0, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 1.49813, NaN, 1.9819, 2.04432, 19.194760000000002, 20.20911, NaN, 19.018522, NaN, 24.216795]\n"
     ]
    }
   ],
   "source": [
    "!cat $input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "['delta', 'gamma', 'delta', 'delta', 'delta']\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "--model $model_name \\\n",
    "--version $model_version \\\n",
    "--json-instances $input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous training pipeline with KFP and Cloud AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/etl-project-datahub/trainer_image:latest'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "\n",
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grep: pipeline/train.py: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#!grep 'BASE_IMAGE =' -A 5 pipeline/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_FOLDER = 'pipeline'\n",
    "os.makedirs(PIPELINE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/amyris_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/amyris_pipeline.py\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"KFP pipeline orchestrating BigQuery and Cloud AI Platform services.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from helper_components import evaluate_model\n",
    "from helper_components import retrieve_best_run\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "USE_KFP_SA = os.getenv('USE_KFP_SA')\n",
    "TRAINING_FILE_PATH = 'gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx'\n",
    "INPUT_FILE = 'gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx'\n",
    "TRAINING_FILE_DIR = 'gs://workshop_trial_artifact_store_pp/data/training'\n",
    "TESTING_FILE_DIR = 'gs://workshop_trial_artifact_store_pp/data/testing'\n",
    "VALIDATION_FILE_DIR = 'gs://workshop_trial_artifact_store_pp/data/validation'\n",
    "\n",
    "\n",
    "TESTING_FILE_PATH = 'gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv'\n",
    "# VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "# TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "# SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 3,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"algorithm\": \"RANDOM_SEARCH\",\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"n_estimators\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 10,\n",
    "                \"maxValue\": 200,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"max_leaf_nodes\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 10,\n",
    "                \"maxValue\": 500,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"max_depth\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 3,\n",
    "                \"maxValue\": 20,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"min_samples_split\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [2,5,10]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"min_samples_leaf\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 10,\n",
    "                \"maxValue\": 500,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"max_features\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.5,\n",
    "                \"maxValue\": 1.0,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },    \n",
    "            {\n",
    "                \"parameterName\": \"class_weight\",\n",
    "                \"type\": \"CATEGORICAL\",\n",
    "                \"categoricalValues\": [\n",
    "                              \"balanced\",\n",
    "                              \"balanced_subsample\"\n",
    "                          ]\n",
    "            },  \n",
    "\n",
    "             {\n",
    "                \"parameterName\": \"random_state\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 35,\n",
    "                \"maxValue\": 75,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"bootstrap\",\n",
    "                \"type\": \"CATEGORICAL\",\n",
    "                \"categoricalValues\": [\n",
    "                              \"TRUE\",\n",
    "                              \"FALSE\"\n",
    "                          ]\n",
    "            } \n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# # Helper functions\n",
    "# def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "#     \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "#     sampling_query_template = \"\"\"\n",
    "#          SELECT *\n",
    "#          FROM \n",
    "#              `{{ source_table }}` AS cover\n",
    "#          WHERE \n",
    "#          MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "#          \"\"\"\n",
    "#     query = Template(sampling_query_template).render(\n",
    "#         source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "#     return query\n",
    "\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "# bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(\n",
    "    retrieve_best_run, base_image=BASE_IMAGE)\n",
    "evaluate_model_op = func_to_container_op(evaluate_model, base_image=BASE_IMAGE)\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Amyris Classifier Training',\n",
    "    description='The pipeline training and deploying the Amyris classifierpipeline_yaml'\n",
    ")\n",
    "def amyris_train(project_id,\n",
    "                    region,\n",
    "                    gcs_root,\n",
    "                    evaluation_metric_name,\n",
    "                    evaluation_metric_threshold,\n",
    "                    model_id,\n",
    "                    version_id,\n",
    "                    replace_existing_version,\n",
    "                    hypertune_settings=HYPERTUNE_SETTINGS,\n",
    "                    dataset_location='US'):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    # Create the training split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "#     training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "#     create_training_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=training_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "#     # Create the validation split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "#     validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "#     create_validation_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=validation_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "    # Create the testing split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "#     testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "#     create_testing_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=testing_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "        #'--training_dataset_path',\n",
    "       # TRAINING_FILE_PATH,\n",
    "        '--training_dataset', TRAINING_FILE_DIR,\n",
    "        '--validation_dataset', VALIDATION_FILE_DIR,\n",
    "        '--testing_dataset', TESTING_FILE_DIR, \n",
    "        '--input_file', INPUT_FILE, \n",
    "        '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                                kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri='gcr.io/etl-project-datahub/trainer_image:latest',\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(\n",
    "            project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    train_args = [\n",
    "       # '--training_dataset_path',\n",
    "       #TRAINING_FILE_PATH,\n",
    "        '--training_dataset', TRAINING_FILE_DIR,\n",
    "        '--validation_dataset', VALIDATION_FILE_DIR,\n",
    "        '--testing_dataset',  TESTING_FILE_DIR, \n",
    "        '--input_file', INPUT_FILE, \n",
    "        '--n_estimators',get_best_trial.outputs['n_estimators'], \n",
    "        '--max_leaf_nodes',get_best_trial.outputs['max_leaf_nodes'], \n",
    "        '--max_depth',get_best_trial.outputs['max_depth'],\n",
    "        '--min_samples_split',get_best_trial.outputs['min_samples_split'],\n",
    "        '--bootstrap' ,get_best_trial.outputs['bootstrap'],\n",
    "        '--random_state' ,get_best_trial.outputs['random_state'],\n",
    "        '--max_features' ,get_best_trial.outputs['max_features'],\n",
    "        '--class_weight' ,get_best_trial.outputs['class_weight'],\n",
    "        '--min_samples_leaf',get_best_trial.outputs['min_samples_leaf'],\n",
    "        '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri='gcr.io/etl-project-datahub/trainer_image:latest',\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=TESTING_FILE_PATH,\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "        deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "    # Configure the pipeline to run using the service account defined\n",
    "      # in the user-gcp-sa k8s secret\n",
    "    if USE_KFP_SA == 'True':\n",
    "        kfp.dsl.get_pipeline_conf().add_op_transformer(\n",
    "              use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/helper_components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/helper_components.py\n",
    "\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\"\"\"Helper components.\"\"\"\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def retrieve_best_run(project_id: str, job_id: str) -> NamedTuple('Outputs', [('metric_value', float), ('n_estimators', int),\n",
    "                            ('max_leaf_nodes', int), ('max_depth', int), ('min_samples_split', int), ('min_samples_leaf', int),\n",
    "                            ('bootstrap', str), ('random_state', int), ('max_features', float), ('class_weight', str)]):\n",
    "    \n",
    "    \n",
    "    \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "    from googleapiclient import discovery\n",
    "    from googleapiclient import errors\n",
    "    \n",
    "    ml = discovery.build('ml', 'v1')\n",
    "\n",
    "    job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "    request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "    try:\n",
    "        response = request.execute()\n",
    "    except errors.HttpError as err:\n",
    "        print(err)\n",
    "    except:\n",
    "        print('Unexpected error')\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "    metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "\n",
    "    n_estimators = int(best_trial['hyperparameters']['n_estimators'])\n",
    "    max_leaf_nodes = int(best_trial['hyperparameters']['max_leaf_nodes'])\n",
    "    max_depth = int(best_trial['hyperparameters']['max_depth'])\n",
    "    min_samples_split = int(best_trial['hyperparameters']['min_samples_split'])\n",
    "    min_samples_leaf = int(best_trial['hyperparameters']['min_samples_leaf'])\n",
    "    bootstrap = str(best_trial['hyperparameters']['bootstrap'])\n",
    "    random_state = int(best_trial['hyperparameters']['random_state'])\n",
    "    max_features = float(best_trial['hyperparameters']['max_features'])\n",
    "    class_weight = str(best_trial['hyperparameters']['class_weight'])\n",
    "        \n",
    "    return (metric_value, n_estimators, max_leaf_nodes, max_depth, min_samples_split, min_samples_leaf, bootstrap, random_state,max_features, class_weight )\n",
    "\n",
    "\n",
    "def evaluate_model(dataset_path: str, model_path: str, metric_name: str) -> NamedTuple('Outputs', [('metric_name', str), ('metric_value', float),\n",
    "                            ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \n",
    "    \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "    df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "    X_test = df_test.drop('Run_Performance', axis=1)\n",
    "    y_test = df_test['Run_Performance']\n",
    "\n",
    "    # Copy the model from GCS\n",
    "    model_filename = 'model.pkl'\n",
    "    gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "    print(gcs_model_filepath)\n",
    "    subprocess.check_call(['gsutil', 'cp', gcs_model_filepath, model_filename],\n",
    "                        stderr=sys.stdout)\n",
    "\n",
    "    with open(model_filename, 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "        \n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if metric_name == 'accuracy':\n",
    "        metric_value = accuracy_score(y_test, y_hat)\n",
    "    elif metric_name == 'recall':\n",
    "        metric_value = recall_score(y_test, y_hat)\n",
    "    else:\n",
    "        metric_name = 'N/A'\n",
    "        metric_value = 0\n",
    "\n",
    "    # Export the metric\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "    }\n",
    "\n",
    "    return (metric_name, metric_value, json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Viable_Cell_Density__g_over_L</th>\n",
       "      <th>Product_Flux_Viable_Cells__mmol_over_gDCW_over_h</th>\n",
       "      <th>Growth_Rate_Viable_Cells__1_over_h</th>\n",
       "      <th>Death_Rate_Avg__1_over_h</th>\n",
       "      <th>Death_Rate_Avg_Viable_Cells__1_over_h</th>\n",
       "      <th>TRS_Flux_Viable_Cells__mmol_over_gDCW_over_h</th>\n",
       "      <th>TRS_Flux__mmol_over_gDCW_over_h</th>\n",
       "      <th>TRS_for_Other_Viable_Cells__mmol_over_gDCW_over_h</th>\n",
       "      <th>sOUR_Viable_Cells__mmol_over_gDCW_over_h</th>\n",
       "      <th>TRS_to_Maintenance_Flux__mmol_over_gDCW_over_h</th>\n",
       "      <th>...</th>\n",
       "      <th>Cap_Feedstock_Solids_End__percent</th>\n",
       "      <th>Cap_Heavy_Em_End__percent</th>\n",
       "      <th>Cap_Light_Em_End__percent</th>\n",
       "      <th>Cap_Oil_Em_End__percent</th>\n",
       "      <th>Cap_PCV_End__percent</th>\n",
       "      <th>Cap_Dead_Cell_Layer_End__percent</th>\n",
       "      <th>Zeex9ieJAlt_end__g_L</th>\n",
       "      <th>Zeex9ieJ_mAU_sec_end__area</th>\n",
       "      <th>Zeex9ieJ_Screening_end__g_L</th>\n",
       "      <th>Run_Performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.022618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.580472</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.59890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>31.23719</td>\n",
       "      <td>14.02679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.279656</td>\n",
       "      <td>711.209220</td>\n",
       "      <td>10.704900</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.640952</td>\n",
       "      <td>0.019322</td>\n",
       "      <td>0.011842</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.003682</td>\n",
       "      <td>0.721315</td>\n",
       "      <td>0.497036</td>\n",
       "      <td>0.486782</td>\n",
       "      <td>2.087218</td>\n",
       "      <td>0.335427</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.21529</td>\n",
       "      <td>1.63002</td>\n",
       "      <td>17.48424</td>\n",
       "      <td>21.26711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.811126</td>\n",
       "      <td>1.287626</td>\n",
       "      <td>15.784800</td>\n",
       "      <td>gamma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.693218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.955499</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64506</td>\n",
       "      <td>12.48656</td>\n",
       "      <td>16.15727</td>\n",
       "      <td>19.72047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.304696</td>\n",
       "      <td>1430.264616</td>\n",
       "      <td>20.768533</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.980806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.690178</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48696</td>\n",
       "      <td>3.48728</td>\n",
       "      <td>27.08137</td>\n",
       "      <td>11.37292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.430124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.538637</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.861936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.072883</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.98190</td>\n",
       "      <td>2.04432</td>\n",
       "      <td>19.19476</td>\n",
       "      <td>20.20911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.018522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.216795</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Viable_Cell_Density__g_over_L  \\\n",
       "0                            NaN   \n",
       "1                      42.640952   \n",
       "2                            NaN   \n",
       "3                            NaN   \n",
       "4                            NaN   \n",
       "\n",
       "   Product_Flux_Viable_Cells__mmol_over_gDCW_over_h  \\\n",
       "0                                               NaN   \n",
       "1                                          0.019322   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               NaN   \n",
       "\n",
       "   Growth_Rate_Viable_Cells__1_over_h  Death_Rate_Avg__1_over_h  \\\n",
       "0                                 NaN                       NaN   \n",
       "1                            0.011842                  0.002537   \n",
       "2                                 NaN                       NaN   \n",
       "3                                 NaN                       NaN   \n",
       "4                                 NaN                       NaN   \n",
       "\n",
       "   Death_Rate_Avg_Viable_Cells__1_over_h  \\\n",
       "0                                    NaN   \n",
       "1                               0.003682   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                    NaN   \n",
       "\n",
       "   TRS_Flux_Viable_Cells__mmol_over_gDCW_over_h  \\\n",
       "0                                           NaN   \n",
       "1                                      0.721315   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "\n",
       "   TRS_Flux__mmol_over_gDCW_over_h  \\\n",
       "0                         1.022618   \n",
       "1                         0.497036   \n",
       "2                         1.693218   \n",
       "3                         0.980806   \n",
       "4                         1.861936   \n",
       "\n",
       "   TRS_for_Other_Viable_Cells__mmol_over_gDCW_over_h  \\\n",
       "0                                                NaN   \n",
       "1                                           0.486782   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "   sOUR_Viable_Cells__mmol_over_gDCW_over_h  \\\n",
       "0                                       NaN   \n",
       "1                                  2.087218   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "\n",
       "   TRS_to_Maintenance_Flux__mmol_over_gDCW_over_h  ...  \\\n",
       "0                                        0.580472  ...   \n",
       "1                                        0.335427  ...   \n",
       "2                                        0.955499  ...   \n",
       "3                                        0.690178  ...   \n",
       "4                                        1.072883  ...   \n",
       "\n",
       "   Cap_Feedstock_Solids_End__percent  Cap_Heavy_Em_End__percent  \\\n",
       "0                                NaN                    0.59890   \n",
       "1                                NaN                    0.21529   \n",
       "2                                NaN                    0.64506   \n",
       "3                                NaN                    0.48696   \n",
       "4                                NaN                    1.98190   \n",
       "\n",
       "   Cap_Light_Em_End__percent  Cap_Oil_Em_End__percent  Cap_PCV_End__percent  \\\n",
       "0                    0.00000                 31.23719              14.02679   \n",
       "1                    1.63002                 17.48424              21.26711   \n",
       "2                   12.48656                 16.15727              19.72047   \n",
       "3                    3.48728                 27.08137              11.37292   \n",
       "4                    2.04432                 19.19476              20.20911   \n",
       "\n",
       "   Cap_Dead_Cell_Layer_End__percent  Zeex9ieJAlt_end__g_L  \\\n",
       "0                               NaN              4.279656   \n",
       "1                               NaN             12.811126   \n",
       "2                               NaN             12.304696   \n",
       "3                               NaN             15.430124   \n",
       "4                               NaN             19.018522   \n",
       "\n",
       "   Zeex9ieJ_mAU_sec_end__area  Zeex9ieJ_Screening_end__g_L  Run_Performance  \n",
       "0                  711.209220                    10.704900            delta  \n",
       "1                    1.287626                    15.784800            gamma  \n",
       "2                 1430.264616                    20.768533            delta  \n",
       "3                         NaN                     9.538637            delta  \n",
       "4                         NaN                    24.216795            delta  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.read_csv('gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv')\n",
    "data = new_df.drop(columns=[\"Run_Execution\",\"Product_Produced__g\",\"Titer_End__g_over_kg\"])\n",
    "new_df.head()\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://workshop_trial_artifact_store_pp/staging/jobdir/ecc50576-f782-4575-97b5-a1f42bbc84e8\n"
     ]
    },
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-aa27f7c15e7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcs_model_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m subprocess.check_call(['gsutil', 'cp', gcs_model_filepath, model_filename],\n\u001b[0;32m---> 23\u001b[0;31m                         stderr=sys.stdout)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#model = pickle.load(model_filename)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \"\"\"\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    751\u001b[0m         (p2cread, p2cwrite,\n\u001b[1;32m    752\u001b[0m          \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m          errread, errwrite) = self._get_handles(stdin, stdout, stderr)\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;31m# We wrap OS handles *before* launching the child, otherwise a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_get_handles\u001b[0;34m(self, stdin, stdout, stderr)\u001b[0m\n\u001b[1;32m   1403\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m                 \u001b[0;31m# Assuming file-like object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m                 \u001b[0merrwrite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m             return (p2cread, p2cwrite,\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "## checking my error\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "dataset_path = 'gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv'\n",
    "df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "X_test = df_test.drop('Run_Performance', axis=1)\n",
    "y_test = df_test['Run_Performance']\n",
    "\n",
    "#X_test.head()\n",
    "#y_test.head(200)\n",
    "model_filename = 'model.pkl'\n",
    "gcs_model_filepath = 'gs://workshop_trial_artifact_store_pp/staging/jobdir/ecc50576-f782-4575-97b5-a1f42bbc84e8'\n",
    "#model_filename = 'model.pkl'\n",
    "print(gcs_model_filepath)\n",
    "subprocess.check_call(['gsutil', 'cp', gcs_model_filepath, model_filename],\n",
    "                        stderr=sys.stdout)\n",
    "\n",
    "#model = pickle.load(model_filename)\n",
    "\n",
    "#with open(model_filename, 'rb') as model_file:\n",
    "#    print(model_filename)\n",
    "#    model = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/etl-project-datahub/trainer_image:latest'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n"
     ]
    }
   ],
   "source": [
    "!cat base_image/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: trainer_image/Dockerfile: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat trainer_image/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an empty folder base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_FOLDER = 'base_image'\n",
    "os.makedirs(BASELINE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./base_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./base_image/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "TAG='latest'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 21 file(s) totalling 104.9 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://etl-project-datahub_cloudbuild/source/1599422845.788046-962c7af219d64b9199c88aa491ba6c69.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/etl-project-datahub/builds/3552809c-043c-4b69-be03-7885deb683d0].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/3552809c-043c-4b69-be03-7885deb683d0?project=448067079266].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"3552809c-043c-4b69-be03-7885deb683d0\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://etl-project-datahub_cloudbuild/source/1599422845.788046-962c7af219d64b9199c88aa491ba6c69.tgz#1599422846138783\n",
      "Copying gs://etl-project-datahub_cloudbuild/source/1599422845.788046-962c7af219d64b9199c88aa491ba6c69.tgz#1599422846138783...\n",
      "/ [1 files][ 26.3 KiB/ 26.3 KiB]                                                \n",
      "Operation completed over 1 objects/26.3 KiB.                                     \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  123.9kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 8ba96806eec9\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=b274a2c287dc2cd518009b6103dc36b39b1b0c41a52707f13ce0456cc0231df9\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=6f5579843c5746b726ae1fd12d911a4a09725bb904762643116dcae3078e7f08\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=36e1b3fc10e30dee2747613adc0037939758f422e2df5f211ac24ae34fafcdfc\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container 8ba96806eec9\n",
      " ---> e40c1d63762a\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in dce44f51375c\n",
      "Removing intermediate container dce44f51375c\n",
      " ---> c3bee3632218\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 7c6af8026e88\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in d12408e9a2e1\n",
      "Removing intermediate container d12408e9a2e1\n",
      " ---> 33dec1b5500b\n",
      "Successfully built 33dec1b5500b\n",
      "Successfully tagged gcr.io/etl-project-datahub/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/etl-project-datahub/trainer_image:latest\n",
      "The push refers to repository [gcr.io/etl-project-datahub/trainer_image]\n",
      "89f626ceb1a0: Preparing\n",
      "7f40277e9053: Preparing\n",
      "639848a38b6d: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "222959643149: Waiting\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "c51fe61c6231: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "89f626ceb1a0: Pushed\n",
      "7f40277e9053: Pushed\n",
      "5133f6c43556: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "639848a38b6d: Pushed\n",
      "latest: digest: sha256:f771c1882b0cee98b189269cb17f4aa9fb56514a42e6f6113b28fa854029b322 size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                             IMAGES                                              STATUS\n",
      "3552809c-043c-4b69-be03-7885deb683d0  2020-09-06T20:07:26+00:00  3M36S     gs://etl-project-datahub_cloudbuild/source/1599422845.788046-962c7af219d64b9199c88aa491ba6c69.tgz  gcr.io/etl-project-datahub/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "#!gcloud builds submit --timeout 15m --tag $TRAINER_IMAGE trainer_image\n",
    "!gcloud builds submit --tag $TRAINER_IMAGE $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 122 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://etl-project-datahub_cloudbuild/source/1599442676.247661-a9eb1d3966ca4c7789ed8eef33ae9d1c.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/etl-project-datahub/builds/c8669f35-1788-4bc3-8494-f6e54ea17491].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/c8669f35-1788-4bc3-8494-f6e54ea17491?project=448067079266].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"c8669f35-1788-4bc3-8494-f6e54ea17491\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://etl-project-datahub_cloudbuild/source/1599442676.247661-a9eb1d3966ca4c7789ed8eef33ae9d1c.tgz#1599442676633524\n",
      "Copying gs://etl-project-datahub_cloudbuild/source/1599442676.247661-a9eb1d3966ca4c7789ed8eef33ae9d1c.tgz#1599442676633524...\n",
      "/ [1 files][  228.0 B/  228.0 B]                                                \n",
      "Operation completed over 1 objects/228.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      " ---> Running in 548db01682fd\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.29.0)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.9.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.17.2)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (47.3.1.post20200616)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.2.5) (2.20)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==0.2.5) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.12.3)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.51.0)\n",
      "Building wheels for collected packages: fire, kfp, termcolor, kfp-server-api, argo-models, tabulate, strip-hints\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=d443f93d864e7e9fd9e7b89af0c6d064986b2e9269925d7f51f7bbc21c04193d\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159978 sha256=6ab80a6624aa8dcd70f64bdd88cb92877265ebbfc44b5ebe2769ea8ba9fd9d23\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=61358f0bddb56fe218c37381ac9ea8721209996b658d0da755498ab7662e0ab7\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102468 sha256=0dd09a8cc46a06cebabff4e8abf4c69786c8061e2f3458d0c7a77836962ad395\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=7a507e7ac2d13d017ef11003d3d9703b12b13c70ed0aabbd293cc09f83148558\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23378 sha256=e3231e40afa2aa5bff8ed8c253e16e8d228a5cf7ec969da728fd3b7cd3b2e0d6\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=9e364323c7ba1c7e1cf7e3c01d007c47dc6a6fb47433f3945f212cc2a366b2c1\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "Successfully built fire kfp termcolor kfp-server-api argo-models tabulate strip-hints\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: jupyterlab-git 0.10.1 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: distributed 2.19.0 has requirement cloudpickle>=1.3.0, but you'll have cloudpickle 1.1.1 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, scikit-learn, pandas, urllib3, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, tabulate, click, Deprecated, strip-hints, kfp\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 11.0.0\n",
      "    Uninstalling kubernetes-11.0.0:\n",
      "      Successfully uninstalled kubernetes-11.0.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.4.1\n",
      "    Uninstalling cloudpickle-1.4.1:\n",
      "      Successfully uninstalled cloudpickle-1.4.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "Successfully installed Deprecated-1.2.10 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 fire-0.3.1 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests-toolbelt-0.9.1 scikit-learn-0.20.4 strip-hints-0.1.9 tabulate-0.8.3 termcolor-1.1.0 urllib3-1.24.3\n",
      "Removing intermediate container 548db01682fd\n",
      " ---> e16e140c0763\n",
      "Successfully built e16e140c0763\n",
      "Successfully tagged gcr.io/etl-project-datahub/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/etl-project-datahub/base_image:latest\n",
      "The push refers to repository [gcr.io/etl-project-datahub/base_image]\n",
      "f1a9c0c7c069: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "89212ed9ad75: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "c51fe61c6231: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "f1a9c0c7c069: Pushed\n",
      "latest: digest: sha256:dca710a1c33bd93b8282eea936851d0e0763b462c8f8ea7d408a80eff278ad4e size: 4293\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                             IMAGES                                           STATUS\n",
      "c8669f35-1788-4bc3-8494-f6e54ea17491  2020-09-07T01:37:56+00:00  3M39S     gs://etl-project-datahub_cloudbuild/source/1599442676.247661-a9eb1d3966ca4c7789ed8eef33ae9d1c.tgz  gcr.io/etl-project-datahub/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and deploying the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the below constants with the settings reflecting your lab environment.\n",
    "\n",
    "* ##### REGION - the compute region for AI Platform Training and Prediction\n",
    "* ##### ARTIFACT_STORE - the GCS bucket created during installation of AI Platform Pipelines. The bucket name starts with the hostedkfp-default- prefix.\n",
    "* ##### ENDPOINT - set the ENDPOINT constant to the endpoint to your AI Platform Pipelines instance. Then endpoint to the * ##### AI Platform Pipelines instance can be found on the <a href=\"https://console.cloud.google.com/ai-platform/pipelines/clusters\">AI Platform Pipelines</a> page in the Google Cloud Console.\n",
    "\n",
    "* ##### Open the SETTINGS for your instance\n",
    "\n",
    "* ##### Use the value of the host variable in the Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SKD section of the SETTINGS window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: kfp in /opt/conda/lib/python3.7/site-packages (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: kubernetes<12.0.0,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (11.0.0)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.30.0)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /opt/conda/lib/python3.7/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /opt/conda/lib/python3.7/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied, skipping upgrade: Deprecated in /opt/conda/lib/python3.7/site-packages (from kfp) (1.2.10)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle in /opt/conda/lib/python3.7/site-packages (from kfp) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: strip-hints in /opt/conda/lib/python3.7/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied, skipping upgrade: kfp-server-api<2.0.0,>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-toolbelt>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.20.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=14.05.14 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.24.2 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.3 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (49.6.0.post20200814)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (20.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp) (0.35.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<12.0.0,>=8.0.0->kfp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<12.0.0,>=8.0.0->kfp) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.22.1)\n",
      "Requirement already satisfied, skipping upgrade: google-crc32c<0.2dev,>=0.1.0; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp) (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.13.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<0.2dev,>=0.1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp) (1.14.1)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<0.2dev,>=0.1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp) (2.20)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ENDPOINT = 'caa07ec40e216a8-dot-us-central2.pipelines.googleusercontent.com'\n",
    "#'19a5aed0f754a516-dot-us-central2.pipelines.googleusercontent.com'\n",
    "ARTIFACT_STORE_URI = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USE_KFP_SA=False\n",
      "env: BASE_IMAGE=gcr.io/etl-project-datahub/base_image:latest\n",
      "env: TRAINER_IMAGE=gcr.io/etl-project-datahub/trainer_image:latest\n",
      "env: COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
      "env: RUNTIME_VERSION=1.15\n",
      "env: PYTHON_VERSION=3.7\n"
     ]
    }
   ],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "client = kfp.Client(host='caa07ec40e216a8-dot-us-central2.pipelines.googleusercontent.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/dsl-compile\n"
     ]
    }
   ],
   "source": [
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/amyris_pipeline.py --output amyris_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: argoproj.io/v1alpha1\n",
      "kind: Workflow\n",
      "metadata:\n",
      "  generateName: amyris-classifier-training-\n",
      "  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.0, pipelines.kubeflow.org/pipeline_compilation_time: '2020-09-07T01:42:15.330038',\n",
      "    pipelines.kubeflow.org/pipeline_spec: '{\"description\": \"The pipeline training\n",
      "      and deploying the Amyris classifierpipeline_yaml\", \"inputs\": [{\"name\": \"project_id\"},\n",
      "      {\"name\": \"region\"}, {\"name\": \"gcs_root\"}, {\"name\": \"evaluation_metric_name\"},\n",
      "      {\"name\": \"evaluation_metric_threshold\"}, {\"name\": \"model_id\"}, {\"name\": \"version_id\"},\n",
      "      {\"name\": \"replace_existing_version\"}, {\"default\": \"\\n{\\n    \\\"hyperparameters\\\":  {\\n        \\\"goal\\\":\n"
     ]
    }
   ],
   "source": [
    "!head amyris_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deploy pipeline package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 65886c7c-0c88-4f9a-983b-f1a561cd2276 has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           65886c7c-0c88-4f9a-983b-f1a561cd2276\n",
      "Name         amyris_pipeline_RF7\n",
      "Description\n",
      "Uploaded at  2020-09-07T01:42:27+00:00\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| Parameter Name              | Default Value                                         |\n",
      "+=============================+=======================================================+\n",
      "| project_id                  |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| region                      |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| gcs_root                    |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| evaluation_metric_name      |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| evaluation_metric_threshold |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| model_id                    |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| version_id                  |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| replace_existing_version    |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| hypertune_settings          | {                                                     |\n",
      "|                             |     \"hyperparameters\":  {                             |\n",
      "|                             |         \"goal\": \"MAXIMIZE\",                           |\n",
      "|                             |         \"maxTrials\": 3,                               |\n",
      "|                             |         \"maxParallelTrials\": 3,                       |\n",
      "|                             |         \"hyperparameterMetricTag\": \"accuracy\",        |\n",
      "|                             |         \"enableTrialEarlyStopping\": True,             |\n",
      "|                             |         \"algorithm\": \"RANDOM_SEARCH\",                 |\n",
      "|                             |         \"params\": [                                   |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"n_estimators\",      |\n",
      "|                             |                 \"type\": \"INTEGER\",                    |\n",
      "|                             |                 \"minValue\": 10,                       |\n",
      "|                             |                 \"maxValue\": 200,                      |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"max_leaf_nodes\",    |\n",
      "|                             |                 \"type\": \"INTEGER\",                    |\n",
      "|                             |                 \"minValue\": 10,                       |\n",
      "|                             |                 \"maxValue\": 500,                      |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"max_depth\",         |\n",
      "|                             |                 \"type\": \"INTEGER\",                    |\n",
      "|                             |                 \"minValue\": 3,                        |\n",
      "|                             |                 \"maxValue\": 20,                       |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"min_samples_split\", |\n",
      "|                             |                 \"type\": \"DISCRETE\",                   |\n",
      "|                             |                 \"discreteValues\": [2,5,10]            |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"min_samples_leaf\",  |\n",
      "|                             |                 \"type\": \"INTEGER\",                    |\n",
      "|                             |                 \"minValue\": 10,                       |\n",
      "|                             |                 \"maxValue\": 500,                      |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"max_features\",      |\n",
      "|                             |                 \"type\": \"DOUBLE\",                     |\n",
      "|                             |                 \"minValue\": 0.5,                      |\n",
      "|                             |                 \"maxValue\": 1.0,                      |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"class_weight\",      |\n",
      "|                             |                 \"type\": \"CATEGORICAL\",                |\n",
      "|                             |                 \"categoricalValues\": [                |\n",
      "|                             |                               \"balanced\",             |\n",
      "|                             |                               \"balanced_subsample\"    |\n",
      "|                             |                           ]                           |\n",
      "|                             |             },                                        |\n",
      "|                             |                                                       |\n",
      "|                             |              {                                        |\n",
      "|                             |                 \"parameterName\": \"random_state\",      |\n",
      "|                             |                 \"type\": \"INTEGER\",                    |\n",
      "|                             |                 \"minValue\": 35,                       |\n",
      "|                             |                 \"maxValue\": 75,                       |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"bootstrap\",         |\n",
      "|                             |                 \"type\": \"CATEGORICAL\",                |\n",
      "|                             |                 \"categoricalValues\": [                |\n",
      "|                             |                               \"TRUE\",                 |\n",
      "|                             |                               \"FALSE\"                 |\n",
      "|                             |                           ]                           |\n",
      "|                             |             }                                         |\n",
      "|                             |         ]                                             |\n",
      "|                             |     }                                                 |\n",
      "|                             | }                                                     |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| dataset_location            | US                                                    |\n",
      "+-----------------------------+-------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME='amyris_pipeline_RF7'\n",
    "\n",
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "amyris_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                            | Uploaded at               |\n",
      "+======================================+=================================================+===========================+\n",
      "| 65886c7c-0c88-4f9a-983b-f1a561cd2276 | amyris_pipeline_RF7                             | 2020-09-07T01:42:27+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 0dd4b8c7-167e-4883-a02c-9ac49b63a6bc | amyris_pipeline_RF6                             | 2020-09-06T23:39:34+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 76e17c3f-8954-4ff5-8099-c3455eb1a723 | amyris_pipeline_RF5                             | 2020-09-06T21:41:43+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| d88efe41-074b-4535-ab43-83864d58ff13 | amyris_pipeline_RF4                             | 2020-09-06T21:23:26+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 9afe138a-6b32-4d3b-ac53-8abb7dad137d | amyris_pipeline_RF3                             | 2020-09-06T20:41:04+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 44c03dea-6c51-4f74-9f9a-3758e31be270 | [Tutorial] DSL - Control structures             | 2020-09-06T19:49:58+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 1e956c77-867c-4f42-b083-d05ec09a3f86 | [Tutorial] Data passing in python components    | 2020-09-06T19:49:57+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 0cc0d4ef-a58f-4a63-8bac-fa382f494b83 | [Demo] TFX - Iris classification pipeline       | 2020-09-06T19:49:56+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 3235b9dd-e6ef-4ca8-8940-4899c4426297 | [Demo] TFX - Taxi tip prediction model trainer  | 2020-09-06T19:49:55+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| cb18c6b0-1903-4fd4-800f-9a5293fc39bc | [Demo] XGBoost - Training with confusion matrix | 2020-09-06T19:49:54+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='65886c7c-0c88-4f9a-983b-f1a561cd2276'\n",
    "#'0dd4b8c7-167e-4883-a02c-9ac49b63a6bc'\n",
    "#'76e17c3f-8954-4ff5-8099-c3455eb1a723'\n",
    "#'d88efe41-074b-4535-ab43-83864d58ff13'\n",
    "#'9afe138a-6b32-4d3b-ac53-8abb7dad137d'\n",
    "##'0906289c-a5dd-4292-ac8c-71c8925dac26'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'amyrisRF_RF1'\n",
    "RUN_ID = 'Run_001'\n",
    "EVALUATION_METRIC = 'accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD = '0.69'\n",
    "MODEL_ID = 'amyris_RFClassifiervRFC' #'amyris_endtoendRF2'\n",
    "VERSION_ID = 'v01'\n",
    "REPLACE_EXISTING_VERSION = 'True'\n",
    "\n",
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9bd73d5f-7a15-41b0-baa4-6d71358a6ff5 is submitted\n",
      "+--------------------------------------+---------+----------+---------------------------+\n",
      "| run id                               | name    | status   | created at                |\n",
      "+======================================+=========+==========+===========================+\n",
      "| 9bd73d5f-7a15-41b0-baa4-6d71358a6ff5 | Run_001 |          | 2020-09-07T01:44:38+00:00 |\n",
      "+--------------------------------------+---------+----------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$GCS_STAGING_PATH \\\n",
    "region=$REGION \\\n",
    "evaluation_metric_name=$EVALUATION_METRIC \\\n",
    "evaluation_metric_threshold=$EVALUATION_METRIC_THRESHOLD \\\n",
    "model_id=$MODEL_ID \\\n",
    "version_id=$VERSION_ID \\\n",
    "replace_existing_version=$REPLACE_EXISTING_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
