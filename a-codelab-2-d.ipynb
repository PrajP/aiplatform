{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "#ENDPOINT = '<YOUR_ENDPOINT>' e.g. '337dd39580cbcbd2-dot-us-central2.pipelines.googleusercontent.com'\n",
    "INPUT_FILE = 'gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx'\n",
    "ARTIFACT_STORE_URI = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'X_train.csv')\n",
    "TRAINING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'training')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'X_validate.csv')\n",
    "VALIDATION_FILE_DIR='{}/{}'.format(DATA_ROOT, 'validation')\n",
    "TESTING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'testing')\n",
    "TESTING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'testing', 'X_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Write the training APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_FOLDER = 'train_image'\n",
    "os.makedirs(TRAIN_IMAGE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Write the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAIN_IMAGE_FOLDER}/train.py\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split  ## from analysis\n",
    "\n",
    "#simple sklearn impute and scale numeric pipeline\n",
    "from sklearn.pipeline import Pipeline ## from analysis\n",
    "from sklearn.impute import SimpleImputer ## from analysis\n",
    "from sklearn.preprocessing import StandardScaler ## from analysis\n",
    "import numpy as np ## from analysis\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, input_file, training_dataset, validation_dataset, testing_dataset, n_estimators, max_depth, min_samples_leaf, max_features, min_samples_split, class_weight, max_leaf_nodes, random_state, hptune, bootstrap):\n",
    "\n",
    "   \n",
    "    obj = input_file\n",
    "    print(\"obj\", obj)\n",
    "    data = pd.read_excel(obj,sheet_name='data') \n",
    "    meta_data = pd.read_excel(obj,sheet_name='meta data') \n",
    "    \n",
    "    ## Preprocess    \n",
    "    #Prepare data for analysis\n",
    "    #Split out numeric from categorical varibles\n",
    "\n",
    "    ##var_type_filter = [x in ['physiological','biochemical','process'] for x in meta_data['variable type']]\n",
    "    var_type_filter = [x in ['independent'] for x in meta_data['variable type']]\n",
    "    var_dtype_filter = (data.dtypes == 'float64') | (data.dtypes == 'int64')\n",
    "\n",
    "    numeric_vars = (var_type_filter & var_dtype_filter).values\n",
    "    numeric_x_data = data[data.columns[numeric_vars]]\n",
    "\n",
    "    #things to try to predict\n",
    "    y_data = data[data.columns[(meta_data['target'] == 1).values]]\n",
    "\n",
    "    #meta data about variables\n",
    "    meta_data = meta_data.query('name in {}'.format(list(data.columns[numeric_vars].values))).set_index('name')\n",
    " \n",
    "    \n",
    "    \n",
    "    #Variables which will be used to build the model\n",
    "    model_target = 'Run_Performance' ## Select target for classification\n",
    "    \n",
    "    y_data = data[[model_target]]\n",
    "    \n",
    "    \n",
    "    #maintain class balance\n",
    "    X_train, X_test, y_train, y_test = train_test_split(numeric_x_data, y_data, test_size=0.25, stratify = y_data[model_target], random_state=42)\n",
    "\n",
    "    #split train set to create a pseudo test or validation dataset\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.33, stratify= y_train[model_target], random_state=42)\n",
    "    \n",
    "    \n",
    "    print('The training, validation and test data contain {}, {} and {} rows respectively'.format(len(X_train),len(X_validate),len(X_test)))\n",
    "\n",
    "    training_file = 'X_train'\n",
    "    testing_file = 'X_test'\n",
    "    validation_file = 'X_validate'\n",
    "    serving_file = 'X_serving'\n",
    "  \n",
    "    Xy_train = X_train.join(y_train)\n",
    "    cmd = \"Xy_train.to_csv('{}/{}.csv', index=False)\".format(training_dataset, training_file)\n",
    "    eval(cmd)\n",
    "    print(\"Saved training files for later..\")\n",
    "    Xy_test =X_test.join(y_test)\n",
    "    cmd = \"Xy_test.to_csv('{}/{}.csv', index=False)\".format(testing_dataset, testing_file)\n",
    "    print(\"Saved testing files for later..\")\n",
    "    eval(cmd)\n",
    "  \n",
    "    cmd = \"X_test.to_csv('{}/{}.csv', index=False)\".format(testing_dataset, serving_file)\n",
    "    print(\"Saved serving instance files for later..\")\n",
    "    eval(cmd)\n",
    "\n",
    "    Xy_validate =X_validate.join(y_validate)\n",
    "    cmd = \"X_validate.to_csv('{}/{}.csv', index=False)\".format(validation_dataset, validation_file)\n",
    "    eval(cmd)\n",
    "    print(\"Saved validation files for later..\")\n",
    "\n",
    "    \n",
    "    if not hptune:\n",
    "        #df_train = pd.concat([df_train, df_validation])\n",
    "        X_train = pd.concat([X_train, X_validate])\n",
    "        y_train = pd.concat([y_train, y_validate])\n",
    "        \n",
    "\n",
    "\n",
    "    ## Train, optimize and validate predictive model\n",
    "    ### Train\n",
    "\n",
    "\n",
    "\n",
    "    classifier = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                max_features=max_features,\n",
    "                min_samples_split=min_samples_split,\n",
    "                class_weight=class_weight,\n",
    "                max_leaf_nodes=max_leaf_nodes,\n",
    "                random_state=random_state,\n",
    "                bootstrap=bootstrap\n",
    "\n",
    "     )\n",
    "\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    #auto scale\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    estimator = Pipeline([\n",
    "      ('imputer', imputer),\n",
    "      ('scaler', scaler),\n",
    "      ('classifier', classifier),\n",
    "    ])\n",
    "    \n",
    "\n",
    "    #prepare data for modeling\n",
    "    #use the pipeline created above\n",
    "\n",
    "    #_X_train = pipe.fit_transform(X_train)\n",
    "    _X_train = X_train    \n",
    "    _y_train = y_train[model_target]    ## selected target label for prediction\n",
    "    _X_validate = X_validate\n",
    "    _y_validate = y_validate[model_target]\n",
    "\n",
    "    _X_test = X_test\n",
    "    _y_test = y_test[model_target]\n",
    "\n",
    "\n",
    "    \n",
    "    print('Starting training: n_estimators={}, max_depth={}, min_samples_leaf={}, max_features={}, min_samples_split={}, class_weight={}, max_leaf_nodes={}, random_state={}, hptune={}, bootstrap={}'.format(n_estimators, max_depth, min_samples_leaf, max_features, min_samples_split, class_weight, max_leaf_nodes, random_state, hptune, bootstrap))\n",
    "\n",
    "    #estimator.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter) \n",
    "    estimator.set_params(classifier__n_estimators=n_estimators, classifier__max_depth=max_depth, classifier__min_samples_leaf=min_samples_leaf, \n",
    "                         classifier__max_features=max_features, classifier__min_samples_split=min_samples_split, classifier__class_weight=class_weight,\n",
    "                         classifier__max_leaf_nodes=max_leaf_nodes, classifier__random_state=random_state, classifier__bootstrap=bootstrap) \n",
    "    #pipeline.fit(X_train, y_train)\n",
    "    estimator.fit(_X_train, _y_train)\n",
    "\n",
    "    \n",
    "    if hptune:\n",
    "        accuracy = estimator.score(_X_validate, _y_validate)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(estimator, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Package the script into a docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAIN_IMAGE_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 8.0 KiB before compression.\n",
      "Uploading tarball of [train_image] to [gs://etl-project-datahub_cloudbuild/source/1599457079.49595-3272fcd80ebf482caa9e97bc0ee56ea7.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/etl-project-datahub/builds/71d38685-cadf-423b-b959-414a93bfcf0d].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/71d38685-cadf-423b-b959-414a93bfcf0d?project=448067079266].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"71d38685-cadf-423b-b959-414a93bfcf0d\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://etl-project-datahub_cloudbuild/source/1599457079.49595-3272fcd80ebf482caa9e97bc0ee56ea7.tgz#1599457079972183\n",
      "Copying gs://etl-project-datahub_cloudbuild/source/1599457079.49595-3272fcd80ebf482caa9e97bc0ee56ea7.tgz#1599457079972183...\n",
      "/ [1 files][  2.8 KiB/  2.8 KiB]                                                \n",
      "Operation completed over 1 objects/2.8 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  10.75kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 45d4853cad17\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=d3b34409e1d61970ec44333e4d5598496d8d74dce8d95b2d3dde5981fc6c8fe2\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=5c4012e08ac34345744ab94dfff609888148cd5a11a626e10fecf3b5f9a3454c\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=13fa02b1f4be368968637e20b452aebc11db8571f083789311bc9df85ad647df\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container 45d4853cad17\n",
      " ---> 862ff1cf08ee\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 3a6a255f51a3\n",
      "Removing intermediate container 3a6a255f51a3\n",
      " ---> 5f78c131fcd6\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 67b7fea04c48\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 3547f54b1f8b\n",
      "Removing intermediate container 3547f54b1f8b\n",
      " ---> f34e379bc9ad\n",
      "Successfully built f34e379bc9ad\n",
      "Successfully tagged gcr.io/etl-project-datahub/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/etl-project-datahub/trainer_image:latest\n",
      "The push refers to repository [gcr.io/etl-project-datahub/trainer_image]\n",
      "857b643027c6: Preparing\n",
      "5d21f79a0b22: Preparing\n",
      "51059729ad28: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "c51fe61c6231: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "5d21f79a0b22: Pushed\n",
      "857b643027c6: Pushed\n",
      "5133f6c43556: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "51059729ad28: Pushed\n",
      "latest: digest: sha256:24ba0a674f249411663ee46e2394191b4fd094d7d6602c325326065d69ea556a size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                              STATUS\n",
      "71d38685-cadf-423b-b959-414a93bfcf0d  2020-09-07T05:38:00+00:00  3M33S     gs://etl-project-datahub_cloudbuild/source/1599457079.49595-3272fcd80ebf482caa9e97bc0ee56ea7.tgz  gcr.io/etl-project-datahub/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TRAINER_IMAGE $TRAIN_IMAGE_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 21 file(s) totalling 105.3 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://etl-project-datahub_cloudbuild/source/1599454981.41656-9664c7ae35c44d2c805c3df46f258a55.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/etl-project-datahub/builds/8c3e8385-4c81-4356-b88b-dbcde075aef1].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/8c3e8385-4c81-4356-b88b-dbcde075aef1?project=448067079266].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"8c3e8385-4c81-4356-b88b-dbcde075aef1\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://etl-project-datahub_cloudbuild/source/1599454981.41656-9664c7ae35c44d2c805c3df46f258a55.tgz#1599454981875812\n",
      "Copying gs://etl-project-datahub_cloudbuild/source/1599454981.41656-9664c7ae35c44d2c805c3df46f258a55.tgz#1599454981875812...\n",
      "/ [1 files][ 26.4 KiB/ 26.4 KiB]                                                \n",
      "Operation completed over 1 objects/26.4 KiB.                                     \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  124.4kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "6ac240b13098: Waiting\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 0c611b1995a4\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=94b2b3134c600d1ab4cd314be5bd9f5fc09b6a47f8b2ad2b9bd06b9e5d5f4b74\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=187f4fc1083faeb90fe252cedc235b4a3f5576a2d4a7b8c67456d18a60cc9724\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=ff0e744d617906127b9f5439208ca4f6d459562d4d1d81197c448ed1cb6acee0\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container 0c611b1995a4\n",
      " ---> c6849299140b\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 357ab928e729\n",
      "Removing intermediate container 357ab928e729\n",
      " ---> 515862538687\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 7d1e8c45baf4\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 290db5f53454\n",
      "Removing intermediate container 290db5f53454\n",
      " ---> 4717aeeadcd6\n",
      "Successfully built 4717aeeadcd6\n",
      "Successfully tagged gcr.io/etl-project-datahub/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/etl-project-datahub/trainer_image:latest\n",
      "The push refers to repository [gcr.io/etl-project-datahub/trainer_image]\n",
      "e5786b1b1d43: Preparing\n",
      "54b9a8a6182b: Preparing\n",
      "324da9d2643c: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "89212ed9ad75: Layer already exists\n",
      "c51fe61c6231: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "e5786b1b1d43: Pushed\n",
      "54b9a8a6182b: Pushed\n",
      "4c4a5579b7a8: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "324da9d2643c: Pushed\n",
      "latest: digest: sha256:0182a42aee4a1b51564ee3d08f512f14831ae4def7dbdb98c2a59a9642d467ec size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                              STATUS\n",
      "8c3e8385-4c81-4356-b88b-dbcde075aef1  2020-09-07T05:03:02+00:00  3M35S     gs://etl-project-datahub_cloudbuild/source/1599454981.41656-9664c7ae35c44d2c805c3df46f258a55.tgz  gcr.io/etl-project-datahub/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TRAINER_IMAGE $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 21 file(s) totalling 105.3 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://etl-project-datahub_cloudbuild/source/1599441013.51649-f0124bcd32aa4f2d8fafc985a9ea39fa.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/etl-project-datahub/builds/d8ce2747-8f95-4e43-a92b-e209fb008e53].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/d8ce2747-8f95-4e43-a92b-e209fb008e53?project=448067079266].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"d8ce2747-8f95-4e43-a92b-e209fb008e53\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://etl-project-datahub_cloudbuild/source/1599441013.51649-f0124bcd32aa4f2d8fafc985a9ea39fa.tgz#1599441013924331\n",
      "Copying gs://etl-project-datahub_cloudbuild/source/1599441013.51649-f0124bcd32aa4f2d8fafc985a9ea39fa.tgz#1599441013924331...\n",
      "/ [1 files][ 26.4 KiB/ 26.4 KiB]                                                \n",
      "Operation completed over 1 objects/26.4 KiB.                                     \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  124.4kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 8bbc75c5ece9\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=44ec0cf3a0c1a7102625fe4c52ad579a3fd236fcbe741b582150885377f2f215\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=b07b7f0f0779daf2455227806b824ec5754d45af195583a53c0b43f08af6cf70\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=49b374e01aafc74967cbee9f7249ef5570a7732a8fe9bd079b3eebaed4ee20da\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container 8bbc75c5ece9\n",
      " ---> 4cb7e2b5be8f\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in e10c27c8ccea\n",
      "Removing intermediate container e10c27c8ccea\n",
      " ---> 8f3be0640fb7\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 6248af4de5cb\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 2b4ef1f662bd\n",
      "Removing intermediate container 2b4ef1f662bd\n",
      " ---> 2f9d9efb1b3f\n",
      "Successfully built 2f9d9efb1b3f\n",
      "Successfully tagged gcr.io/etl-project-datahub/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/etl-project-datahub/trainer_image:latest\n",
      "The push refers to repository [gcr.io/etl-project-datahub/trainer_image]\n",
      "3755115a0aef: Preparing\n",
      "3b7529e4c8d7: Preparing\n",
      "fe63651ff77a: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "89212ed9ad75: Layer already exists\n",
      "c51fe61c6231: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "3b7529e4c8d7: Pushed\n",
      "3755115a0aef: Pushed\n",
      "7f996c16a28a: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "fe63651ff77a: Pushed\n",
      "latest: digest: sha256:63756419ac40872807b411a38f1b95703d0758f1bb8a04efc61009ba0c8b925c size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                              STATUS\n",
      "d8ce2747-8f95-4e43-a92b-e209fb008e53  2020-09-07T01:10:14+00:00  3M43S     gs://etl-project-datahub_cloudbuild/source/1599441013.51649-f0124bcd32aa4f2d8fafc985a9ea39fa.tgz  gcr.io/etl-project-datahub/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Run hyperparameter tuning jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the hyperparameter configuration file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall that the training code uses SGDClassifier. The training application has been designed to accept two hyperparameters that control SGDClassifier:\n",
    "\n",
    "* ##### CLASS_WEIGHT \n",
    "* ##### MAX_DEPTH \n",
    "* ##### MAX_FEATURES \n",
    "* ##### MAX_LEAF_NODES \n",
    "* ##### MIN_SAMPLE_LEAF \n",
    "* ##### MIN_SAMPLE_SPLIT \n",
    "* ##### N_ESTIMATORS \n",
    "* ##### RANDOM_STATE\n",
    "\n",
    "#### The below file configures AI Platform hypertuning to run up to 4 trials on up to three nodes and to choose from three discrete values of min_samples_split and the linear range between 10 and 200 for n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_image/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAIN_IMAGE_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 4\n",
    "    maxParallelTrials: 4\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE\n",
    "    algorithm: RANDOM_SEARCH\n",
    "    params:\n",
    "    - parameterName: n_estimators\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 200\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: max_depth\n",
    "      type: INTEGER\n",
    "      minValue: 3\n",
    "      maxValue: 100\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: min_samples_leaf\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 500\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: max_features\n",
    "      type: DOUBLE\n",
    "      minValue: 0.5\n",
    "      maxValue: 1.0\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: min_samples_split\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          2,\n",
    "          5,\n",
    "          10\n",
    "      ]\n",
    "    - parameterName: class_weight\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"balanced\",\n",
    "          \"balanced_subsample\"\n",
    "      ]\n",
    "    - parameterName: max_leaf_nodes\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 500\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: random_state\n",
    "      type: INTEGER\n",
    "      minValue: 35\n",
    "      maxValue: 75\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: bootstrap\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"TRUE\",\n",
    "          \"FALSE\"\n",
    "      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5a: Start the hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "job_dir -> GCS Path for storing the job packages & model.\n",
    "\n",
    "training_dataset_path -> GCS path holding training dataset.\n",
    "\n",
    "validation_dataset_path -> GCS path holding validation dataset.\n",
    "\n",
    "alpha -> hyperparameter\n",
    "\n",
    "max_iter -> hyperparameter\n",
    "\n",
    "hptune -> variable to decide if hyperparameter tuning is to be done or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'X_train.xlsx')\n",
    "TRAINING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'training')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'X_validate.xlsx')\n",
    "VALIDATION_FILE_DIR='{}/{}'.format(DATA_ROOT, 'validation')\n",
    "TESTING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200907_054530] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200907_054530\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200907_054530\n",
      "jobId: JOB_20200907_054530\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameter Tuning Job\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "  \n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$TRAINER_IMAGE \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAIN_IMAGE_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset=$TRAINING_FILE_DIR \\\n",
    "--validation_dataset=$VALIDATION_FILE_DIR \\\n",
    "--testing_dataset=$TESTING_FILE_DIR \\\n",
    "--input_file=$INPUT_FILE \\\n",
    "--hptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5b:Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2020-09-07T05:45:32Z'\n",
      "endTime: '2020-09-07T05:53:20Z'\n",
      "etag: gj2qHf1qyIA=\n",
      "jobId: JOB_20200907_054530\n",
      "startTime: '2020-09-07T05:45:34Z'\n",
      "state: SUCCEEDED\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --training_dataset=gs://workshop_trial_artifact_store_pp/data/training\n",
      "  - --validation_dataset=gs://workshop_trial_artifact_store_pp/data/validation\n",
      "  - --testing_dataset=gs://workshop_trial_artifact_store_pp/data/testing\n",
      "  - --input_file=gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx\n",
      "  - --hptune\n",
      "  hyperparameters:\n",
      "    algorithm: RANDOM_SEARCH\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: accuracy\n",
      "    maxParallelTrials: 4\n",
      "    maxTrials: 4\n",
      "    params:\n",
      "    - maxValue: 200.0\n",
      "      minValue: 10.0\n",
      "      parameterName: n_estimators\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 100.0\n",
      "      minValue: 3.0\n",
      "      parameterName: max_depth\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 500.0\n",
      "      minValue: 10.0\n",
      "      parameterName: min_samples_leaf\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 1.0\n",
      "      minValue: 0.5\n",
      "      parameterName: max_features\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: DOUBLE\n",
      "    - discreteValues:\n",
      "      - 2.0\n",
      "      - 5.0\n",
      "      - 10.0\n",
      "      parameterName: min_samples_split\n",
      "      type: DISCRETE\n",
      "    - categoricalValues:\n",
      "      - balanced\n",
      "      - balanced_subsample\n",
      "      parameterName: class_weight\n",
      "      type: CATEGORICAL\n",
      "    - maxValue: 500.0\n",
      "      minValue: 10.0\n",
      "      parameterName: max_leaf_nodes\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 75.0\n",
      "      minValue: 35.0\n",
      "      parameterName: random_state\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - categoricalValues:\n",
      "      - 'TRUE'\n",
      "      - 'FALSE'\n",
      "      parameterName: bootstrap\n",
      "      type: CATEGORICAL\n",
      "  jobDir: gs://workshop_trial_artifact_store_pp/jobs/JOB_20200907_054530\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/etl-project-datahub/trainer_image:latest\n",
      "  region: us-central1\n",
      "trainingOutput:\n",
      "  completedTrialCount: '4'\n",
      "  consumedMLUnits: 0.26\n",
      "  hyperparameterMetricTag: accuracy\n",
      "  isHyperparameterTuningJob: true\n",
      "  trials:\n",
      "  - endTime: '2020-09-07T05:52:42Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 0.958869\n",
      "      trainingStep: '1'\n",
      "    hyperparameters:\n",
      "      bootstrap: 'TRUE'\n",
      "      class_weight: balanced_subsample\n",
      "      max_depth: '18'\n",
      "      max_features: '0.56918819952535549'\n",
      "      max_leaf_nodes: '364'\n",
      "      min_samples_leaf: '11'\n",
      "      min_samples_split: '5'\n",
      "      n_estimators: '127'\n",
      "      random_state: '53'\n",
      "    startTime: '2020-09-07T05:46:11.933947341Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '1'\n",
      "  - endTime: '2020-09-07T05:52:22Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 0.673522\n",
      "      trainingStep: '1'\n",
      "    hyperparameters:\n",
      "      bootstrap: 'TRUE'\n",
      "      class_weight: balanced_subsample\n",
      "      max_depth: '54'\n",
      "      max_features: '0.63766721726916786'\n",
      "      max_leaf_nodes: '494'\n",
      "      min_samples_leaf: '462'\n",
      "      min_samples_split: '5'\n",
      "      n_estimators: '40'\n",
      "      random_state: '37'\n",
      "    startTime: '2020-09-07T05:46:11.934413986Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '4'\n",
      "  - endTime: '2020-09-07T05:53:02Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 0.673522\n",
      "      trainingStep: '1'\n",
      "    hyperparameters:\n",
      "      bootstrap: 'TRUE'\n",
      "      class_weight: balanced\n",
      "      max_depth: '86'\n",
      "      max_features: '0.60243688709681886'\n",
      "      max_leaf_nodes: '484'\n",
      "      min_samples_leaf: '359'\n",
      "      min_samples_split: '2'\n",
      "      n_estimators: '121'\n",
      "      random_state: '37'\n",
      "    startTime: '2020-09-07T05:46:11.934194245Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '2'\n",
      "  - endTime: '2020-09-07T05:53:10Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 0.326478\n",
      "      trainingStep: '1'\n",
      "    hyperparameters:\n",
      "      bootstrap: 'FALSE'\n",
      "      class_weight: balanced\n",
      "      max_depth: '62'\n",
      "      max_features: '0.722012654595876'\n",
      "      max_leaf_nodes: '108'\n",
      "      min_samples_leaf: '296'\n",
      "      min_samples_split: '5'\n",
      "      n_estimators: '26'\n",
      "      random_state: '46'\n",
      "    startTime: '2020-09-07T05:46:11.934316606Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '3'\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20200907_054530?project=etl-project-datahub\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2FJOB_20200907_054530&project=etl-project-datahub\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning Job description\n",
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5c:Monitor the job - stream the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for streaming the logs\n",
    "#!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6c: Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20200907_054530',\n",
       " 'trainingInput': {'args': ['--training_dataset=gs://workshop_trial_artifact_store_pp/data/training',\n",
       "   '--validation_dataset=gs://workshop_trial_artifact_store_pp/data/validation',\n",
       "   '--testing_dataset=gs://workshop_trial_artifact_store_pp/data/testing',\n",
       "   '--input_file=gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx',\n",
       "   '--hptune'],\n",
       "  'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "   'params': [{'parameterName': 'n_estimators',\n",
       "     'minValue': 10,\n",
       "     'maxValue': 200,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'max_depth',\n",
       "     'minValue': 3,\n",
       "     'maxValue': 100,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'min_samples_leaf',\n",
       "     'minValue': 10,\n",
       "     'maxValue': 500,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'max_features',\n",
       "     'minValue': 0.5,\n",
       "     'maxValue': 1,\n",
       "     'type': 'DOUBLE',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'min_samples_split',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [2, 5, 10]},\n",
       "    {'parameterName': 'class_weight',\n",
       "     'type': 'CATEGORICAL',\n",
       "     'categoricalValues': ['balanced', 'balanced_subsample']},\n",
       "    {'parameterName': 'max_leaf_nodes',\n",
       "     'minValue': 10,\n",
       "     'maxValue': 500,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'random_state',\n",
       "     'minValue': 35,\n",
       "     'maxValue': 75,\n",
       "     'type': 'INTEGER',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'},\n",
       "    {'parameterName': 'bootstrap',\n",
       "     'type': 'CATEGORICAL',\n",
       "     'categoricalValues': ['TRUE', 'FALSE']}],\n",
       "   'maxTrials': 4,\n",
       "   'maxParallelTrials': 4,\n",
       "   'hyperparameterMetricTag': 'accuracy',\n",
       "   'enableTrialEarlyStopping': True,\n",
       "   'algorithm': 'RANDOM_SEARCH'},\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://workshop_trial_artifact_store_pp/jobs/JOB_20200907_054530',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/etl-project-datahub/trainer_image:latest'}},\n",
       " 'createTime': '2020-09-07T05:45:32Z',\n",
       " 'startTime': '2020-09-07T05:45:34Z',\n",
       " 'endTime': '2020-09-07T05:53:20Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingOutput': {'completedTrialCount': '4',\n",
       "  'trials': [{'trialId': '1',\n",
       "    'hyperparameters': {'max_leaf_nodes': '364',\n",
       "     'min_samples_leaf': '11',\n",
       "     'max_features': '0.56918819952535549',\n",
       "     'n_estimators': '127',\n",
       "     'bootstrap': 'TRUE',\n",
       "     'class_weight': 'balanced_subsample',\n",
       "     'max_depth': '18',\n",
       "     'min_samples_split': '5',\n",
       "     'random_state': '53'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9588688946015425},\n",
       "    'startTime': '2020-09-07T05:46:11.933947341Z',\n",
       "    'endTime': '2020-09-07T05:52:42Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '4',\n",
       "    'hyperparameters': {'bootstrap': 'TRUE',\n",
       "     'random_state': '37',\n",
       "     'class_weight': 'balanced_subsample',\n",
       "     'max_features': '0.63766721726916786',\n",
       "     'n_estimators': '40',\n",
       "     'min_samples_leaf': '462',\n",
       "     'min_samples_split': '5',\n",
       "     'max_depth': '54',\n",
       "     'max_leaf_nodes': '494'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6735218508997429},\n",
       "    'startTime': '2020-09-07T05:46:11.934413986Z',\n",
       "    'endTime': '2020-09-07T05:52:22Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '2',\n",
       "    'hyperparameters': {'max_depth': '86',\n",
       "     'n_estimators': '121',\n",
       "     'max_leaf_nodes': '484',\n",
       "     'min_samples_leaf': '359',\n",
       "     'random_state': '37',\n",
       "     'min_samples_split': '2',\n",
       "     'max_features': '0.60243688709681886',\n",
       "     'bootstrap': 'TRUE',\n",
       "     'class_weight': 'balanced'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.6735218508997429},\n",
       "    'startTime': '2020-09-07T05:46:11.934194245Z',\n",
       "    'endTime': '2020-09-07T05:53:02Z',\n",
       "    'state': 'SUCCEEDED'},\n",
       "   {'trialId': '3',\n",
       "    'hyperparameters': {'n_estimators': '26',\n",
       "     'max_leaf_nodes': '108',\n",
       "     'max_features': '0.722012654595876',\n",
       "     'class_weight': 'balanced',\n",
       "     'min_samples_split': '5',\n",
       "     'min_samples_leaf': '296',\n",
       "     'bootstrap': 'FALSE',\n",
       "     'random_state': '46',\n",
       "     'max_depth': '62'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.3264781491002571},\n",
       "    'startTime': '2020-09-07T05:46:11.934316606Z',\n",
       "    'endTime': '2020-09-07T05:53:10Z',\n",
       "    'state': 'SUCCEEDED'}],\n",
       "  'consumedMLUnits': 0.26,\n",
       "  'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'accuracy'},\n",
       " 'etag': 'gj2qHf1qyIA='}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trialId': '1',\n",
       " 'hyperparameters': {'max_leaf_nodes': '364',\n",
       "  'min_samples_leaf': '11',\n",
       "  'max_features': '0.56918819952535549',\n",
       "  'n_estimators': '127',\n",
       "  'bootstrap': 'TRUE',\n",
       "  'class_weight': 'balanced_subsample',\n",
       "  'max_depth': '18',\n",
       "  'min_samples_split': '5',\n",
       "  'random_state': '53'},\n",
       " 'finalMetric': {'trainingStep': '1', 'objectiveValue': 0.9588688946015425},\n",
       " 'startTime': '2020-09-07T05:46:11.933947341Z',\n",
       " 'endTime': '2020-09-07T05:52:42Z',\n",
       " 'state': 'SUCCEEDED'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['trainingOutput']['trials'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Retrain the model with the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the best parameters\n",
    "CLASS_WEIGHT = response['trainingOutput']['trials'][0]['hyperparameters']['class_weight']\n",
    "MAX_DEPTH = response['trainingOutput']['trials'][0]['hyperparameters']['max_depth']\n",
    "MAX_FEATURES = response['trainingOutput']['trials'][0]['hyperparameters']['max_features']\n",
    "MAX_LEAF_NODES = response['trainingOutput']['trials'][0]['hyperparameters']['max_leaf_nodes']\n",
    "MIN_SAMPLE_LEAF = response['trainingOutput']['trials'][0]['hyperparameters']['min_samples_leaf']\n",
    "MIN_SAMPLE_SPLIT = response['trainingOutput']['trials'][0]['hyperparameters']['min_samples_split']\n",
    "N_ESTIMATORS = response['trainingOutput']['trials'][0]['hyperparameters']['n_estimators']\n",
    "RANDOM_STATE = response['trainingOutput']['trials'][0]['hyperparameters']['random_state']\n",
    "BOOTSTRAP = response['trainingOutput']['trials'][0]['hyperparameters']['bootstrap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200907_055554] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200907_055554\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200907_055554\n",
      "jobId: JOB_20200907_055554\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "#Now select the best parameters for deploying the model\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "#BOOTSTRAP = \"TRUE\"\n",
    "#CLASS_WEIGHT = \"balanced_subsample\"\n",
    "#MAX_DEPTH = \"22\"\n",
    "#MAX_FEATURES = \"0.92715679885914515\"\n",
    "#MAX_LEAF_NODES = \"156\"\n",
    "#MIN_SAMPLE_LEAF = \"55\"\n",
    "#MIN_SAMPLE_SPLIT = \"5\"\n",
    "#N_ESTIMATORS = \"147\"\n",
    "#RANDOM_STATE = \"37\"\n",
    "\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$TRAINER_IMAGE \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "-- \\\n",
    "--training_dataset=$TRAINING_FILE_DIR \\\n",
    "--validation_dataset=$VALIDATION_FILE_DIR \\\n",
    "--testing_dataset=$TESTING_FILE_DIR \\\n",
    "--input_file=$INPUT_FILE \\\n",
    "--bootstrap=$BOOTSTRAP \\\n",
    "--class_weight=$CLASS_WEIGHT \\\n",
    "--max_depth=$MAX_DEPTH \\\n",
    "--max_features=$MAX_FEATURES \\\n",
    "--min_samples_leaf=$MIN_SAMPLE_LEAF \\\n",
    "--max_leaf_nodes=$MAX_LEAF_NODES \\\n",
    "--min_samples_split=$MIN_SAMPLE_SPLIT \\\n",
    "--n_estimators=$N_ESTIMATORS \\\n",
    "--random_state=$RANDOM_STATE \\\n",
    "--nohptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20200907_055554',\n",
       " 'trainingInput': {'args': ['--training_dataset=gs://workshop_trial_artifact_store_pp/data/training',\n",
       "   '--validation_dataset=gs://workshop_trial_artifact_store_pp/data/validation',\n",
       "   '--testing_dataset=gs://workshop_trial_artifact_store_pp/data/testing',\n",
       "   '--input_file=gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx',\n",
       "   '--bootstrap=TRUE',\n",
       "   '--class_weight=balanced_subsample',\n",
       "   '--max_depth=18',\n",
       "   '--max_features=0.56918819952535549',\n",
       "   '--min_samples_leaf=11',\n",
       "   '--max_leaf_nodes=364',\n",
       "   '--min_samples_split=5',\n",
       "   '--n_estimators=127',\n",
       "   '--random_state=53',\n",
       "   '--nohptune'],\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://workshop_trial_artifact_store_pp/jobs/JOB_20200907_055554',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/etl-project-datahub/trainer_image:latest'}},\n",
       " 'createTime': '2020-09-07T05:55:56Z',\n",
       " 'startTime': '2020-09-07T05:59:44Z',\n",
       " 'endTime': '2020-09-07T06:02:16Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingOutput': {'consumedMLUnits': 0.06},\n",
       " 'etag': 'O1pblkx/+5U='}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Model Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Created ml engine model [projects/etl-project-datahub/models/amyris_RFClassifiervRFC1].\n"
     ]
    }
   ],
   "source": [
    "model_name = 'amyris_RFClassifiervRFC1'\n",
    "labels = \"task=classifier,domain=biotech\"\n",
    "filter = 'name:{}'.format(model_name)\n",
    "models = !(gcloud ai-platform models list --filter={filter} --format='value(name)')\n",
    "\n",
    "#if not models:\n",
    "!gcloud ai-platform models create  $model_name \\\n",
    "    --regions=$REGION \\\n",
    "    --labels=$labels\n",
    "#else:\n",
    "#    print(\"Model: {} already exists.\".format(models[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "model_version = 'v01'\n",
    "filter = 'name:{}'.format(model_version)\n",
    "# versions = !(gcloud ai-platform versions list --model={model_name} --format='value(name)' --filter={filter})\n",
    "\n",
    "\n",
    "#if not versions:\n",
    "!gcloud ai-platform versions create {model_version} \\\n",
    "    --model={model_name} \\\n",
    "    --origin=$JOB_DIR \\\n",
    "    --runtime-version=1.15 \\\n",
    "    --framework=scikit-learn \\\n",
    "    --python-version=3.7\n",
    "#else:\n",
    "#     print(\"Model version: {} already exists.\".format(versions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Predict with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input file is in json format. Use that for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'X_train.xlsx')\n",
    "TRAINING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'training')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'X_validate.xlsx')\n",
    "VALIDATION_FILE_DIR='{}/{}'.format(DATA_ROOT, 'validation')\n",
    "TESTING_FILE_DIR='{}/{}'.format(DATA_ROOT, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://workshop_trial_artifact_store_pp/data/testing'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTING_FILE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input file with JSON formated instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "input_file = 'serving_instance_2a.json'\n",
    "X_test_file= '{}/{}'.format(TESTING_FILE_DIR,'X_serving.csv')\n",
    "X_test_file\n",
    "##'gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv'\n",
    "##'gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv'\n",
    "\n",
    "#X_test = pd.read_csv('gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv')\n",
    "X_test = pd.read_csv(X_test_file)\n",
    "\n",
    "X_test.head()\n",
    "\n",
    "with open(input_file, 'w') as f:\n",
    "    for index, row in X_test.head().iterrows():\n",
    "        f.write(json.dumps(list(row.values)))\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NaN, NaN, NaN, NaN, NaN, NaN, 1.022618, NaN, NaN, 0.580472404447558, 98.58567, 94.81158, 93.470631380586, 4.0, 4.0, 0.0, 0.00169426013012948, 14.95093, 33.61875, NaN, 78.16006999999999, 126.72975, 16.18621, 126.72975, 78.16006999999999, 0.02351, 0.00526, 0.0224465933240588, 140.0, 70.52042, 2.1858920909737303, NaN, NaN, NaN, NaN, NaN, NaN, 100000000.0, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 0.61466, NaN, 0.5989, 0.0, 31.237190000000002, 14.02679, NaN, 4.279656, 711.2092200000001, 10.7049]\n",
      "[42.6409517041319, 0.0193221824684571, 0.0118420543402428, 0.00253719182104052, 0.0036820543402428297, 0.7213146226785458, 0.497036, 0.48678195857648104, 2.08721842367735, 0.33542666397717497, 97.62640999999999, 89.44386999999999, 87.32083924606701, 6.0, 6.0, 0.0, 0.0008921167757984731, 69.745, 99.41664, 0.58315, 355.05412, 524.79891, NaN, 524.79891, 355.05412, 0.008159999999999999, 0.00312, 0.013314329119974, 90.0, 89.00098, 1.4682557528184, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 0.35368, NaN, 0.21529, 1.63002, 17.48424, 21.26711, NaN, 12.811126, 1.287626, 15.7848]\n",
      "[NaN, NaN, NaN, NaN, NaN, NaN, 1.6932180000000001, NaN, NaN, 0.9554986283213609, NaN, NaN, NaN, 4.0, 4.0, 0.0, 0.00297494313398885, 18.4575, 18.23629, NaN, 50.302209999999995, 86.99600000000001, 8.78011, 86.99600000000001, 50.302209999999995, 0.030760000000000003, 0.01661, 0.0708817329111439, 140.0, 168.41207, 3.71302716136047, NaN, NaN, NaN, NaN, NaN, NaN, 100000000.0, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 1.24405, NaN, 0.6450600000000001, 12.48656, 16.15727, 19.720470000000002, NaN, 12.304696, 1430.264616, 20.768532999999998]\n",
      "[NaN, NaN, NaN, NaN, NaN, NaN, 0.980806, NaN, NaN, 0.690177863661256, NaN, NaN, NaN, 7.0, 7.0, 0.0, 0.00190368925637996, 31.705920000000003, 40.42024, 7.39486, 149.67999, 229.201, 19.46087, 229.201, 149.67999, 0.01499, 0.0069299999999999995, 0.0295731733337885, 140.0, 87.49154, 3.3447693453404903, 1.7048, 0.0, 22.94114, 0.50056, 3.5184599999999997, 0.45547, 100000000.0, 13.572656333144, 6.9966652, 9.65686244, 56.237906846628, 1.446976942106, 0.203274694945, 0.859721212755, 0.0, 9.789365745419, 0.0, 0.0, 0.26555664073699997, 0.97101394388, 2.7804, NaN, 0.48696000000000006, 3.48728, 27.08137, 11.37292, NaN, 15.430124, NaN, 9.538637]\n",
      "[NaN, NaN, NaN, NaN, NaN, NaN, 1.861936, NaN, NaN, 1.0728828516409301, 97.75506, 87.43739000000001, 85.474473056934, 4.0, 4.0, 0.0, 0.00326319420489824, 19.62814, 16.72851, NaN, 52.7306, 89.08725, 8.05417, 89.08725, 52.7306, 0.0303, 0.01892, 0.08073945735574001, 140.0, 202.54311, 4.35755099669864, NaN, NaN, NaN, NaN, NaN, NaN, 100000000.0, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 1.49813, NaN, 1.9819, 2.04432, 19.194760000000002, 20.20911, NaN, 19.018522, NaN, 24.216795]\n"
     ]
    }
   ],
   "source": [
    "!cat $input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "['delta', 'gamma', 'delta', 'delta', 'delta']\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "--model $model_name \\\n",
    "--version $model_version \\\n",
    "--json-instances $input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous training pipeline with KFP and Cloud AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/etl-project-datahub/trainer_image:latest'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grep: pipeline/train.py: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#!grep 'BASE_IMAGE =' -A 5 pipeline/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_FOLDER = 'pipeline'\n",
    "os.makedirs(PIPELINE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/amyris_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/amyris_pipeline.py\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"KFP pipeline orchestrating BigQuery and Cloud AI Platform services.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from helper_components import evaluate_model\n",
    "from helper_components import retrieve_best_run\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "USE_KFP_SA = os.getenv('USE_KFP_SA')\n",
    "TRAINING_FILE_PATH = 'gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx'\n",
    "INPUT_FILE = 'gs://input_data_amy_bkt1/Anonymized_Fermentation_Data_final.xlsx'\n",
    "TRAINING_FILE_DIR = 'gs://workshop_trial_artifact_store_pp/data/training'\n",
    "TESTING_FILE_DIR = 'gs://workshop_trial_artifact_store_pp/data/testing'\n",
    "VALIDATION_FILE_DIR = 'gs://workshop_trial_artifact_store_pp/data/validation'\n",
    "\n",
    "\n",
    "TESTING_FILE_PATH = 'gs://workshop_trial_artifact_store_pp/data/testing/X_test.csv'\n",
    "# VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "# TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "# SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 3,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"algorithm\": \"RANDOM_SEARCH\",\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"n_estimators\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 10,\n",
    "                \"maxValue\": 200,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"max_leaf_nodes\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 10,\n",
    "                \"maxValue\": 500,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"max_depth\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 3,\n",
    "                \"maxValue\": 20,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"min_samples_split\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [2,5,10]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"min_samples_leaf\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 10,\n",
    "                \"maxValue\": 500,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"max_features\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.5,\n",
    "                \"maxValue\": 1.0,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },    \n",
    "            {\n",
    "                \"parameterName\": \"class_weight\",\n",
    "                \"type\": \"CATEGORICAL\",\n",
    "                \"categoricalValues\": [\n",
    "                              \"balanced\",\n",
    "                              \"balanced_subsample\"\n",
    "                          ]\n",
    "            },  \n",
    "\n",
    "             {\n",
    "                \"parameterName\": \"random_state\",\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minValue\": 35,\n",
    "                \"maxValue\": 75,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"bootstrap\",\n",
    "                \"type\": \"CATEGORICAL\",\n",
    "                \"categoricalValues\": [\n",
    "                              \"TRUE\",\n",
    "                              \"FALSE\"\n",
    "                          ]\n",
    "            } \n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# # Helper functions\n",
    "# def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "#     \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "#     sampling_query_template = \"\"\"\n",
    "#          SELECT *\n",
    "#          FROM \n",
    "#              `{{ source_table }}` AS cover\n",
    "#          WHERE \n",
    "#          MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "#          \"\"\"\n",
    "#     query = Template(sampling_query_template).render(\n",
    "#         source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "#     return query\n",
    "\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "# bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(\n",
    "    retrieve_best_run, base_image=BASE_IMAGE)\n",
    "evaluate_model_op = func_to_container_op(evaluate_model, base_image=BASE_IMAGE)\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Amyris Classifier Training',\n",
    "    description='The pipeline training and deploying the Amyris classifierpipeline_yaml'\n",
    ")\n",
    "def amyris_train(project_id,\n",
    "                    region,\n",
    "                    gcs_root,\n",
    "                    evaluation_metric_name,\n",
    "                    evaluation_metric_threshold,\n",
    "                    model_id,\n",
    "                    version_id,\n",
    "                    replace_existing_version,\n",
    "                    hypertune_settings=HYPERTUNE_SETTINGS,\n",
    "                    dataset_location='US'):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    # Create the training split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "#     training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "#     create_training_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=training_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "#     # Create the validation split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "#     validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "#     create_validation_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=validation_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "    # Create the testing split\n",
    "#     query = generate_sampling_query(\n",
    "#         source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "#     testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "#     create_testing_split = bigquery_query_op(\n",
    "#         query=query,\n",
    "#         project_id=project_id,\n",
    "#         dataset_id=dataset_id,\n",
    "#         table_id='',\n",
    "#         output_gcs_path=testing_file_path,\n",
    "#         dataset_location=dataset_location)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "        #'--training_dataset_path',\n",
    "       # TRAINING_FILE_PATH,\n",
    "        '--training_dataset', TRAINING_FILE_DIR,\n",
    "        '--validation_dataset', VALIDATION_FILE_DIR,\n",
    "        '--testing_dataset', TESTING_FILE_DIR, \n",
    "        '--input_file', INPUT_FILE, \n",
    "        '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                                kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri='gcr.io/etl-project-datahub/trainer_image:latest',\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(\n",
    "            project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    train_args = [\n",
    "       # '--training_dataset_path',\n",
    "       #TRAINING_FILE_PATH,\n",
    "        '--training_dataset', TRAINING_FILE_DIR,\n",
    "        '--validation_dataset', VALIDATION_FILE_DIR,\n",
    "        '--testing_dataset',  TESTING_FILE_DIR, \n",
    "        '--input_file', INPUT_FILE, \n",
    "        '--n_estimators',get_best_trial.outputs['n_estimators'], \n",
    "        '--max_leaf_nodes',get_best_trial.outputs['max_leaf_nodes'], \n",
    "        '--max_depth',get_best_trial.outputs['max_depth'],\n",
    "        '--min_samples_split',get_best_trial.outputs['min_samples_split'],\n",
    "        '--bootstrap' ,get_best_trial.outputs['bootstrap'],\n",
    "        '--random_state' ,get_best_trial.outputs['random_state'],\n",
    "        '--max_features' ,get_best_trial.outputs['max_features'],\n",
    "        '--class_weight' ,get_best_trial.outputs['class_weight'],\n",
    "        '--min_samples_leaf',get_best_trial.outputs['min_samples_leaf'],\n",
    "        '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri='gcr.io/etl-project-datahub/trainer_image:latest',\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=TESTING_FILE_PATH,\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "        deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "    # Configure the pipeline to run using the service account defined\n",
    "      # in the user-gcp-sa k8s secret\n",
    "    if USE_KFP_SA == 'True':\n",
    "        kfp.dsl.get_pipeline_conf().add_op_transformer(\n",
    "              use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/helper_components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/helper_components.py\n",
    "\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\"\"\"Helper components.\"\"\"\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def retrieve_best_run(project_id: str, job_id: str) -> NamedTuple('Outputs', [('metric_value', float), ('n_estimators', int),\n",
    "                            ('max_leaf_nodes', int), ('max_depth', int), ('min_samples_split', int), ('min_samples_leaf', int),\n",
    "                            ('bootstrap', str), ('random_state', int), ('max_features', float), ('class_weight', str)]):\n",
    "    \n",
    "    \n",
    "    \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "    from googleapiclient import discovery\n",
    "    from googleapiclient import errors\n",
    "    \n",
    "    ml = discovery.build('ml', 'v1')\n",
    "\n",
    "    job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "    request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "    try:\n",
    "        response = request.execute()\n",
    "    except errors.HttpError as err:\n",
    "        print(err)\n",
    "    except:\n",
    "        print('Unexpected error')\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "    metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "\n",
    "    n_estimators = int(best_trial['hyperparameters']['n_estimators'])\n",
    "    max_leaf_nodes = int(best_trial['hyperparameters']['max_leaf_nodes'])\n",
    "    max_depth = int(best_trial['hyperparameters']['max_depth'])\n",
    "    min_samples_split = int(best_trial['hyperparameters']['min_samples_split'])\n",
    "    min_samples_leaf = int(best_trial['hyperparameters']['min_samples_leaf'])\n",
    "    bootstrap = str(best_trial['hyperparameters']['bootstrap'])\n",
    "    random_state = int(best_trial['hyperparameters']['random_state'])\n",
    "    max_features = float(best_trial['hyperparameters']['max_features'])\n",
    "    class_weight = str(best_trial['hyperparameters']['class_weight'])\n",
    "        \n",
    "    return (metric_value, n_estimators, max_leaf_nodes, max_depth, min_samples_split, min_samples_leaf, bootstrap, random_state,max_features, class_weight )\n",
    "\n",
    "\n",
    "def evaluate_model(dataset_path: str, model_path: str, metric_name: str) -> NamedTuple('Outputs', [('metric_name', str), ('metric_value', float),\n",
    "                            ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \n",
    "    \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "    df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "    X_test = df_test.drop('Run_Performance', axis=1)\n",
    "    y_test = df_test['Run_Performance']\n",
    "\n",
    "    # Copy the model from GCS\n",
    "    model_filename = 'model.pkl'\n",
    "    gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "    print(gcs_model_filepath)\n",
    "    subprocess.check_call(['gsutil', 'cp', gcs_model_filepath, model_filename],\n",
    "                        stderr=sys.stdout)\n",
    "\n",
    "    with open(model_filename, 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "        \n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if metric_name == 'accuracy':\n",
    "        metric_value = accuracy_score(y_test, y_hat)\n",
    "    elif metric_name == 'recall':\n",
    "        metric_value = recall_score(y_test, y_hat)\n",
    "    else:\n",
    "        metric_name = 'N/A'\n",
    "        metric_value = 0\n",
    "\n",
    "    # Export the metric\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "    }\n",
    "\n",
    "    return (metric_name, metric_value, json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/etl-project-datahub/trainer_image:latest'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n"
     ]
    }
   ],
   "source": [
    "!cat base_image/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      "WORKDIR /app\n",
      "COPY train.py .\n",
      "\n",
      "ENTRYPOINT [\"python\", \"train.py\"]\n"
     ]
    }
   ],
   "source": [
    "!cat train_image/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an empty folder base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_FOLDER = 'base_image'\n",
    "os.makedirs(BASELINE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./base_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./base_image/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 122 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://etl-project-datahub_cloudbuild/source/1599464691.07469-cde00f50db21421ba23ffe86e6d1e972.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/etl-project-datahub/builds/b4a40d09-5505-46ee-ad92-c8c698872017].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/b4a40d09-5505-46ee-ad92-c8c698872017?project=448067079266].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"b4a40d09-5505-46ee-ad92-c8c698872017\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://etl-project-datahub_cloudbuild/source/1599464691.07469-cde00f50db21421ba23ffe86e6d1e972.tgz#1599464691523695\n",
      "Copying gs://etl-project-datahub_cloudbuild/source/1599464691.07469-cde00f50db21421ba23ffe86e6d1e972.tgz#1599464691523695...\n",
      "/ [1 files][  228.0 B/  228.0 B]                                                \n",
      "Operation completed over 1 objects/228.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      " ---> Running in 313bde06dcde\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.29.0)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.9.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.17.2)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (47.3.1.post20200616)\n",
      "Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.2.5) (2.20)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==0.2.5) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.12.3)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.51.0)\n",
      "Building wheels for collected packages: fire, kfp, termcolor, kfp-server-api, argo-models, tabulate, strip-hints\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=21ceec80e635c321833c8b60441e3085cc47c0efb5db116f2ce044f4bc3e2cc8\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159978 sha256=0297c7a9ca80f9920d1c4cdd92b5c58080ff6cfac07584019e0bf60fac685b0f\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=9f38f22017dea8f9d8326430c530a6b8817032f39a1e4c57fe2f576014e4b205\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102468 sha256=86d11a0dfb3f9b483123199f02b23f6db015bbc7d36523eab00c1e7a29f79845\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=bf58e6798d0b246aa5d379783df696721a0d7e66168c79e0e021b5e38b3ae490\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23378 sha256=110fe5d62b45b3960930339ef3db75cc93542c2ab7de397f66c3177f3501afda\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=71b0a378a4f01d9fd6cd0115310edf3cb0c0242e97c4f5aab2f49d11480bef32\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "Successfully built fire kfp termcolor kfp-server-api argo-models tabulate strip-hints\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: jupyterlab-git 0.10.1 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: distributed 2.19.0 has requirement cloudpickle>=1.3.0, but you'll have cloudpickle 1.1.1 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, scikit-learn, pandas, urllib3, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, tabulate, click, Deprecated, strip-hints, kfp\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 11.0.0\n",
      "    Uninstalling kubernetes-11.0.0:\n",
      "      Successfully uninstalled kubernetes-11.0.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.4.1\n",
      "    Uninstalling cloudpickle-1.4.1:\n",
      "      Successfully uninstalled cloudpickle-1.4.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "Successfully installed Deprecated-1.2.10 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 fire-0.3.1 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests-toolbelt-0.9.1 scikit-learn-0.20.4 strip-hints-0.1.9 tabulate-0.8.3 termcolor-1.1.0 urllib3-1.24.3\n",
      "Removing intermediate container 313bde06dcde\n",
      " ---> f9b5218115bf\n",
      "Successfully built f9b5218115bf\n",
      "Successfully tagged gcr.io/etl-project-datahub/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/etl-project-datahub/base_image:latest\n",
      "The push refers to repository [gcr.io/etl-project-datahub/base_image]\n",
      "ed7e074bddc2: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "c51fe61c6231: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "ed7e074bddc2: Pushed\n",
      "latest: digest: sha256:9f5a641cf364025966b51c7970b1b147e559b5cf5f7714e991f8a8695afa05da size: 4293\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                           STATUS\n",
      "b4a40d09-5505-46ee-ad92-c8c698872017  2020-09-07T07:44:51+00:00  3M45S     gs://etl-project-datahub_cloudbuild/source/1599464691.07469-cde00f50db21421ba23ffe86e6d1e972.tgz  gcr.io/etl-project-datahub/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and deploying the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the below constants with the settings reflecting your lab environment.\n",
    "\n",
    "* ##### REGION - the compute region for AI Platform Training and Prediction\n",
    "* ##### ARTIFACT_STORE - the GCS bucket created during installation of AI Platform Pipelines. The bucket name starts with the hostedkfp-default- prefix.\n",
    "* ##### ENDPOINT - set the ENDPOINT constant to the endpoint to your AI Platform Pipelines instance. Then endpoint to the * ##### AI Platform Pipelines instance can be found on the <a href=\"https://console.cloud.google.com/ai-platform/pipelines/clusters\">AI Platform Pipelines</a> page in the Google Cloud Console.\n",
    "\n",
    "* ##### Open the SETTINGS for your instance\n",
    "\n",
    "* ##### Use the value of the host variable in the Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SKD section of the SETTINGS window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: kfp in /opt/conda/lib/python3.7/site-packages (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: kubernetes<12.0.0,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (11.0.0)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.30.0)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /opt/conda/lib/python3.7/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /opt/conda/lib/python3.7/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied, skipping upgrade: Deprecated in /opt/conda/lib/python3.7/site-packages (from kfp) (1.2.10)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle in /opt/conda/lib/python3.7/site-packages (from kfp) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: strip-hints in /opt/conda/lib/python3.7/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied, skipping upgrade: kfp-server-api<2.0.0,>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-toolbelt>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.20.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=14.05.14 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.24.2 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.3 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (49.6.0.post20200814)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (20.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp) (0.35.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<12.0.0,>=8.0.0->kfp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<12.0.0,>=8.0.0->kfp) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.22.1)\n",
      "Requirement already satisfied, skipping upgrade: google-crc32c<0.2dev,>=0.1.0; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp) (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.13.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<0.2dev,>=0.1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp) (1.14.1)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<0.2dev,>=0.1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp) (2.20)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ENDPOINT = 'caa07ec40e216a8-dot-us-central2.pipelines.googleusercontent.com'\n",
    "#'19a5aed0f754a516-dot-us-central2.pipelines.googleusercontent.com'\n",
    "ARTIFACT_STORE_URI = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USE_KFP_SA=False\n",
      "env: BASE_IMAGE=gcr.io/etl-project-datahub/base_image:latest\n",
      "env: TRAINER_IMAGE=gcr.io/etl-project-datahub/trainer_image:latest\n",
      "env: COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
      "env: RUNTIME_VERSION=1.15\n",
      "env: PYTHON_VERSION=3.7\n"
     ]
    }
   ],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "client = kfp.Client(host='caa07ec40e216a8-dot-us-central2.pipelines.googleusercontent.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/dsl-compile\n"
     ]
    }
   ],
   "source": [
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/amyris_pipeline.py --output amyris_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: argoproj.io/v1alpha1\n",
      "kind: Workflow\n",
      "metadata:\n",
      "  generateName: amyris-classifier-training-\n",
      "  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.0, pipelines.kubeflow.org/pipeline_compilation_time: '2020-09-07T13:06:28.167611',\n",
      "    pipelines.kubeflow.org/pipeline_spec: '{\"description\": \"The pipeline training\n",
      "      and deploying the Amyris classifierpipeline_yaml\", \"inputs\": [{\"name\": \"project_id\"},\n",
      "      {\"name\": \"region\"}, {\"name\": \"gcs_root\"}, {\"name\": \"evaluation_metric_name\"},\n",
      "      {\"name\": \"evaluation_metric_threshold\"}, {\"name\": \"model_id\"}, {\"name\": \"version_id\"},\n",
      "      {\"name\": \"replace_existing_version\"}, {\"default\": \"\\n{\\n    \\\"hyperparameters\\\":  {\\n        \\\"goal\\\":\n"
     ]
    }
   ],
   "source": [
    "!head amyris_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deploy pipeline package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 701161a5-26df-4c7a-81b6-4b6f5fc5ab99 has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           701161a5-26df-4c7a-81b6-4b6f5fc5ab99\n",
      "Name         amyris_pipeline_RF8\n",
      "Description\n",
      "Uploaded at  2020-09-07T13:06:39+00:00\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| Parameter Name              | Default Value                                         |\n",
      "+=============================+=======================================================+\n",
      "| project_id                  |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| region                      |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| gcs_root                    |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| evaluation_metric_name      |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| evaluation_metric_threshold |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| model_id                    |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| version_id                  |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| replace_existing_version    |                                                       |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| hypertune_settings          | {                                                     |\n",
      "|                             |     \"hyperparameters\":  {                             |\n",
      "|                             |         \"goal\": \"MAXIMIZE\",                           |\n",
      "|                             |         \"maxTrials\": 3,                               |\n",
      "|                             |         \"maxParallelTrials\": 3,                       |\n",
      "|                             |         \"hyperparameterMetricTag\": \"accuracy\",        |\n",
      "|                             |         \"enableTrialEarlyStopping\": True,             |\n",
      "|                             |         \"algorithm\": \"RANDOM_SEARCH\",                 |\n",
      "|                             |         \"params\": [                                   |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"n_estimators\",      |\n",
      "|                             |                 \"type\": \"INTEGER\",                    |\n",
      "|                             |                 \"minValue\": 10,                       |\n",
      "|                             |                 \"maxValue\": 200,                      |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"max_leaf_nodes\",    |\n",
      "|                             |                 \"type\": \"INTEGER\",                    |\n",
      "|                             |                 \"minValue\": 10,                       |\n",
      "|                             |                 \"maxValue\": 500,                      |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"max_depth\",         |\n",
      "|                             |                 \"type\": \"INTEGER\",                    |\n",
      "|                             |                 \"minValue\": 3,                        |\n",
      "|                             |                 \"maxValue\": 20,                       |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"min_samples_split\", |\n",
      "|                             |                 \"type\": \"DISCRETE\",                   |\n",
      "|                             |                 \"discreteValues\": [2,5,10]            |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"min_samples_leaf\",  |\n",
      "|                             |                 \"type\": \"INTEGER\",                    |\n",
      "|                             |                 \"minValue\": 10,                       |\n",
      "|                             |                 \"maxValue\": 500,                      |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"max_features\",      |\n",
      "|                             |                 \"type\": \"DOUBLE\",                     |\n",
      "|                             |                 \"minValue\": 0.5,                      |\n",
      "|                             |                 \"maxValue\": 1.0,                      |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"class_weight\",      |\n",
      "|                             |                 \"type\": \"CATEGORICAL\",                |\n",
      "|                             |                 \"categoricalValues\": [                |\n",
      "|                             |                               \"balanced\",             |\n",
      "|                             |                               \"balanced_subsample\"    |\n",
      "|                             |                           ]                           |\n",
      "|                             |             },                                        |\n",
      "|                             |                                                       |\n",
      "|                             |              {                                        |\n",
      "|                             |                 \"parameterName\": \"random_state\",      |\n",
      "|                             |                 \"type\": \"INTEGER\",                    |\n",
      "|                             |                 \"minValue\": 35,                       |\n",
      "|                             |                 \"maxValue\": 75,                       |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "|                             |             },                                        |\n",
      "|                             |             {                                         |\n",
      "|                             |                 \"parameterName\": \"bootstrap\",         |\n",
      "|                             |                 \"type\": \"CATEGORICAL\",                |\n",
      "|                             |                 \"categoricalValues\": [                |\n",
      "|                             |                               \"TRUE\",                 |\n",
      "|                             |                               \"FALSE\"                 |\n",
      "|                             |                           ]                           |\n",
      "|                             |             }                                         |\n",
      "|                             |         ]                                             |\n",
      "|                             |     }                                                 |\n",
      "|                             | }                                                     |\n",
      "+-----------------------------+-------------------------------------------------------+\n",
      "| dataset_location            | US                                                    |\n",
      "+-----------------------------+-------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME='amyris_pipeline_RF8'\n",
    "\n",
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "amyris_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                            | Uploaded at               |\n",
      "+======================================+=================================================+===========================+\n",
      "| 701161a5-26df-4c7a-81b6-4b6f5fc5ab99 | amyris_pipeline_RF8                             | 2020-09-07T13:06:39+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 65886c7c-0c88-4f9a-983b-f1a561cd2276 | amyris_pipeline_RF7                             | 2020-09-07T01:42:27+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 0dd4b8c7-167e-4883-a02c-9ac49b63a6bc | amyris_pipeline_RF6                             | 2020-09-06T23:39:34+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 76e17c3f-8954-4ff5-8099-c3455eb1a723 | amyris_pipeline_RF5                             | 2020-09-06T21:41:43+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| d88efe41-074b-4535-ab43-83864d58ff13 | amyris_pipeline_RF4                             | 2020-09-06T21:23:26+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 9afe138a-6b32-4d3b-ac53-8abb7dad137d | amyris_pipeline_RF3                             | 2020-09-06T20:41:04+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 44c03dea-6c51-4f74-9f9a-3758e31be270 | [Tutorial] DSL - Control structures             | 2020-09-06T19:49:58+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 1e956c77-867c-4f42-b083-d05ec09a3f86 | [Tutorial] Data passing in python components    | 2020-09-06T19:49:57+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 0cc0d4ef-a58f-4a63-8bac-fa382f494b83 | [Demo] TFX - Iris classification pipeline       | 2020-09-06T19:49:56+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 3235b9dd-e6ef-4ca8-8940-4899c4426297 | [Demo] TFX - Taxi tip prediction model trainer  | 2020-09-06T19:49:55+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| cb18c6b0-1903-4fd4-800f-9a5293fc39bc | [Demo] XGBoost - Training with confusion matrix | 2020-09-06T19:49:54+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='701161a5-26df-4c7a-81b6-4b6f5fc5ab99'\n",
    "#'65886c7c-0c88-4f9a-983b-f1a561cd2276'\n",
    "#'0dd4b8c7-167e-4883-a02c-9ac49b63a6bc'\n",
    "#'76e17c3f-8954-4ff5-8099-c3455eb1a723'\n",
    "#'d88efe41-074b-4535-ab43-83864d58ff13'\n",
    "#'9afe138a-6b32-4d3b-ac53-8abb7dad137d'\n",
    "##'0906289c-a5dd-4292-ac8c-71c8925dac26'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'amyrisRF_RF2'\n",
    "RUN_ID = 'Run_001'\n",
    "EVALUATION_METRIC = 'accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD = '0.69'\n",
    "MODEL_ID = 'amyris_RFClassifiervRFC1' #'amyris_endtoendRF2'\n",
    "VERSION_ID = 'v01'\n",
    "REPLACE_EXISTING_VERSION = 'True'\n",
    "\n",
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating experiment amyrisRF_RF2.\n",
      "Run 60c3bfa6-1180-4b9f-8862-211c20c8f50c is submitted\n",
      "+--------------------------------------+---------+----------+---------------------------+\n",
      "| run id                               | name    | status   | created at                |\n",
      "+======================================+=========+==========+===========================+\n",
      "| 60c3bfa6-1180-4b9f-8862-211c20c8f50c | Run_001 |          | 2020-09-07T13:07:47+00:00 |\n",
      "+--------------------------------------+---------+----------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$GCS_STAGING_PATH \\\n",
    "region=$REGION \\\n",
    "evaluation_metric_name=$EVALUATION_METRIC \\\n",
    "evaluation_metric_threshold=$EVALUATION_METRIC_THRESHOLD \\\n",
    "model_id=$MODEL_ID \\\n",
    "version_id=$VERSION_ID \\\n",
    "replace_existing_version=$REPLACE_EXISTING_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI/CD Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'etl-project-datahub'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENDPOINT = 'caa07ec40e216a8-dot-us-central2.pipelines.googleusercontent.com' ##'<YOUR_ENDPOINT>'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFP_CLI_FOLDER = 'kfp-cli'\n",
    "os.makedirs(KFP_CLI_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kfp-cli/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {KFP_CLI_FOLDER}/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install kfp==0.2.5\n",
    "ENTRYPOINT [\"/bin/bash\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install kfp==0.2.5\n",
      "ENTRYPOINT [\"/bin/bash\"]\n"
     ]
    }
   ],
   "source": [
    "!cat kfp-cli/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='kfp-cli'\n",
    "TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 103 bytes before compression.\n",
      "Uploading tarball of [kfp-cli] to [gs://etl-project-datahub_cloudbuild/source/1599485809.305995-e348c57df61f4e7c8d4b0ab6e72bca98.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/etl-project-datahub/builds/2db3bc47-5725-42c2-b984-8e3e877d8700].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/2db3bc47-5725-42c2-b984-8e3e877d8700?project=448067079266].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"2db3bc47-5725-42c2-b984-8e3e877d8700\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://etl-project-datahub_cloudbuild/source/1599485809.305995-e348c57df61f4e7c8d4b0ab6e72bca98.tgz#1599485809773088\n",
      "Copying gs://etl-project-datahub_cloudbuild/source/1599485809.305995-e348c57df61f4e7c8d4b0ab6e72bca98.tgz#1599485809773088...\n",
      "/ [1 files][  226.0 B/  226.0 B]                                                \n",
      "Operation completed over 1 objects/226.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/3 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/3 : RUN pip install kfp==0.2.5\n",
      " ---> Running in d28f21856dc4\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.15.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.8.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.3.1)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.29.0)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.7.1)\n",
      "Requirement already satisfied: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.9.2)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.17.2)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (0.5.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.24.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (47.3.1.post20200616)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.14.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (1.7.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.11.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.34.2)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.16.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.2.5) (2.20)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==0.2.5) (3.1.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (2020.1)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.12.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.51.0)\n",
      "Building wheels for collected packages: kfp, kfp-server-api, argo-models, tabulate, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159978 sha256=fbeda2ed4f68f24901c4ae79be05f41ae0ec38b5b45d724cf45c863986ce7306\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102468 sha256=d01d86fc5f3b92f2866b0cb7c236aee477c0872adb0839dfc20f1d8cabe87136\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=1ca7a1f3a5de00e5e71b7e9c8a610abdae07d886e70f14ee3b30c159d4820996\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23378 sha256=1a31573c5d56cfdcf1e1215ed9b067419630369f730015b379711915a9926e81\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=d4d41056a1975f3d6f482e129d18f798194f3fd2419c4136d197246b3582d5da\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "Successfully built kfp kfp-server-api argo-models tabulate strip-hints\n",
      "\u001b[91mERROR: jupyterlab-git 0.10.1 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: distributed 2.19.0 has requirement cloudpickle>=1.3.0, but you'll have cloudpickle 1.1.1 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: urllib3, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, tabulate, click, Deprecated, strip-hints, kfp\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 11.0.0\n",
      "    Uninstalling kubernetes-11.0.0:\n",
      "      Successfully uninstalled kubernetes-11.0.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.4.1\n",
      "    Uninstalling cloudpickle-1.4.1:\n",
      "      Successfully uninstalled cloudpickle-1.4.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "Successfully installed Deprecated-1.2.10 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 requests-toolbelt-0.9.1 strip-hints-0.1.9 tabulate-0.8.3 urllib3-1.24.3\n",
      "Removing intermediate container d28f21856dc4\n",
      " ---> d04e14ec0e0e\n",
      "Step 3/3 : ENTRYPOINT [\"/bin/bash\"]\n",
      " ---> Running in 25b281aec212\n",
      "Removing intermediate container 25b281aec212\n",
      " ---> c0c0d5f28e79\n",
      "Successfully built c0c0d5f28e79\n",
      "Successfully tagged gcr.io/etl-project-datahub/kfp-cli:latest\n",
      "PUSH\n",
      "Pushing gcr.io/etl-project-datahub/kfp-cli:latest\n",
      "The push refers to repository [gcr.io/etl-project-datahub/kfp-cli]\n",
      "a11cb8240dc3: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "badaf1bc8335: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "c51fe61c6231: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "a11cb8240dc3: Pushed\n",
      "latest: digest: sha256:5c74afb0298bbab4b97ada8f09e6032985b9a99db6641430a43f296c4898e839 size: 4292\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                             IMAGES                                        STATUS\n",
      "2db3bc47-5725-42c2-b984-8e3e877d8700  2020-09-07T13:36:49+00:00  3M26S     gs://etl-project-datahub_cloudbuild/source/1599485809.305995-e348c57df61f4e7c8d4b0ab6e72bca98.tgz  gcr.io/etl-project-datahub/kfp-cli (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag {IMAGE_URI} kfp-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_ENDPOINT={},\\\n",
    "_TRAINER_IMAGE_NAME=trainer_image,\\\n",
    "_BASE_IMAGE_NAME=base_image,\\\n",
    "TAG_NAME=latest,\\\n",
    "_PIPELINE_FOLDER=.,\\\n",
    "_PIPELINE_DSL=amyris_pipeline.py,\\\n",
    "_PIPELINE_PACKAGE=amyris_pipeline.yaml,\\\n",
    "_PIPELINE_NAME=amyris_pipeline_RF8,\\\n",
    "_RUNTIME_VERSION=1.15,\\\n",
    "_PYTHON_VERSION=3.7,\\\n",
    "_USE_KFP_SA=True,\\\n",
    "_COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
    "\"\"\".format(ENDPOINT).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "# Submits a Cloud Build job that builds and deploys\n",
    "# the pipelines and pipelines components \n",
    "#\n",
    "# Build and deploy a TFX pipeline. This is an interim solution till tfx CLI fully \n",
    "# supports automated building and deploying.\n",
    "# \n",
    "\n",
    "steps:\n",
    "# Build the trainer image\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME', '.']\n",
    "  dir: $_PIPELINE_FOLDER/train_image\n",
    "  \n",
    "# Build the base image for lightweight components\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME', '.']\n",
    "  dir: $_PIPELINE_FOLDER/base_image\n",
    "\n",
    "# Compile the pipeline\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    dsl-compile --py $_PIPELINE_DSL --output $_PIPELINE_PACKAGE\n",
    "  env:\n",
    "  - 'BASE_IMAGE=gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME'\n",
    "  - 'TRAINER_IMAGE=gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME'\n",
    "  - 'RUNTIME_VERSION=$_RUNTIME_VERSION'\n",
    "  - 'PYTHON_VERSION=$_PYTHON_VERSION'\n",
    "  - 'COMPONENT_URL_SEARCH_PREFIX=$_COMPONENT_URL_SEARCH_PREFIX'\n",
    "  - 'USE_KFP_SA=$_USE_KFP_SA'\n",
    "  dir: $_PIPELINE_FOLDER/pipeline\n",
    "  \n",
    " # Upload the pipeline\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    kfp --endpoint $_ENDPOINT pipeline upload -p ${_PIPELINE_NAME}_$TAG_NAME $_PIPELINE_PACKAGE\n",
    "  dir: $_PIPELINE_FOLDER/pipeline\n",
    "\n",
    "\n",
    "# Push the images to Container Registry \n",
    "images: ['gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME', 'gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 94 file(s) totalling 28.3 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://etl-project-datahub_cloudbuild/source/1599486609.236557-e99c5af839c149b78a99e56f30e178d8.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/etl-project-datahub/builds/3abcbdfe-ff4f-4674-bdde-5c501b7bc12d].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/3abcbdfe-ff4f-4674-bdde-5c501b7bc12d?project=448067079266].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"3abcbdfe-ff4f-4674-bdde-5c501b7bc12d\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://etl-project-datahub_cloudbuild/source/1599486609.236557-e99c5af839c149b78a99e56f30e178d8.tgz#1599486611822088\n",
      "Copying gs://etl-project-datahub_cloudbuild/source/1599486609.236557-e99c5af839c149b78a99e56f30e178d8.tgz#1599486611822088...\n",
      "/ [1 files][  9.5 MiB/  9.5 MiB]                                                \n",
      "Operation completed over 1 objects/9.5 MiB.                                      \n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0: Sending build context to Docker daemon  13.31kB\n",
      "Step #0: Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "Step #0: latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "Step #0: d7c3167c320d: Pulling fs layer\n",
      "Step #0: 131f805ec7fd: Pulling fs layer\n",
      "Step #0: 322ed380e680: Pulling fs layer\n",
      "Step #0: 6ac240b13098: Pulling fs layer\n",
      "Step #0: 9ce3a9266402: Pulling fs layer\n",
      "Step #0: 72c706dfac1d: Pulling fs layer\n",
      "Step #0: 6383427606e5: Pulling fs layer\n",
      "Step #0: 3e8b21666cec: Pulling fs layer\n",
      "Step #0: 358bb5d659ed: Pulling fs layer\n",
      "Step #0: 8ade7556a8f1: Pulling fs layer\n",
      "Step #0: b2ebb7e1223e: Pulling fs layer\n",
      "Step #0: 8d5d283ad922: Pulling fs layer\n",
      "Step #0: 14c0fd48a5f3: Pulling fs layer\n",
      "Step #0: ceaad5dc04d2: Pulling fs layer\n",
      "Step #0: c1074350f761: Pulling fs layer\n",
      "Step #0: 687ad0b9a318: Pulling fs layer\n",
      "Step #0: d2365d2ee19a: Pulling fs layer\n",
      "Step #0: 5095b04f1d98: Pulling fs layer\n",
      "Step #0: 6ac240b13098: Waiting\n",
      "Step #0: 9ce3a9266402: Waiting\n",
      "Step #0: 72c706dfac1d: Waiting\n",
      "Step #0: 6383427606e5: Waiting\n",
      "Step #0: 3e8b21666cec: Waiting\n",
      "Step #0: 358bb5d659ed: Waiting\n",
      "Step #0: 8ade7556a8f1: Waiting\n",
      "Step #0: b2ebb7e1223e: Waiting\n",
      "Step #0: 8d5d283ad922: Waiting\n",
      "Step #0: 14c0fd48a5f3: Waiting\n",
      "Step #0: ceaad5dc04d2: Waiting\n",
      "Step #0: c1074350f761: Waiting\n",
      "Step #0: 687ad0b9a318: Waiting\n",
      "Step #0: d2365d2ee19a: Waiting\n",
      "Step #0: 5095b04f1d98: Waiting\n",
      "Step #0: 131f805ec7fd: Verifying Checksum\n",
      "Step #0: 131f805ec7fd: Download complete\n",
      "Step #0: 322ed380e680: Verifying Checksum\n",
      "Step #0: 322ed380e680: Download complete\n",
      "Step #0: 6ac240b13098: Verifying Checksum\n",
      "Step #0: 6ac240b13098: Download complete\n",
      "Step #0: d7c3167c320d: Verifying Checksum\n",
      "Step #0: d7c3167c320d: Download complete\n",
      "Step #0: 6383427606e5: Verifying Checksum\n",
      "Step #0: 6383427606e5: Download complete\n",
      "Step #0: 72c706dfac1d: Verifying Checksum\n",
      "Step #0: 72c706dfac1d: Download complete\n",
      "Step #0: 358bb5d659ed: Verifying Checksum\n",
      "Step #0: 358bb5d659ed: Download complete\n",
      "Step #0: 8ade7556a8f1: Verifying Checksum\n",
      "Step #0: 8ade7556a8f1: Download complete\n",
      "Step #0: b2ebb7e1223e: Verifying Checksum\n",
      "Step #0: b2ebb7e1223e: Download complete\n",
      "Step #0: 8d5d283ad922: Verifying Checksum\n",
      "Step #0: 8d5d283ad922: Download complete\n",
      "Step #0: 3e8b21666cec: Verifying Checksum\n",
      "Step #0: 3e8b21666cec: Download complete\n",
      "Step #0: 14c0fd48a5f3: Verifying Checksum\n",
      "Step #0: 14c0fd48a5f3: Download complete\n",
      "Step #0: ceaad5dc04d2: Verifying Checksum\n",
      "Step #0: ceaad5dc04d2: Download complete\n",
      "Step #0: c1074350f761: Verifying Checksum\n",
      "Step #0: c1074350f761: Download complete\n",
      "Step #0: 687ad0b9a318: Verifying Checksum\n",
      "Step #0: 687ad0b9a318: Download complete\n",
      "Step #0: 5095b04f1d98: Verifying Checksum\n",
      "Step #0: 5095b04f1d98: Download complete\n",
      "Step #0: 9ce3a9266402: Verifying Checksum\n",
      "Step #0: 9ce3a9266402: Download complete\n",
      "Step #0: d7c3167c320d: Pull complete\n",
      "Step #0: 131f805ec7fd: Pull complete\n",
      "Step #0: d2365d2ee19a: Verifying Checksum\n",
      "Step #0: d2365d2ee19a: Download complete\n",
      "Step #0: 322ed380e680: Pull complete\n",
      "Step #0: 6ac240b13098: Pull complete\n",
      "Step #0: 9ce3a9266402: Pull complete\n",
      "Step #0: 72c706dfac1d: Pull complete\n",
      "Step #0: 6383427606e5: Pull complete\n",
      "Step #0: 3e8b21666cec: Pull complete\n",
      "Step #0: 358bb5d659ed: Pull complete\n",
      "Step #0: 8ade7556a8f1: Pull complete\n",
      "Step #0: b2ebb7e1223e: Pull complete\n",
      "Step #0: 8d5d283ad922: Pull complete\n",
      "Step #0: 14c0fd48a5f3: Pull complete\n",
      "Step #0: ceaad5dc04d2: Pull complete\n",
      "Step #0: c1074350f761: Pull complete\n",
      "Step #0: 687ad0b9a318: Pull complete\n",
      "Step #0: d2365d2ee19a: Pull complete\n",
      "Step #0: 5095b04f1d98: Pull complete\n",
      "Step #0: Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Step #0: Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      "Step #0:  ---> d8706668f140\n",
      "Step #0: Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      "Step #0:  ---> Running in f111a7a279ae\n",
      "Step #0: Collecting fire\n",
      "Step #0:   Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Step #0: Collecting cloudml-hypertune\n",
      "Step #0:   Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Step #0: Collecting scikit-learn==0.20.4\n",
      "Step #0:   Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Step #0: Collecting pandas==0.24.2\n",
      "Step #0:   Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Step #0: Collecting termcolor\n",
      "Step #0:   Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Step #0: Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Step #0: Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "Step #0:   Building wheel for fire (setup.py): started\n",
      "Step #0:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=f68b86665545a7741e32fda27fb6d67015dfbb5a618ae414cc42b7daebe6f7f1\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "Step #0:   Building wheel for cloudml-hypertune (setup.py): started\n",
      "Step #0:   Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=aebdc3a0be9feff7925d7d547c64d10c40d21fc34dec5a0e1d5aed71d83a5902\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Step #0:   Building wheel for termcolor (setup.py): started\n",
      "Step #0:   Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=bd452ab72f0acc8c945f8efee00b41f07065cca91d03cc829758a8232b5ffaf5\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Step #0: Successfully built fire cloudml-hypertune termcolor\n",
      "Step #0: \u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "Step #0: \u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "Step #0: \u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "Step #0:   Attempting uninstall: scikit-learn\n",
      "Step #0:     Found existing installation: scikit-learn 0.23.1\n",
      "Step #0:     Uninstalling scikit-learn-0.23.1:\n",
      "Step #0:       Successfully uninstalled scikit-learn-0.23.1\n",
      "Step #0:   Attempting uninstall: pandas\n",
      "Step #0:     Found existing installation: pandas 1.0.5\n",
      "Step #0:     Uninstalling pandas-1.0.5:\n",
      "Step #0:       Successfully uninstalled pandas-1.0.5\n",
      "Step #0: Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Step #0: Removing intermediate container f111a7a279ae\n",
      "Step #0:  ---> 011c37beff9a\n",
      "Step #0: Step 3/5 : WORKDIR /app\n",
      "Step #0:  ---> Running in 69645ec6facb\n",
      "Step #0: Removing intermediate container 69645ec6facb\n",
      "Step #0:  ---> b360ed4fe6ab\n",
      "Step #0: Step 4/5 : COPY train.py .\n",
      "Step #0:  ---> e723f255f78b\n",
      "Step #0: Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      "Step #0:  ---> Running in 1aa1afef1821\n",
      "Step #0: Removing intermediate container 1aa1afef1821\n",
      "Step #0:  ---> d84fdd0f5acb\n",
      "Step #0: Successfully built d84fdd0f5acb\n",
      "Step #0: Successfully tagged gcr.io/etl-project-datahub/trainer_image:latest\n",
      "Finished Step #0\n",
      "Starting Step #1\n",
      "Step #1: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1: Sending build context to Docker daemon  2.048kB\n",
      "Step #1: Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "Step #1:  ---> d8706668f140\n",
      "Step #1: Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      "Step #1:  ---> Running in 39e44eaa7762\n",
      "Step #1: Collecting fire\n",
      "Step #1:   Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Step #1: Collecting scikit-learn==0.20.4\n",
      "Step #1:   Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Step #1: Collecting pandas==0.24.2\n",
      "Step #1:   Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Step #1: Collecting kfp==0.2.5\n",
      "Step #1:   Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Step #1: Collecting termcolor\n",
      "Step #1:   Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Step #1: Collecting urllib3<1.25,>=1.15\n",
      "Step #1:   Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2020.6.20)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.3.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.29.0)\n",
      "Step #1: Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "Step #1:   Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.7.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.9.2)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.17.2)\n",
      "Step #1: Collecting requests_toolbelt>=0.8.0\n",
      "Step #1:   Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Step #1: Collecting cloudpickle==1.1.1\n",
      "Step #1:   Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Step #1: Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "Step #1:   Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Step #1: Collecting argo-models==2.2.1a\n",
      "Step #1:   Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Step #1: Collecting tabulate==0.8.3\n",
      "Step #1:   Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Step #1: Collecting click==7.0\n",
      "Step #1:   Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Step #1: Collecting Deprecated\n",
      "Step #1:   Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Step #1: Collecting strip-hints\n",
      "Step #1:   Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (0.5.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.3.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.24.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (47.3.1.post20200616)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.14.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.6)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.1.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (1.7.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (19.3.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.16.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.11.2)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.34.2)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.16.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (2.10)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.4)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.0.1)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.2.5) (2.20)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==0.2.5) (3.1.0)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.12.3)\n",
      "Step #1: Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.51.0)\n",
      "Step #1: Building wheels for collected packages: fire, kfp, termcolor, kfp-server-api, argo-models, tabulate, strip-hints\n",
      "Step #1:   Building wheel for fire (setup.py): started\n",
      "Step #1:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=680f085d59fe43c5d7cb7590981b7faf71dc27f0a4dec83e9b5c325dc74d2d03\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "Step #1:   Building wheel for kfp (setup.py): started\n",
      "Step #1:   Building wheel for kfp (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159978 sha256=1f8b3f8d85316905b6966a4a7791b8f054774a2fc66eb8efea2156b1bb64175f\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "Step #1:   Building wheel for termcolor (setup.py): started\n",
      "Step #1:   Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=1166ab9179cf1b9b0e27634c77c999d90b0b6efc49acc02da20afb6c5efcd4c1\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Step #1:   Building wheel for kfp-server-api (setup.py): started\n",
      "Step #1:   Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102468 sha256=d69653cfb77584c18bd9ca73bf548aeaf4703d0cea7dc0a4ff24e085830a4570\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "Step #1:   Building wheel for argo-models (setup.py): started\n",
      "Step #1:   Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=4ba2fa7288337e443d2467e19b03153513ace41b0676285177e34377905dd61f\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "Step #1:   Building wheel for tabulate (setup.py): started\n",
      "Step #1:   Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23378 sha256=98adb8a87662ac19a2140c7f49f1427823d87fa6335601f4b946e06750c89646\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "Step #1:   Building wheel for strip-hints (setup.py): started\n",
      "Step #1:   Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=b501585824e30ae0e6e9775f9b9d0d520d780ea98797793deb4474cb40151078\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "Step #1: Successfully built fire kfp termcolor kfp-server-api argo-models tabulate strip-hints\n",
      "Step #1: \u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "Step #1: \u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "Step #1: \u001b[0m\u001b[91mERROR: jupyterlab-git 0.10.1 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\n",
      "Step #1: \u001b[0m\u001b[91mERROR: distributed 2.19.0 has requirement cloudpickle>=1.3.0, but you'll have cloudpickle 1.1.1 which is incompatible.\n",
      "Step #1: \u001b[0mInstalling collected packages: termcolor, fire, scikit-learn, pandas, urllib3, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, tabulate, click, Deprecated, strip-hints, kfp\n",
      "Step #1:   Attempting uninstall: scikit-learn\n",
      "Step #1:     Found existing installation: scikit-learn 0.23.1\n",
      "Step #1:     Uninstalling scikit-learn-0.23.1:\n",
      "Step #1:       Successfully uninstalled scikit-learn-0.23.1\n",
      "Step #1:   Attempting uninstall: pandas\n",
      "Step #1:     Found existing installation: pandas 1.0.5\n",
      "Step #1:     Uninstalling pandas-1.0.5:\n",
      "Step #1:       Successfully uninstalled pandas-1.0.5\n",
      "Step #1:   Attempting uninstall: urllib3\n",
      "Step #1:     Found existing installation: urllib3 1.25.9\n",
      "Step #1:     Uninstalling urllib3-1.25.9:\n",
      "Step #1:       Successfully uninstalled urllib3-1.25.9\n",
      "Step #1:   Attempting uninstall: kubernetes\n",
      "Step #1:     Found existing installation: kubernetes 11.0.0\n",
      "Step #1:     Uninstalling kubernetes-11.0.0:\n",
      "Step #1:       Successfully uninstalled kubernetes-11.0.0\n",
      "Step #1:   Attempting uninstall: cloudpickle\n",
      "Step #1:     Found existing installation: cloudpickle 1.4.1\n",
      "Step #1:     Uninstalling cloudpickle-1.4.1:\n",
      "Step #1:       Successfully uninstalled cloudpickle-1.4.1\n",
      "Step #1:   Attempting uninstall: click\n",
      "Step #1:     Found existing installation: click 7.1.2\n",
      "Step #1:     Uninstalling click-7.1.2:\n",
      "Step #1:       Successfully uninstalled click-7.1.2\n",
      "Step #1: Successfully installed Deprecated-1.2.10 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 fire-0.3.1 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests-toolbelt-0.9.1 scikit-learn-0.20.4 strip-hints-0.1.9 tabulate-0.8.3 termcolor-1.1.0 urllib3-1.24.3\n",
      "Step #1: Removing intermediate container 39e44eaa7762\n",
      "Step #1:  ---> 2a9030faa50d\n",
      "Step #1: Successfully built 2a9030faa50d\n",
      "Step #1: Successfully tagged gcr.io/etl-project-datahub/base_image:latest\n",
      "Finished Step #1\n",
      "Starting Step #2\n",
      "Step #2: Pulling image: gcr.io/etl-project-datahub/kfp-cli\n",
      "Step #2: Using default tag: latest\n",
      "Step #2: latest: Pulling from etl-project-datahub/kfp-cli\n",
      "Step #2: d7c3167c320d: Already exists\n",
      "Step #2: 131f805ec7fd: Already exists\n",
      "Step #2: 322ed380e680: Already exists\n",
      "Step #2: 6ac240b13098: Already exists\n",
      "Step #2: 9ce3a9266402: Already exists\n",
      "Step #2: 72c706dfac1d: Already exists\n",
      "Step #2: 6383427606e5: Already exists\n",
      "Step #2: 3e8b21666cec: Already exists\n",
      "Step #2: 358bb5d659ed: Already exists\n",
      "Step #2: 8ade7556a8f1: Already exists\n",
      "Step #2: b2ebb7e1223e: Already exists\n",
      "Step #2: 8d5d283ad922: Already exists\n",
      "Step #2: 14c0fd48a5f3: Already exists\n",
      "Step #2: ceaad5dc04d2: Already exists\n",
      "Step #2: c1074350f761: Already exists\n",
      "Step #2: 687ad0b9a318: Already exists\n",
      "Step #2: d2365d2ee19a: Already exists\n",
      "Step #2: 5095b04f1d98: Already exists\n",
      "Step #2: 1a233cb861cc: Pulling fs layer\n",
      "Step #2: 1a233cb861cc: Verifying Checksum\n",
      "Step #2: 1a233cb861cc: Download complete\n",
      "Step #2: 1a233cb861cc: Pull complete\n",
      "Step #2: Digest: sha256:5c74afb0298bbab4b97ada8f09e6032985b9a99db6641430a43f296c4898e839\n",
      "Step #2: Status: Downloaded newer image for gcr.io/etl-project-datahub/kfp-cli:latest\n",
      "Step #2: gcr.io/etl-project-datahub/kfp-cli:latest\n",
      "Finished Step #2\n",
      "Starting Step #3\n",
      "Step #3: Already have image (with digest): gcr.io/etl-project-datahub/kfp-cli\n",
      "Step #3: Pipeline c8e2f002-eecb-4897-98ca-cd138b7d7b09 has been submitted\n",
      "Step #3: \n",
      "Step #3: Pipeline Details\n",
      "Step #3: ------------------\n",
      "Step #3: ID           c8e2f002-eecb-4897-98ca-cd138b7d7b09\n",
      "Step #3: Name         amyris_pipeline_RF8_latest\n",
      "Step #3: Description\n",
      "Step #3: Uploaded at  2020-09-07T13:54:18+00:00\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | Parameter Name              | Default Value                                         |\n",
      "Step #3: +=============================+=======================================================+\n",
      "Step #3: | project_id                  |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | region                      |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | gcs_root                    |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | evaluation_metric_name      |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | evaluation_metric_threshold |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | model_id                    |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | version_id                  |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | replace_existing_version    |                                                       |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | hypertune_settings          | {                                                     |\n",
      "Step #3: |                             |     \"hyperparameters\":  {                             |\n",
      "Step #3: |                             |         \"goal\": \"MAXIMIZE\",                           |\n",
      "Step #3: |                             |         \"maxTrials\": 3,                               |\n",
      "Step #3: |                             |         \"maxParallelTrials\": 3,                       |\n",
      "Step #3: |                             |         \"hyperparameterMetricTag\": \"accuracy\",        |\n",
      "Step #3: |                             |         \"enableTrialEarlyStopping\": True,             |\n",
      "Step #3: |                             |         \"algorithm\": \"RANDOM_SEARCH\",                 |\n",
      "Step #3: |                             |         \"params\": [                                   |\n",
      "Step #3: |                             |             {                                         |\n",
      "Step #3: |                             |                 \"parameterName\": \"n_estimators\",      |\n",
      "Step #3: |                             |                 \"type\": \"INTEGER\",                    |\n",
      "Step #3: |                             |                 \"minValue\": 10,                       |\n",
      "Step #3: |                             |                 \"maxValue\": 200,                      |\n",
      "Step #3: |                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "Step #3: |                             |             },                                        |\n",
      "Step #3: |                             |             {                                         |\n",
      "Step #3: |                             |                 \"parameterName\": \"max_leaf_nodes\",    |\n",
      "Step #3: |                             |                 \"type\": \"INTEGER\",                    |\n",
      "Step #3: |                             |                 \"minValue\": 10,                       |\n",
      "Step #3: |                             |                 \"maxValue\": 500,                      |\n",
      "Step #3: |                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "Step #3: |                             |             },                                        |\n",
      "Step #3: |                             |             {                                         |\n",
      "Step #3: |                             |                 \"parameterName\": \"max_depth\",         |\n",
      "Step #3: |                             |                 \"type\": \"INTEGER\",                    |\n",
      "Step #3: |                             |                 \"minValue\": 3,                        |\n",
      "Step #3: |                             |                 \"maxValue\": 20,                       |\n",
      "Step #3: |                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "Step #3: |                             |             },                                        |\n",
      "Step #3: |                             |             {                                         |\n",
      "Step #3: |                             |                 \"parameterName\": \"min_samples_split\", |\n",
      "Step #3: |                             |                 \"type\": \"DISCRETE\",                   |\n",
      "Step #3: |                             |                 \"discreteValues\": [2,5,10]            |\n",
      "Step #3: |                             |             },                                        |\n",
      "Step #3: |                             |             {                                         |\n",
      "Step #3: |                             |                 \"parameterName\": \"min_samples_leaf\",  |\n",
      "Step #3: |                             |                 \"type\": \"INTEGER\",                    |\n",
      "Step #3: |                             |                 \"minValue\": 10,                       |\n",
      "Step #3: |                             |                 \"maxValue\": 500,                      |\n",
      "Step #3: |                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "Step #3: |                             |             },                                        |\n",
      "Step #3: |                             |             {                                         |\n",
      "Step #3: |                             |                 \"parameterName\": \"max_features\",      |\n",
      "Step #3: |                             |                 \"type\": \"DOUBLE\",                     |\n",
      "Step #3: |                             |                 \"minValue\": 0.5,                      |\n",
      "Step #3: |                             |                 \"maxValue\": 1.0,                      |\n",
      "Step #3: |                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "Step #3: |                             |             },                                        |\n",
      "Step #3: |                             |             {                                         |\n",
      "Step #3: |                             |                 \"parameterName\": \"class_weight\",      |\n",
      "Step #3: |                             |                 \"type\": \"CATEGORICAL\",                |\n",
      "Step #3: |                             |                 \"categoricalValues\": [                |\n",
      "Step #3: |                             |                               \"balanced\",             |\n",
      "Step #3: |                             |                               \"balanced_subsample\"    |\n",
      "Step #3: |                             |                           ]                           |\n",
      "Step #3: |                             |             },                                        |\n",
      "Step #3: |                             |                                                       |\n",
      "Step #3: |                             |              {                                        |\n",
      "Step #3: |                             |                 \"parameterName\": \"random_state\",      |\n",
      "Step #3: |                             |                 \"type\": \"INTEGER\",                    |\n",
      "Step #3: |                             |                 \"minValue\": 35,                       |\n",
      "Step #3: |                             |                 \"maxValue\": 75,                       |\n",
      "Step #3: |                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"      |\n",
      "Step #3: |                             |             },                                        |\n",
      "Step #3: |                             |             {                                         |\n",
      "Step #3: |                             |                 \"parameterName\": \"bootstrap\",         |\n",
      "Step #3: |                             |                 \"type\": \"CATEGORICAL\",                |\n",
      "Step #3: |                             |                 \"categoricalValues\": [                |\n",
      "Step #3: |                             |                               \"TRUE\",                 |\n",
      "Step #3: |                             |                               \"FALSE\"                 |\n",
      "Step #3: |                             |                           ]                           |\n",
      "Step #3: |                             |             }                                         |\n",
      "Step #3: |                             |         ]                                             |\n",
      "Step #3: |                             |     }                                                 |\n",
      "Step #3: |                             | }                                                     |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Step #3: | dataset_location            | US                                                    |\n",
      "Step #3: +-----------------------------+-------------------------------------------------------+\n",
      "Finished Step #3\n",
      "PUSH\n",
      "Pushing gcr.io/etl-project-datahub/trainer_image:latest\n",
      "The push refers to repository [gcr.io/etl-project-datahub/trainer_image]\n",
      "357c60c7c6f1: Preparing\n",
      "5b60e8777898: Preparing\n",
      "787212a125c4: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "c51fe61c6231: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "5b60e8777898: Pushed\n",
      "357c60c7c6f1: Pushed\n",
      "4c4a5579b7a8: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "787212a125c4: Pushed\n",
      "latest: digest: sha256:954c475df85134899de6db739525519d29df62ca5d2a23b52ea909cbe9bf8352 size: 4708\n",
      "Pushing gcr.io/etl-project-datahub/base_image:latest\n",
      "The push refers to repository [gcr.io/etl-project-datahub/base_image]\n",
      "83662df0852a: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "c51fe61c6231: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "7f996c16a28a: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "83662df0852a: Pushed\n",
      "latest: digest: sha256:7932e508c761de4cddbbdb0dc7e35ee9e8d5d24b1e3b7e87b68d4ca1d0de87de size: 4293\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                             IMAGES                                              STATUS\n",
      "3abcbdfe-ff4f-4674-bdde-5c501b7bc12d  2020-09-07T13:50:12+00:00  4M28S     gs://etl-project-datahub_cloudbuild/source/1599486609.236557-e99c5af839c149b78a99e56f30e178d8.tgz  gcr.io/etl-project-datahub/trainer_image (+3 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit . --config cloudbuild.yaml --substitutions {SUBSTITUTIONS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
