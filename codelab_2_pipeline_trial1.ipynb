{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "#ENDPOINT = '<YOUR_ENDPOINT>' e.g. '337dd39580cbcbd2-dot-us-central2.pipelines.googleusercontent.com'\n",
    "ARTIFACT_STORE_URI = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create starified samples of training, validation, incremental data\n",
    "import time\n",
    "training_dataset_path = 'gs://input_data_amy_bkt1/input_data/Anonymized_Fermentation_Data_final.xlsx'\n",
    "validation_dataset_path = 'gs://input_data_amy_bkt1/input_data/Anonymized_Fermentation_Validation_Data_final.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Write the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "    #data = pd.read_excel(bkt_excl,sheet_name='data') \n",
    "    df_train = pd.read_excel(training_dataset_path,sheet_name='data')\n",
    "    df_validation = pd.read_excel(validation_dataset_path,sheet_name='data')\n",
    "    meta_data = pd.read_excel(training_dataset_path,sheet_name='meta data') \n",
    "    \n",
    "    #df_train = pd.read_csv(training_dataset_path)\n",
    "    #df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    #numeric_feature_indexes = slice(0, 10)\n",
    "    #categorical_feature_indexes = slice(10, 12)\n",
    "   \n",
    "    \n",
    "    #Prepare data for analysis\n",
    "    #Split out numeric from categorical varibles\n",
    "\n",
    "    numeric_vars = ((df_train.dtypes == 'float64') | (df_train.dtypes == 'int64')) & (meta_data['variable type'] == 'independent').values\n",
    "    numeric_x_data = df_train[df_train.columns[numeric_vars]]\n",
    "\n",
    "    numeric_vars_val = ((df_validation.dtypes == 'float64') | (df_validation.dtypes == 'int64')) & (meta_data['variable type'] == 'independent').values\n",
    "    numeric_x_data_val = df_validation[df_validation.columns[numeric_vars_val]]\n",
    "\n",
    "    \n",
    "    cat_vars = ((df_train.dtypes == 'string') | (df_train.dtypes == 'object')) & (meta_data['variable type'] == 'independent').values\n",
    "    cat_x_data = df_train[df_train.columns[cat_vars]]\n",
    "\n",
    "    #Things to try to predict\n",
    "\n",
    "    y_data = df_train[df_train.columns[(meta_data['target'] == 1).values]]\n",
    "    y_data_val = df_validation[df_validation.columns[(meta_data['target'] == 1).values]]\n",
    "    \n",
    "   \n",
    "    # meta data about variables\n",
    "\n",
    "    meta_data = meta_data.set_index('name')    \n",
    "\n",
    "    #Impute missing with median #handle missing values with median. SKlearn provides imputer\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "    #auto scale\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    #pipe = Pipeline([('imputer',imputer),('scaler', scaler)])\n",
    "    \n",
    "    #preprocessor = ColumnTransformer(\n",
    "    #transformers=[\n",
    "    #    ('num', StandardScaler(), numeric_x_data),\n",
    "    #    ('cat', OneHotEncoder(), cat_x_data) \n",
    "    #])\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('imputer',imputer), ('scaler', scaler) \n",
    "    ])\n",
    "\n",
    "    \n",
    "    scaled_numeric_x_data = pipe.fit_transform(numeric_x_data)\n",
    "    scaled_numeric_x_data_val = pipe.fit_transform(numeric_x_data_val)\n",
    "    \n",
    "    pca = PCA(n_components=3)\n",
    "    pca_result = pca.fit_transform(scaled_numeric_x_data)\n",
    "    pca_result_val = pca.fit_transform(scaled_numeric_x_data_val)\n",
    "\n",
    "    \n",
    "    #pipe = Pipeline([\n",
    "    #    ('preprocessor', preprocessor),('imputer',imputer),\n",
    "    #    ('sgdregressor', SGDRegressor(loss='squared_loss',tol=1e-3))\n",
    "    #])\n",
    "\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('sgdregressor', SGDRegressor(loss='squared_loss',tol=1e-3))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    ##########\n",
    "    #preprocessor = ColumnTransformer(\n",
    "    #transformers=[\n",
    "    #    ('num', StandardScaler(), numeric_feature_indexes),\n",
    "    #    ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    #])\n",
    "\n",
    "    #pipeline = Pipeline([\n",
    "    #    ('preprocessor', preprocessor),\n",
    "    #    ('classifier', SGDClassifier(loss='log',tol=1e-3))\n",
    "    #])\n",
    "\n",
    "    #num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "    #df_train = df_train.astype(num_features_type_map)\n",
    "    #df_validation = df_validation.astype(num_features_type_map) \n",
    "     \n",
    "    \n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    df_train = get_results(pca_result,'pca-', add = y_data)\n",
    "    df1_train = df_train\n",
    "    df2_train = df1_train.dropna()\n",
    "    \n",
    "    df_validation = get_results(pca_result_val,'pca-', add = y_data_val)\n",
    "    df1_validation = df_validation\n",
    "    df2_validation = df1_validation.dropna()\n",
    "\n",
    "    \n",
    "    #X_train = df_train.drop(\"Run_Execution\",\"Run_Performance\",\"Product_Produced__g\",\"Titer_End__g_over_kg\", axis=1)\n",
    "    #y_train = df_train[\"Product_Produced__g\"]\n",
    "    #X_train = df2_train.drop(\"Run_Execution\",\"Run_Performance\",\"Product_Produced__g\",\"Titer_End__g_over_kg\", axis=1)\n",
    "    X_train = df2_train.drop(columns=[\"Run_Execution\",\"Run_Performance\",\"Product_Produced__g\",\"Titer_End__g_over_kg\"])\n",
    "    y_train = df2_train[\"Product_Produced__g\"]\n",
    "\n",
    "\n",
    "    #pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.set_params(sgdregressor__alpha=alpha, sgdregressor__max_iter=max_iter)   \n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if hptune:\n",
    "        X_validation = df2_validation.drop(columns=[\"Run_Execution\",\"Run_Performance\",\"Product_Produced__g\",\"Titer_End__g_over_kg\"])\n",
    "        y_validation = df2_validation[\"Product_Produced__g\"]\n",
    "        accuracy = pipeline.score(X_validation, y_validation)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(pipeline, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "\n",
    "        \n",
    "\n",
    "def get_results(res,prefix='',ncol=3, add=None):\n",
    "    #collect results    \n",
    "    out= pd.DataFrame()\n",
    "    for i in range(ncol):\n",
    "        key = prefix + str(i+1)\n",
    "        value = res[:,i]\n",
    "        out.loc[:,key] = value\n",
    "    \n",
    "    if add is not None:\n",
    "        out = pd.concat([out,add],axis=1)\n",
    "    \n",
    "    return out\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Package the script into a docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: base_image/Dockerfile: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat base_image/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: trainer_image/Dockerfile: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat trainer_image/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 5 file(s) totalling 12.5 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://etl-project-datahub_cloudbuild/source/1599064124.944803-638cf3bf1baf4081984c065aab452fed.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/etl-project-datahub/builds/c667b57a-f1e0-4031-85d0-490bb0cb5149].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/c667b57a-f1e0-4031-85d0-490bb0cb5149?project=448067079266].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"c667b57a-f1e0-4031-85d0-490bb0cb5149\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://etl-project-datahub_cloudbuild/source/1599064124.944803-638cf3bf1baf4081984c065aab452fed.tgz#1599064125372248\n",
      "Copying gs://etl-project-datahub_cloudbuild/source/1599064124.944803-638cf3bf1baf4081984c065aab452fed.tgz#1599064125372248...\n",
      "/ [1 files][  3.0 KiB/  3.0 KiB]                                                \n",
      "Operation completed over 1 objects/3.0 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  17.92kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d7c3167c320d: Pulling fs layer\n",
      "131f805ec7fd: Pulling fs layer\n",
      "322ed380e680: Pulling fs layer\n",
      "6ac240b13098: Pulling fs layer\n",
      "9ce3a9266402: Pulling fs layer\n",
      "72c706dfac1d: Pulling fs layer\n",
      "6383427606e5: Pulling fs layer\n",
      "3e8b21666cec: Pulling fs layer\n",
      "358bb5d659ed: Pulling fs layer\n",
      "8ade7556a8f1: Pulling fs layer\n",
      "b2ebb7e1223e: Pulling fs layer\n",
      "8d5d283ad922: Pulling fs layer\n",
      "14c0fd48a5f3: Pulling fs layer\n",
      "ceaad5dc04d2: Pulling fs layer\n",
      "c1074350f761: Pulling fs layer\n",
      "687ad0b9a318: Pulling fs layer\n",
      "d2365d2ee19a: Pulling fs layer\n",
      "5095b04f1d98: Pulling fs layer\n",
      "6ac240b13098: Waiting\n",
      "9ce3a9266402: Waiting\n",
      "72c706dfac1d: Waiting\n",
      "6383427606e5: Waiting\n",
      "3e8b21666cec: Waiting\n",
      "358bb5d659ed: Waiting\n",
      "8ade7556a8f1: Waiting\n",
      "b2ebb7e1223e: Waiting\n",
      "14c0fd48a5f3: Waiting\n",
      "ceaad5dc04d2: Waiting\n",
      "c1074350f761: Waiting\n",
      "687ad0b9a318: Waiting\n",
      "d2365d2ee19a: Waiting\n",
      "5095b04f1d98: Waiting\n",
      "8d5d283ad922: Waiting\n",
      "322ed380e680: Verifying Checksum\n",
      "322ed380e680: Download complete\n",
      "131f805ec7fd: Verifying Checksum\n",
      "131f805ec7fd: Download complete\n",
      "6ac240b13098: Verifying Checksum\n",
      "6ac240b13098: Download complete\n",
      "d7c3167c320d: Verifying Checksum\n",
      "d7c3167c320d: Download complete\n",
      "6383427606e5: Verifying Checksum\n",
      "6383427606e5: Download complete\n",
      "72c706dfac1d: Verifying Checksum\n",
      "72c706dfac1d: Download complete\n",
      "358bb5d659ed: Verifying Checksum\n",
      "358bb5d659ed: Download complete\n",
      "3e8b21666cec: Verifying Checksum\n",
      "3e8b21666cec: Download complete\n",
      "8ade7556a8f1: Verifying Checksum\n",
      "8ade7556a8f1: Download complete\n",
      "b2ebb7e1223e: Verifying Checksum\n",
      "b2ebb7e1223e: Download complete\n",
      "8d5d283ad922: Verifying Checksum\n",
      "8d5d283ad922: Download complete\n",
      "14c0fd48a5f3: Verifying Checksum\n",
      "14c0fd48a5f3: Download complete\n",
      "ceaad5dc04d2: Verifying Checksum\n",
      "ceaad5dc04d2: Download complete\n",
      "687ad0b9a318: Verifying Checksum\n",
      "687ad0b9a318: Download complete\n",
      "c1074350f761: Verifying Checksum\n",
      "c1074350f761: Download complete\n",
      "5095b04f1d98: Verifying Checksum\n",
      "5095b04f1d98: Download complete\n",
      "9ce3a9266402: Verifying Checksum\n",
      "9ce3a9266402: Download complete\n",
      "d7c3167c320d: Pull complete\n",
      "131f805ec7fd: Pull complete\n",
      "322ed380e680: Pull complete\n",
      "6ac240b13098: Pull complete\n",
      "d2365d2ee19a: Verifying Checksum\n",
      "d2365d2ee19a: Download complete\n",
      "9ce3a9266402: Pull complete\n",
      "72c706dfac1d: Pull complete\n",
      "6383427606e5: Pull complete\n",
      "3e8b21666cec: Pull complete\n",
      "358bb5d659ed: Pull complete\n",
      "8ade7556a8f1: Pull complete\n",
      "b2ebb7e1223e: Pull complete\n",
      "8d5d283ad922: Pull complete\n",
      "14c0fd48a5f3: Pull complete\n",
      "ceaad5dc04d2: Pull complete\n",
      "c1074350f761: Pull complete\n",
      "687ad0b9a318: Pull complete\n",
      "d2365d2ee19a: Pull complete\n",
      "5095b04f1d98: Pull complete\n",
      "Digest: sha256:4d7a2b0e4c15c7d80bf2b3f32de29fd985f3617a21384510ea3c964a7bd5cd91\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> d8706668f140\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in 8f01636f401a\n",
      "Collecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=f5c17b7cbe5d24239a8b32a72943282954385f22d490ed3ee304f572f7f3ec6e\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=15d3a0f16638082961eb1ad897b8d1f3095d9dbab01f4d69efa562e028984566\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=5131a73510b1839f3a2cc27b32d0fc4b0d2ed3b3ef34cc6486d3d90d944bb8c0\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "\u001b[91mERROR: visions 0.4.4 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: pandas-profiling 2.8.0 has requirement pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: termcolor, fire, cloudml-hypertune, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.3.1 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Removing intermediate container 8f01636f401a\n",
      " ---> 2b56f23a69e0\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 1c7b605d6153\n",
      "Removing intermediate container 1c7b605d6153\n",
      " ---> b53f649676eb\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> e551af4d2a26\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 3286b1a88312\n",
      "Removing intermediate container 3286b1a88312\n",
      " ---> 65fa93557bcd\n",
      "Successfully built 65fa93557bcd\n",
      "Successfully tagged gcr.io/etl-project-datahub/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/etl-project-datahub/trainer_image:latest\n",
      "The push refers to repository [gcr.io/etl-project-datahub/trainer_image]\n",
      "b453667c69cd: Preparing\n",
      "ab23f04b4adb: Preparing\n",
      "c1a3d6b88d08: Preparing\n",
      "89212ed9ad75: Preparing\n",
      "c51fe61c6231: Preparing\n",
      "222959643149: Preparing\n",
      "badaf1bc8335: Preparing\n",
      "c9057fce4bef: Preparing\n",
      "81da25416dd1: Preparing\n",
      "67169bef6670: Preparing\n",
      "c8cc397a1d54: Preparing\n",
      "4c4a5579b7a8: Preparing\n",
      "7f996c16a28a: Preparing\n",
      "5133f6c43556: Preparing\n",
      "5b5017461bc6: Preparing\n",
      "69b6474ff053: Preparing\n",
      "c2fd7a04bf9f: Preparing\n",
      "ddc500d84994: Preparing\n",
      "c64c52ea2c16: Preparing\n",
      "5930c9e5703f: Preparing\n",
      "b187ff70b2e4: Preparing\n",
      "222959643149: Waiting\n",
      "badaf1bc8335: Waiting\n",
      "c9057fce4bef: Waiting\n",
      "81da25416dd1: Waiting\n",
      "67169bef6670: Waiting\n",
      "c8cc397a1d54: Waiting\n",
      "4c4a5579b7a8: Waiting\n",
      "7f996c16a28a: Waiting\n",
      "5133f6c43556: Waiting\n",
      "5b5017461bc6: Waiting\n",
      "69b6474ff053: Waiting\n",
      "c2fd7a04bf9f: Waiting\n",
      "ddc500d84994: Waiting\n",
      "c64c52ea2c16: Waiting\n",
      "5930c9e5703f: Waiting\n",
      "b187ff70b2e4: Waiting\n",
      "c51fe61c6231: Layer already exists\n",
      "89212ed9ad75: Layer already exists\n",
      "222959643149: Layer already exists\n",
      "badaf1bc8335: Layer already exists\n",
      "c9057fce4bef: Layer already exists\n",
      "81da25416dd1: Layer already exists\n",
      "67169bef6670: Layer already exists\n",
      "c8cc397a1d54: Layer already exists\n",
      "4c4a5579b7a8: Layer already exists\n",
      "ab23f04b4adb: Pushed\n",
      "7f996c16a28a: Layer already exists\n",
      "5b5017461bc6: Layer already exists\n",
      "5133f6c43556: Layer already exists\n",
      "b453667c69cd: Pushed\n",
      "69b6474ff053: Layer already exists\n",
      "c2fd7a04bf9f: Layer already exists\n",
      "c64c52ea2c16: Layer already exists\n",
      "ddc500d84994: Layer already exists\n",
      "5930c9e5703f: Layer already exists\n",
      "b187ff70b2e4: Layer already exists\n",
      "c1a3d6b88d08: Pushed\n",
      "latest: digest: sha256:20f49f9262fc10113db1539f0777bdd5b649d3d9692cb13fc40a0bc5dd6ce05c size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                             IMAGES                                              STATUS\n",
      "c667b57a-f1e0-4031-85d0-490bb0cb5149  2020-09-02T16:28:45+00:00  3M39S     gs://etl-project-datahub_cloudbuild/source/1599064124.944803-638cf3bf1baf4081984c065aab452fed.tgz  gcr.io/etl-project-datahub/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter tuning jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the hyperparameter configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 3\n",
    "    maxParallelTrials: 3\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_iter\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          200,\n",
    "          500\n",
    "          ]\n",
    "    - parameterName: alpha\n",
    "      type: DOUBLE\n",
    "      minValue:  0.00001\n",
    "      maxValue:  0.001\n",
    "      scaleType: UNIT_LINEAR_SCALE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Start the hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "job_dir -> GCS Path for storing the job packages & model.\n",
    "\n",
    "training_dataset_path -> GCS path holding training dataset.\n",
    "\n",
    "validation_dataset_path -> GCS path holding validation dataset.\n",
    "\n",
    "alpha -> hyperparameter\n",
    "\n",
    "max_iter -> hyperparameter\n",
    "\n",
    "hptune -> variable to decide if hyperparameter tuning is to be done or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://workshop_trial_artifact_store_pp/staging\n",
      "gs://workshop_trial_artifact_store_pp/jobs\n",
      "JOB_20200902_163433\n",
      "gs://workshop_trial_artifact_store_pp/jobs/JOB_20200902_163433\n"
     ]
    }
   ],
   "source": [
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)\n",
    "gcs_root= GCS_STAGING_PATH \n",
    "print(gcs_root)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE_URI)\n",
    "print(JOB_DIR_ROOT)\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "print(JOB_NAME)\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "print(JOB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ARTIFACT_STORE = 'gs://workshop_trial_artifact_store_pp'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'training', 'dataset.xlsx')\n",
    "VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://workshop_trial_artifact_store_pp/jobs\n",
      "JOB_20200902_163437\n",
      "gs://workshop_trial_artifact_store_pp/jobs/JOB_20200902_163437\n"
     ]
    }
   ],
   "source": [
    "print(JOB_DIR_ROOT)\n",
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "print(JOB_NAME)\n",
    "print(JOB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20200902_163440] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20200902_163440\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20200902_163440\n",
      "jobId: JOB_20200902_163440\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--validation_dataset_path=$VALIDATION_FILE_PATH \\\n",
    "--hptune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2020-09-02T15:45:42Z'\n",
      "etag: Ay31Bn19aeg=\n",
      "jobId: JOB_20200902_154540\n",
      "startTime: '2020-09-02T15:45:43Z'\n",
      "state: RUNNING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --training_dataset_path=gs://workshop_trial_artifact_store_pp/data/training/dataset.xlsx\n",
      "  - --validation_dataset_path=gs://workshop_trial_artifact_store_pp/data/validation/dataset.xlsx\n",
      "  - --hptune\n",
      "  hyperparameters:\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: accuracy\n",
      "    maxParallelTrials: 1\n",
      "    maxTrials: 1\n",
      "    params:\n",
      "    - discreteValues:\n",
      "      - 200.0\n",
      "      - 500.0\n",
      "      parameterName: max_iter\n",
      "      type: DISCRETE\n",
      "    - maxValue: 0.001\n",
      "      minValue: 1e-05\n",
      "      parameterName: alpha\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: DOUBLE\n",
      "  jobDir: gs://workshop_trial_artifact_store_pp/jobs/JOB_20200902_154540\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/etl-project-datahub/trainer_image:latest\n",
      "  region: us-central1\n",
      "trainingOutput:\n",
      "  isHyperparameterTuningJob: true\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20200902_154540?project=etl-project-datahub\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2FJOB_20200902_154540&project=etl-project-datahub\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'JOB_20200902_154540',\n",
       " 'trainingInput': {'args': ['--training_dataset_path=gs://workshop_trial_artifact_store_pp/data/training/dataset.xlsx',\n",
       "   '--validation_dataset_path=gs://workshop_trial_artifact_store_pp/data/validation/dataset.xlsx',\n",
       "   '--hptune'],\n",
       "  'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "   'params': [{'parameterName': 'max_iter',\n",
       "     'type': 'DISCRETE',\n",
       "     'discreteValues': [200, 500]},\n",
       "    {'parameterName': 'alpha',\n",
       "     'minValue': 1e-05,\n",
       "     'maxValue': 0.001,\n",
       "     'type': 'DOUBLE',\n",
       "     'scaleType': 'UNIT_LINEAR_SCALE'}],\n",
       "   'maxTrials': 1,\n",
       "   'maxParallelTrials': 1,\n",
       "   'hyperparameterMetricTag': 'accuracy',\n",
       "   'enableTrialEarlyStopping': True},\n",
       "  'region': 'us-central1',\n",
       "  'jobDir': 'gs://workshop_trial_artifact_store_pp/jobs/JOB_20200902_154540',\n",
       "  'masterConfig': {'imageUri': 'gcr.io/etl-project-datahub/trainer_image:latest'}},\n",
       " 'createTime': '2020-09-02T15:45:42Z',\n",
       " 'startTime': '2020-09-02T15:45:43Z',\n",
       " 'endTime': '2020-09-02T15:55:30Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingOutput': {'completedTrialCount': '1',\n",
       "  'trials': [{'trialId': '1',\n",
       "    'hyperparameters': {'alpha': '0.000505', 'max_iter': '500'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': -2.148612042206689},\n",
       "    'startTime': '2020-09-02T15:46:20.963219131Z',\n",
       "    'endTime': '2020-09-02T15:54:33Z',\n",
       "    'state': 'SUCCEEDED'}],\n",
       "  'consumedMLUnits': 0.06,\n",
       "  'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'accuracy'},\n",
       " 'etag': 'Eayej/Nolv0='}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "ml = discovery.build('ml', 'v1')\n",
    "\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOB_NAME)\n",
    "request = ml.projects().jobs().get(name=job_id)\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "except errors.HttpError as err:\n",
    "    print(err)\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    \n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
